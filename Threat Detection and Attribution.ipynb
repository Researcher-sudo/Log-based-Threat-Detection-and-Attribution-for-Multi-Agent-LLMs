{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Zl6ZWhsVlUCl",
        "outputId": "1c887a7f-8d47-4539-f42e-787ff8b7c346"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# === Cell A: Setup + robust gpickle loader (drop this at the very top) ===\n",
        "import os, json, math, statistics, pickle, random\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# bundles & output dir\n",
        "BUNDLE_DIRS = [\n",
        "    \"/content/drive/MyDrive/logs_bundle\",\n",
        "    \"/content/drive/MyDrive/logs_bundle2\",\n",
        "    \"/content/drive/MyDrive/logs_bundle3\",\n",
        "    \"/content/drive/MyDrive/Attack_bundle\",\n",
        "]\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\"); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "FIG_DIR = OUT_DIR / \"figs\"; FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- loader that works on all NetworkX versions ---\n",
        "try:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _nx_read_gpickle\n",
        "except Exception:\n",
        "    _nx_read_gpickle = None\n",
        "\n",
        "def load_gpickle(path: Path):\n",
        "    if _nx_read_gpickle is not None:\n",
        "        return _nx_read_gpickle(path)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def list_gpickle_exports(base_dir: Path) -> List[Path]:\n",
        "    exp = base_dir / \"_exports\"\n",
        "    return sorted(exp.glob(\"*.gpickle\")) if exp.exists() else []\n",
        "\n",
        "def to_simple_digraph(G: nx.MultiDiGraph) -> nx.DiGraph:\n",
        "    DG = nx.DiGraph()\n",
        "    DG.add_nodes_from(G.nodes(data=True))\n",
        "    for u, v in G.edges():\n",
        "        DG.add_edge(u, v)\n",
        "    return DG\n",
        "\n",
        "def to_simple_graph(G: nx.MultiDiGraph) -> nx.Graph:\n",
        "    UG = nx.Graph()\n",
        "    UG.add_nodes_from(G.nodes(data=True))\n",
        "    for u, v in G.edges():\n",
        "        if u != v:\n",
        "            UG.add_edge(u, v)\n",
        "    return UG\n",
        "\n",
        "def safe_stat(fn, default=np.nan):\n",
        "    try:\n",
        "        v = fn()\n",
        "        return float(v) if isinstance(v, (int, float, np.floating)) else v\n",
        "    except Exception:\n",
        "        return default\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell B: Correlations + PCA (fixed) ===\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read the extended CSV you already created earlier\n",
        "dfx = pd.read_csv(OUT_DIR / \"graph_stats_extended.csv\")\n",
        "\n",
        "metrics_for_corr = [\n",
        "    \"n_nodes\",\"n_edges\",\"simple_edges\",\"parallel_edge_ratio\",\"self_loops\",\n",
        "    \"density_directed\",\"density_undirected\",\"reciprocity\",\"avg_clustering\",\n",
        "    \"diameter_undirected_lcc\",\"wcc_count\",\"scc_count\",\"largest_wcc_nodes\",\n",
        "    \"frac_in_largest_wcc_nodes\",\"in_degree_mean\",\"in_degree_median\",\n",
        "    \"in_degree_max\",\"in_degree_gini\",\"out_degree_mean\",\"out_degree_median\",\n",
        "    \"out_degree_max\",\"out_degree_gini\",\"assort_in_in\",\"assort_out_out\",\n",
        "    \"assort_undirected\",\"kcore_kmax\",\"kcore_kmax_size\",\"triangles\",\n",
        "    \"transitivity\",\"pl_in_R2\",\"pl_out_R2\",\"frac_can_reach_giant\",\n",
        "    \"frac_reachable_from_giant\",\"role_entropy\",\"n_roles\"\n",
        "]\n",
        "\n",
        "Xraw = dfx[metrics_for_corr].select_dtypes(include=[np.number])\n",
        "all_nan_cols = [c for c in Xraw.columns if Xraw[c].isna().all()]\n",
        "Xraw = Xraw.drop(columns=all_nan_cols, errors=\"ignore\")\n",
        "\n",
        "vt = VarianceThreshold(0.0)\n",
        "tmp_imp = SimpleImputer(strategy=\"median\")\n",
        "Xtmp = tmp_imp.fit_transform(Xraw)\n",
        "vt.fit(Xtmp)\n",
        "kept_cols = Xraw.columns[vt.get_support()]\n",
        "\n",
        "imp = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
        "X_imp = imp.fit_transform(dfx[kept_cols])\n",
        "scaler = StandardScaler()\n",
        "Xz = scaler.fit_transform(X_imp)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=0)\n",
        "pcs = pca.fit_transform(Xz)\n",
        "\n",
        "pc_df = pd.DataFrame({\"PC1\": pcs[:,0], \"PC2\": pcs[:,1], \"bundle\": dfx[\"bundle\"].values})\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "for b, sub in pc_df.groupby(\"bundle\"):\n",
        "    plt.scatter(sub[\"PC1\"], sub[\"PC2\"], label=b, alpha=0.7, s=40)\n",
        "plt.legend()\n",
        "plt.title(f\"PCA of Graph Metrics (PC1={pca.explained_variance_ratio_[0]:.2f}, PC2={pca.explained_variance_ratio_[1]:.2f})\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"pca_scatter.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# also refresh a simple correlation heatmap on the kept columns\n",
        "corr_df = dfx[kept_cols].corr(numeric_only=True)\n",
        "plt.figure(figsize=(12,10))\n",
        "im = plt.imshow(corr_df, interpolation='nearest')\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.xticks(range(len(corr_df.columns)), corr_df.columns, rotation=90)\n",
        "plt.yticks(range(len(corr_df.columns)), corr_df.columns)\n",
        "plt.title(\"Metric Correlations (filtered)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"correlations.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"FIGURES:\")\n",
        "print(\" -\", FIG_DIR / \"pca_scatter.png\")\n",
        "print(\" -\", FIG_DIR / \"correlations.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Qq-iAfgClWnh",
        "outputId": "378ae761-3176-4411-ba30-e7abf883fbe4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIGURES:\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/pca_scatter.png\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/correlations.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Temporal round-trip from moderator to moderator ===\n",
        "# For each mod→X edge at time t0, find the earliest time-respecting path\n",
        "# that returns to ANY moderator node with non-decreasing timestamps.\n",
        "# Outputs per-graph aggregate stats and cohort plots.\n",
        "\n",
        "import os, math, pickle, bisect, numpy as np, pandas as pd, networkx as nx\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "FIG_DIR = OUT_DIR / \"figs\"; FIG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# --- helpers (compatible with your earlier cells) ---\n",
        "try:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _nx_read_gpickle\n",
        "except Exception:\n",
        "    _nx_read_gpickle = None\n",
        "\n",
        "def load_gpickle(path: Path):\n",
        "    if _nx_read_gpickle is not None:\n",
        "        return _nx_read_gpickle(path)\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def list_gpickle_exports(base_dir: Path):\n",
        "    exp = base_dir / \"_exports\"\n",
        "    return sorted(exp.glob(\"*.gpickle\")) if exp.exists() else []\n",
        "\n",
        "def to_simple_digraph(G: nx.MultiDiGraph) -> nx.DiGraph:\n",
        "    D = nx.DiGraph()\n",
        "    D.add_nodes_from(G.nodes(data=True))\n",
        "    D.add_edges_from([(u, v, d) for u, v, d in G.edges(data=True)])  # collapse multiedges\n",
        "    return D\n",
        "\n",
        "def get_role(d):\n",
        "    return d.get(\"role\") or d.get(\"agent_role\") or d.get(\"type\")\n",
        "\n",
        "# choose your timestamp keys in priority order\n",
        "TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\")\n",
        "\n",
        "def edge_time(ed):\n",
        "    for k in TS_KEYS:\n",
        "        if k in ed and ed[k] is not None:\n",
        "            try:\n",
        "                return float(ed[k])\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None  # no usable timestamp\n",
        "\n",
        "def temporal_roundtrip_for_graph(path: Path, mod_role=\"moderator\", max_expansions=5000):\n",
        "    \"\"\"Return per-mod-prompt roundtrips and per-graph summary for one graph.\"\"\"\n",
        "    G = load_gpickle(path)\n",
        "    if isinstance(G, (nx.Graph, nx.DiGraph, nx.MultiGraph)):\n",
        "        G = nx.MultiDiGraph(G)\n",
        "    D = to_simple_digraph(G)\n",
        "\n",
        "    roles = {n: get_role(D.nodes[n]) for n in D.nodes()}\n",
        "    mods = {n for n,r in roles.items() if str(r).lower().startswith(\"mod\")} or \\\n",
        "           {n for n,r in roles.items() if str(r).lower()==\"moderator\"}\n",
        "\n",
        "    # collect time-stamped edges; ignore edges with no timestamp for temporal paths\n",
        "    E = []\n",
        "    for u, v, ed in D.edges(data=True):\n",
        "        t = edge_time(ed)\n",
        "        if t is not None:\n",
        "            E.append((u, v, t))\n",
        "    if not E or not mods:\n",
        "        return dict(\n",
        "            file=str(path),\n",
        "            bundle=path.parent.parent.name,\n",
        "            rt_hops_mean=np.nan, rt_hops_median=np.nan, rt_hops_p95=np.nan,\n",
        "            rt_time_mean=np.nan, rt_time_median=np.nan, rt_time_p95=np.nan,\n",
        "            rt_completion_rate=np.nan, n_prompts=0\n",
        "        ), pd.DataFrame(columns=[\"file\",\"bundle\",\"t0\",\"start_u\",\"start_v\",\"rt_hops\",\"rt_time\"])\n",
        "\n",
        "    # index outgoing edges by source with times sorted for fast \"next events\"\n",
        "    out_times = {}\n",
        "    out_neighs = {}\n",
        "    for u, v, t in E:\n",
        "        out_times.setdefault(u, []).append(t)\n",
        "        out_neighs.setdefault(u, []).append(v)\n",
        "    for u in out_times:\n",
        "        # sort by time, and keep the neighbor list aligned\n",
        "        order = np.argsort(out_times[u])\n",
        "        out_times[u]  = [out_times[u][i]  for i in order]\n",
        "        out_neighs[u] = [out_neighs[u][i] for i in order]\n",
        "\n",
        "    # all moderator prompts (edges mod -> x)\n",
        "    prompts = [(u, v, t) for (u, v, t) in E if u in mods and v not in mods]\n",
        "    rows = []\n",
        "\n",
        "    for (m, x, t0) in sorted(prompts, key=lambda z: z[2]):\n",
        "        # temporal BFS: (node, current_time, hops_from_x)\n",
        "        # we already used the first edge m->x, so total hops = 1 + hops_from_x_to_mod\n",
        "        # require non-decreasing timestamps along the path (allow equal times)\n",
        "        from collections import deque\n",
        "        q = deque()\n",
        "        q.append((x, t0, 0))\n",
        "        seen = {(x, t0)}  # node with time threshold\n",
        "        found_hops = None\n",
        "        found_time = None\n",
        "        expansions = 0\n",
        "\n",
        "        while q and expansions < max_expansions:\n",
        "            u, t_curr, h = q.popleft()\n",
        "            expansions += 1\n",
        "            # if we already returned to ANY moderator, record and stop\n",
        "            if u in mods and h > 0:  # only counts after at least one step from x\n",
        "                found_hops = 1 + h  # include the initial m->x hop\n",
        "                found_time = t_curr - t0\n",
        "                break\n",
        "\n",
        "            # advance time-respecting edges u -> w with time >= t_curr\n",
        "            ts = out_times.get(u)\n",
        "            ws = out_neighs.get(u)\n",
        "            if not ts:\n",
        "                continue\n",
        "            i = bisect.bisect_left(ts, t_curr)  # allow same-time edge\n",
        "            for j in range(i, len(ts)):\n",
        "                t_next = ts[j]\n",
        "                w = ws[j]\n",
        "                key = (w, t_next)\n",
        "                if key in seen:\n",
        "                    continue\n",
        "                seen.add(key)\n",
        "                q.append((w, t_next, h+1))\n",
        "\n",
        "        if found_hops is not None:\n",
        "            rows.append(dict(\n",
        "                file=str(path), bundle=path.parent.parent.name,\n",
        "                t0=t0, start_u=m, start_v=x,\n",
        "                rt_hops=int(found_hops),\n",
        "                rt_time=float(found_time)\n",
        "            ))\n",
        "        else:\n",
        "            rows.append(dict(\n",
        "                file=str(path), bundle=path.parent.parent.name,\n",
        "                t0=t0, start_u=m, start_v=x,\n",
        "                rt_hops=np.nan, rt_time=np.nan\n",
        "            ))\n",
        "\n",
        "    per_prompt = pd.DataFrame(rows)\n",
        "    n_prompts = len(per_prompt)\n",
        "    ok = per_prompt.dropna(subset=[\"rt_hops\",\"rt_time\"])\n",
        "    comp_rate = (len(ok)/n_prompts) if n_prompts else np.nan\n",
        "\n",
        "    def q(s, p):\n",
        "        return float(np.nanquantile(s, p)) if len(s.dropna()) else np.nan\n",
        "\n",
        "    summary = dict(\n",
        "        file=str(path),\n",
        "        bundle=path.parent.parent.name,\n",
        "        rt_hops_mean = float(ok[\"rt_hops\"].mean()) if len(ok) else np.nan,\n",
        "        rt_hops_median = float(ok[\"rt_hops\"].median()) if len(ok) else np.nan,\n",
        "        rt_hops_p95 = q(ok[\"rt_hops\"], 0.95),\n",
        "        rt_time_mean = float(ok[\"rt_time\"].mean()) if len(ok) else np.nan,\n",
        "        rt_time_median = float(ok[\"rt_time\"].median()) if len(ok) else np.nan,\n",
        "        rt_time_p95 = q(ok[\"rt_time\"], 0.95),\n",
        "        rt_completion_rate = comp_rate,\n",
        "        n_prompts = int(n_prompts),\n",
        "    )\n",
        "    return summary, per_prompt\n",
        "\n",
        "# ---- run across bundles ----\n",
        "BUNDLE_DIRS = [\n",
        "    \"/content/drive/MyDrive/logs_bundle\",\n",
        "    \"/content/drive/MyDrive/logs_bundle2\",\n",
        "    \"/content/drive/MyDrive/logs_bundle3\",\n",
        "    \"/content/drive/MyDrive/Attack_bundle\",\n",
        "]\n",
        "\n",
        "summ_rows = []\n",
        "pp_rows = []\n",
        "\n",
        "for bundle in BUNDLE_DIRS:\n",
        "    base = Path(bundle)\n",
        "    gpks = list_gpickle_exports(base)\n",
        "    if not gpks:\n",
        "        print(f\"[WARN] no graphs in {base}/_exports\")\n",
        "        continue\n",
        "    # OLD: print(f\"[{base.split('/')[-1]}] {len(gpks)} graphs\")\n",
        "    print(f\"[{base.name}] {len(gpks)} graphs\")   # <-- use .name for Path\n",
        "    for p in gpks:\n",
        "        try:\n",
        "            s, pp = temporal_roundtrip_for_graph(p)\n",
        "            summ_rows.append(s)\n",
        "            if not pp.empty:\n",
        "                pp_rows.append(pp)\n",
        "        except Exception as e:\n",
        "            print(\"[ERR]\", p.name, e)\n",
        "\n",
        "\n",
        "rt_summary = pd.DataFrame(summ_rows).sort_values([\"bundle\",\"file\"])\n",
        "rt_prompts = pd.concat(pp_rows, ignore_index=True) if pp_rows else pd.DataFrame()\n",
        "\n",
        "# label cohorts\n",
        "NORMAL = {\"logs_bundle\",\"logs_bundle2\",\"logs_bundle3\"}\n",
        "rt_summary[\"cohort\"] = np.where(rt_summary[\"bundle\"].isin(NORMAL), \"normal\", \"attack\")\n",
        "rt_prompts[\"cohort\"] = np.where(rt_prompts[\"bundle\"].isin(NORMAL), \"normal\", \"attack\")\n",
        "\n",
        "# save\n",
        "rt_sum_csv = OUT_DIR / \"roundtrip_summary.csv\"\n",
        "rt_pp_csv  = OUT_DIR / \"roundtrip_per_prompt.csv\"\n",
        "rt_summary.to_csv(rt_sum_csv, index=False)\n",
        "rt_prompts.to_csv(rt_pp_csv, index=False)\n",
        "print(\"WROTE:\")\n",
        "print(\" -\", rt_sum_csv)\n",
        "print(\" -\", rt_pp_csv)\n",
        "\n",
        "# quick cohort plots (hops/time medians + completion rate)\n",
        "plt.figure(figsize=(7,5))\n",
        "rt_summary.boxplot(column=\"rt_hops_median\", by=\"cohort\")\n",
        "plt.suptitle(\"\")\n",
        "plt.title(\"Round-trip hops (median) by cohort\")\n",
        "plt.xlabel(\"cohort\"); plt.ylabel(\"hops\")\n",
        "plt.tight_layout(); plt.savefig(FIG_DIR / \"rt_hops_median_by_cohort.png\", dpi=160); plt.close()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "rt_summary.boxplot(column=\"rt_time_median\", by=\"cohort\")\n",
        "plt.suptitle(\"\")\n",
        "plt.title(\"Round-trip time (median) by cohort\")\n",
        "plt.xlabel(\"cohort\"); plt.ylabel(\"seconds\")\n",
        "plt.tight_layout(); plt.savefig(FIG_DIR / \"rt_time_median_by_cohort.png\", dpi=160); plt.close()\n",
        "\n",
        "plt.figure(figsize=(7,5))\n",
        "rt_summary.boxplot(column=\"rt_completion_rate\", by=\"cohort\")\n",
        "plt.suptitle(\"\")\n",
        "plt.title(\"Round-trip completion rate by cohort\")\n",
        "plt.xlabel(\"cohort\"); plt.ylabel(\"fraction of prompts that returned to mod\")\n",
        "plt.tight_layout(); plt.savefig(FIG_DIR / \"rt_completion_rate_by_cohort.png\", dpi=160); plt.close()\n",
        "\n",
        "print(\"FIGS:\")\n",
        "print(\" -\", FIG_DIR / \"rt_hops_median_by_cohort.png\")\n",
        "print(\" -\", FIG_DIR / \"rt_time_median_by_cohort.png\")\n",
        "print(\" -\", FIG_DIR / \"rt_completion_rate_by_cohort.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "DVQKAgsplWx2",
        "outputId": "a16c247f-f5b7-4640-bc12-9cacf3cbffbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[logs_bundle] 39 graphs\n",
            "[logs_bundle2] 219 graphs\n",
            "[logs_bundle3] 115 graphs\n",
            "[Attack_bundle] 397 graphs\n",
            "WROTE:\n",
            " - /content/drive/MyDrive/graph_stats_out/roundtrip_summary.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/roundtrip_per_prompt.csv\n",
            "FIGS:\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/rt_hops_median_by_cohort.png\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/rt_time_median_by_cohort.png\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/rt_completion_rate_by_cohort.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 700x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Round-trip analysis: tidy tables & stats ===\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "import scipy.stats as st\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "SUM_CSV = OUT_DIR / \"roundtrip_summary.csv\"          # written by your code\n",
        "PERPROMPT_CSV = OUT_DIR / \"roundtrip_per_prompt.csv\" # written by your code\n",
        "\n",
        "# Load\n",
        "df = pd.read_csv(SUM_CSV)\n",
        "dpp = pd.read_csv(PERPROMPT_CSV)\n",
        "\n",
        "# Column detection (be robust to naming)\n",
        "def pick(colnames, keys):\n",
        "    for k in keys:\n",
        "        c = [c for c in colnames if k.lower() in c.lower()]\n",
        "        if c: return c[0]\n",
        "    return None\n",
        "\n",
        "cohort_col = pick(df.columns, [\"cohort\"])\n",
        "bundle_col = pick(df.columns, [\"bundle\"])\n",
        "hops_col   = pick(df.columns, [\"rt_hops_median\",\"roundtrip_hops\",\"hops\"])\n",
        "time_col   = pick(df.columns, [\"rt_time_median\",\"roundtrip_time\",\"seconds\",\"duration\"])\n",
        "rate_col   = pick(df.columns, [\"rt_completion_rate\",\"completion\",\"success_rate\"])\n",
        "\n",
        "need = [cohort_col, bundle_col, hops_col, time_col, rate_col]\n",
        "if any(c is None for c in need):\n",
        "    missing = [n for n,c in zip([\"cohort\",\"bundle\",\"hops\",\"time\",\"rate\"], need) if c is None]\n",
        "    raise ValueError(f\"Could not find columns: {missing}. Have: {list(df.columns)}\")\n",
        "\n",
        "# === Helper stats\n",
        "def tidy(s: pd.Series):\n",
        "    s = s.dropna()\n",
        "    return pd.Series(dict(\n",
        "        n=int(s.size),\n",
        "        mean=s.mean(),\n",
        "        median=s.median(),\n",
        "        p25=s.quantile(0.25) if s.size else np.nan,\n",
        "        p75=s.quantile(0.75) if s.size else np.nan,\n",
        "        std=s.std(ddof=1),\n",
        "        min=s.min() if s.size else np.nan,\n",
        "        max=s.max() if s.size else np.nan,\n",
        "    ))\n",
        "\n",
        "def cliffs_delta(x, y):\n",
        "    x = np.asarray(x); y = np.asarray(y)\n",
        "    nx, ny = x.size, y.size\n",
        "    gt = sum((xi > y).sum() for xi in x)\n",
        "    lt = sum((xi < y).sum() for xi in x)\n",
        "    return (gt - lt) / (nx * ny)\n",
        "\n",
        "def mw_row(df, metric_col):\n",
        "    a = df.loc[df[cohort_col]==\"attack\", metric_col].dropna().values\n",
        "    n = df.loc[df[cohort_col]==\"normal\", metric_col].dropna().values\n",
        "    if len(a)==0 or len(n)==0:\n",
        "        return dict(metric=metric_col, normal_mean=np.nan, attack_mean=np.nan,\n",
        "                    U=np.nan, p_value=np.nan, cliffs_delta=np.nan, n_normal=len(n), n_attack=len(a))\n",
        "    U, p = st.mannwhitneyu(a, n, alternative=\"two-sided\")\n",
        "    d = cliffs_delta(a, n)\n",
        "    return dict(metric=metric_col, normal_mean=n.mean(), attack_mean=a.mean(),\n",
        "                U=U, p_value=p, cliffs_delta=d, n_normal=len(n), n_attack=len(a))\n",
        "\n",
        "# === 1) Cohort summary tables\n",
        "metrics = {\n",
        "    \"Hops (round-trip)\": hops_col,\n",
        "    \"Time (sec, round-trip)\": time_col,\n",
        "    \"Completion rate\": rate_col\n",
        "}\n",
        "\n",
        "cohort_tables = {}\n",
        "for label, col in metrics.items():\n",
        "    t = (df.groupby(cohort_col)[col].apply(tidy).unstack()\n",
        "         .loc[:, [\"n\",\"min\",\"p25\",\"median\",\"mean\",\"p75\",\"max\",\"std\"]]\n",
        "         .round(3))\n",
        "    cohort_tables[label] = t\n",
        "    t.to_csv(OUT_DIR / f\"rt_{col}_by_cohort.csv\")\n",
        "\n",
        "# === 2) Significance (Mann–Whitney U + Cliff’s Δ)\n",
        "sig_rows = [mw_row(df, hops_col), mw_row(df, time_col), mw_row(df, rate_col)]\n",
        "sig_df_sorted = pd.DataFrame(sig_rows).sort_values(\"p_value\")\n",
        "sig_df_sorted.to_csv(OUT_DIR / \"rt_significance_normal_vs_attack.csv\", index=False)\n",
        "\n",
        "# === 3) Bundle breakdown (nice for appendix)\n",
        "bundle_tbl = (df.groupby([bundle_col, cohort_col])[[hops_col, time_col, rate_col]]\n",
        "                .agg([\"count\",\"mean\",\"median\",\"std\"]).round(3))\n",
        "bundle_tbl.to_csv(OUT_DIR / \"rt_by_bundle_and_cohort.csv\")\n",
        "\n",
        "# === 4) Threshold prevalence (interpretability)\n",
        "thr = {\n",
        "    hops_col: [1,2,3],\n",
        "    rate_col: [0.6,0.8,0.95],\n",
        "    time_col: [10,30,60,120]\n",
        "}\n",
        "prev_blocks = []\n",
        "for m, cutpoints in thr.items():\n",
        "    for c in cutpoints:\n",
        "        name = f\"{m}≥{c}\" if m!=time_col else f\"{m}≥{c}s\"\n",
        "        prev = df.groupby(cohort_col)[m].apply(lambda s: (s>=c).mean()).rename(name)\n",
        "        prev_blocks.append(prev)\n",
        "prev_df = pd.concat(prev_blocks, axis=1).round(3)\n",
        "prev_df.to_csv(OUT_DIR / \"rt_threshold_prevalence.csv\")\n",
        "\n",
        "# === 5) Per-prompt (or per-case) reliability — robust to column name differences\n",
        "id_candidates = [\n",
        "    \"prompt_id\", \"case_id\", \"case\", \"root_id\", \"root\", \"thread_id\",\n",
        "    \"conversation_id\", \"session_id\", \"file\", \"caseId\", \"promptId\"\n",
        "]\n",
        "id_col = next((c for c in id_candidates if c in dpp.columns), None)\n",
        "\n",
        "# Pick metric columns FROM dpp (names may differ from df)\n",
        "dpp_cohort_col = next((c for c in dpp.columns if \"cohort\" in c.lower()), None)\n",
        "dpp_hops_col   = next((c for c in dpp.columns if any(k in c.lower() for k in\n",
        "                     [\"rt_hops_median\",\"roundtrip_hops\",\"hops\"])), None)\n",
        "dpp_time_col   = next((c for c in dpp.columns if any(k in c.lower() for k in\n",
        "                     [\"rt_time_median\",\"roundtrip_time\",\"seconds\",\"duration\",\"time\"])), None)\n",
        "dpp_rate_col   = next((c for c in dpp.columns if any(k in c.lower() for k in\n",
        "                     [\"rt_completion_rate\",\"completion\",\"success_rate\",\"rate\"])), None)\n",
        "\n",
        "needed = [dpp_cohort_col, id_col, dpp_rate_col, dpp_time_col, dpp_hops_col]\n",
        "if all(c is not None for c in needed):\n",
        "    perprompt = (dpp.groupby([dpp_cohort_col, id_col])[[dpp_rate_col, dpp_time_col, dpp_hops_col]]\n",
        "                   .agg([\"count\",\"mean\",\"median\"]).round(3))\n",
        "    perprompt_out = OUT_DIR / f\"rt_perprompt_summary_by_{id_col}.csv\"\n",
        "    perprompt.to_csv(perprompt_out)\n",
        "    print(\"WROTE:\", perprompt_out)\n",
        "else:\n",
        "    missing = [\"cohort_col\",\"id_col\",\"rate\",\"time\",\"hops\"]\n",
        "    have = dict(zip(missing, needed))\n",
        "    print(\"[SKIP] Per-prompt summary: missing columns in roundtrip_per_prompt.csv\")\n",
        "    print(\"      Needed (any reasonable alias):\",\n",
        "          \"{cohort, id, completion, time, hops}\")\n",
        "    print(\"      Detected:\", have)\n",
        "    print(\"      dpp columns:\", list(dpp.columns))\n",
        "\n",
        "\n",
        "# === 6) Export pretty markdown snippets for your report\n",
        "md = []\n",
        "md.append(\"# Round-trip Interaction Analysis\\n\")\n",
        "md.append(\"## Cohort summary (attack vs normal)\\n\")\n",
        "for label, t in cohort_tables.items():\n",
        "    md.append(f\"### {label}\\n\")\n",
        "    md.append(t.to_markdown()); md.append(\"\\n\")\n",
        "\n",
        "md.append(\"## Significance tests (Mann–Whitney U & Cliff’s Δ)\\n\")\n",
        "md.append(sig_df_sorted.round(4).to_markdown(index=False)); md.append(\"\\n\")\n",
        "\n",
        "md.append(\"## Threshold prevalence (proportions)\\n\")\n",
        "md.append(prev_df.to_markdown()); md.append(\"\\n\")\n",
        "\n",
        "md.append(\"## Bundle × Cohort breakdown (counts/means/medians)\\n\")\n",
        "md.append(bundle_tbl.to_markdown())\n",
        "(OUT_DIR / \"roundtrip_report_tables.md\").write_text(\"\\n\".join(md), encoding=\"utf-8\")\n",
        "\n",
        "print(\"WROTE TABLES:\")\n",
        "print(\" -\", OUT_DIR / \"rt_significance_normal_vs_attack.csv\")\n",
        "print(\" -\", OUT_DIR / \"rt_by_bundle_and_cohort.csv\")\n",
        "print(\" -\", OUT_DIR / \"rt_threshold_prevalence.csv\")\n",
        "print(\" -\", OUT_DIR / \"roundtrip_report_tables.md\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "q3-WbbxllW0V",
        "outputId": "d9e3a1a9-b3f3-482b-b2c3-e61c101c2308"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SKIP] Per-prompt summary: missing columns in roundtrip_per_prompt.csv\n",
            "      Needed (any reasonable alias): {cohort, id, completion, time, hops}\n",
            "      Detected: {'cohort_col': 'cohort', 'id_col': 'file', 'rate': None, 'time': 'rt_time', 'hops': 'rt_hops'}\n",
            "      dpp columns: ['file', 'bundle', 't0', 'start_u', 'start_v', 'rt_hops', 'rt_time', 'cohort']\n",
            "WROTE TABLES:\n",
            " - /content/drive/MyDrive/graph_stats_out/rt_significance_normal_vs_attack.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/rt_by_bundle_and_cohort.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/rt_threshold_prevalence.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/roundtrip_report_tables.md\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Classification with round-trip features (time/hops/completion) ===\n",
        "import pandas as pd, numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, balanced_accuracy_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "# Prefer enriched topo features if present\n",
        "ENR = OUT_DIR / \"graph_stats_enriched.csv\"\n",
        "EXT = OUT_DIR / \"graph_stats_extended.csv\"\n",
        "RTS = OUT_DIR / \"roundtrip_summary.csv\"\n",
        "\n",
        "# --- Load & merge ---\n",
        "dfx = pd.read_csv(ENR if ENR.exists() else EXT).copy()\n",
        "rt = pd.read_csv(RTS).copy()\n",
        "\n",
        "# robust column pickers\n",
        "def pick(df, *cands):\n",
        "    cols = df.columns.str.lower()\n",
        "    for cand in cands:\n",
        "        if cand in df.columns: return cand\n",
        "        # substring match\n",
        "        idx = [c for c in df.columns if cand.lower() == c.lower()]\n",
        "        if idx: return idx[0]\n",
        "        idx = [c for c in df.columns if cand.lower() in c.lower()]\n",
        "        if idx: return idx[0]\n",
        "    return None\n",
        "\n",
        "# keys to merge\n",
        "file_col_x = pick(dfx, \"file\", \"path\")\n",
        "file_col_r = pick(rt,  \"file\", \"path\")\n",
        "\n",
        "if file_col_x and file_col_r:\n",
        "    merged = dfx.merge(rt, left_on=file_col_x, right_on=file_col_r, how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "else:\n",
        "    # fallback: merge by bundle + case_id if present\n",
        "    bundle_x = pick(dfx, \"bundle\")\n",
        "    bundle_r = pick(rt,  \"bundle\")\n",
        "    case_x   = pick(dfx, \"case_id\", \"case\")\n",
        "    case_r   = pick(rt,  \"case_id\", \"case\")\n",
        "    if not (bundle_x and bundle_r and case_x and case_r):\n",
        "        raise SystemExit(\"Could not align roundtrip_summary with topo features. \"\n",
        "                         \"Need 'file' in both, or both ('bundle','case_id').\")\n",
        "    merged = dfx.merge(rt, left_on=[bundle_x, case_x], right_on=[bundle_r, case_r],\n",
        "                       how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "\n",
        "# cohort label rebuild (attack vs normal)\n",
        "NORMAL = {\"logs_bundle\",\"logs_bundle2\",\"logs_bundle3\"}\n",
        "ATTACK = {\"Attack_bundle\"}\n",
        "\n",
        "bundle_col = pick(merged, \"bundle\")\n",
        "merged[\"cohort\"] = np.where(merged[bundle_col].isin(ATTACK), \"attack\",\n",
        "                     np.where(merged[bundle_col].isin(NORMAL), \"normal\", \"other\"))\n",
        "merged = merged[merged[\"cohort\"].isin([\"normal\",\"attack\"])].copy()\n",
        "y = (merged[\"cohort\"]==\"attack\").astype(int).values\n",
        "\n",
        "# --- identify round-trip columns ---\n",
        "rt_time = pick(merged, \"rt_time_median\", \"rt_time\", \"roundtrip_time\", \"seconds\", \"duration\")\n",
        "rt_hops = pick(merged, \"rt_hops_median\", \"rt_hops\", \"roundtrip_hops\", \"hops\")\n",
        "rt_rate = pick(merged, \"rt_completion_rate\", \"completion\", \"success_rate\", \"rt_complete\")\n",
        "\n",
        "# fallback if completion not present -> synthesize as 1.0 (from your summary it’s 100%)\n",
        "if rt_rate is None:\n",
        "    merged[\"rt_completion_dummy\"] = 1.0\n",
        "    rt_rate = \"rt_completion_dummy\"\n",
        "\n",
        "# --- choose topo features (everything numeric you already computed) ---\n",
        "drop_like = {\"file\",\"path\",\"bundle\",\"cohort\",\"case_id\",\"case\",\"role_counts_json\"}\n",
        "num_cols  = [c for c in merged.columns\n",
        "             if (c not in drop_like) and pd.api.types.is_numeric_dtype(merged[c])]\n",
        "\n",
        "# blocks\n",
        "RT_BLOCK  = [rt_time, rt_hops, rt_rate]\n",
        "RT_BLOCK  = [c for c in RT_BLOCK if c is not None]\n",
        "TOPO_ONLY = [c for c in num_cols if c not in RT_BLOCK]\n",
        "COMBINED  = list(dict.fromkeys(TOPO_ONLY + RT_BLOCK))  # de-dup while preserving order\n",
        "\n",
        "def eval_block(cols, name):\n",
        "    if not cols:\n",
        "        return dict(model=name, AUC=np.nan, AP=np.nan, BalAcc=np.nan, F1=np.nan)\n",
        "    X = merged[cols].copy()\n",
        "    # simple models (NaN-safe via imputer)\n",
        "    pipelines = [\n",
        "        (\"LogReg\", make_pipeline(SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "                                 StandardScaler(with_mean=False),\n",
        "                                 LogisticRegression(max_iter=3000, class_weight=\"balanced\"))),\n",
        "        (\"RF\",     make_pipeline(SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "                                 StandardScaler(with_mean=False),\n",
        "                                 RandomForestClassifier(n_estimators=400, random_state=42,\n",
        "                                                        class_weight=\"balanced_subsample\"))),\n",
        "        (\"HGB\",    make_pipeline(SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "                                 StandardScaler(with_mean=False),\n",
        "                                 HistGradientBoostingClassifier(random_state=42)))\n",
        "    ]\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    out_rows = []\n",
        "    for mname, pipe in pipelines:\n",
        "        aucs, aps, bals, f1s = [], [], [], []\n",
        "        for tr, te in skf.split(X, y):\n",
        "            pipe.fit(X.iloc[tr], y[tr])\n",
        "            # probability or decision function\n",
        "            if hasattr(pipe[-1], \"predict_proba\"):\n",
        "                proba = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "            else:\n",
        "                proba = pipe.decision_function(X.iloc[te])\n",
        "            pred  = (proba >= 0.5).astype(int)\n",
        "            aucs.append(roc_auc_score(y[te], proba))\n",
        "            aps.append(average_precision_score(y[te], proba))\n",
        "            bals.append(balanced_accuracy_score(y[te], pred))\n",
        "            f1s.append(f1_score(y[te], pred))\n",
        "        out_rows.append(dict(\n",
        "            block=name, model=mname,\n",
        "            AUC=float(np.mean(aucs)),  AUC_std=float(np.std(aucs)),\n",
        "            AP=float(np.mean(aps)),    AP_std=float(np.std(aps)),\n",
        "            BalAcc=float(np.mean(bals)), F1=float(np.mean(f1s))\n",
        "        ))\n",
        "    return out_rows\n",
        "\n",
        "results = []\n",
        "results += eval_block(TOPO_ONLY, \"Topo-only\")\n",
        "results += eval_block(RT_BLOCK,  \"RoundTrip-only\")\n",
        "results += eval_block(COMBINED,  \"Combined\")\n",
        "\n",
        "res_df = pd.DataFrame(results).round(3)\n",
        "res_df\n",
        "res_df.to_csv(OUT_DIR / \"clf_with_roundtrip.csv\", index=False)\n",
        "\n",
        "# Also dump which columns were used\n",
        "pd.Series(TOPO_ONLY, name=\"TOPO_ONLY\").to_csv(OUT_DIR/\"features_topo_only.txt\", index=False)\n",
        "pd.Series(RT_BLOCK,  name=\"RT_BLOCK\").to_csv(OUT_DIR/\"features_roundtrip.txt\", index=False)\n",
        "pd.Series(COMBINED,  name=\"COMBINED\").to_csv(OUT_DIR/\"features_combined.txt\", index=False)\n",
        "\n",
        "print(\"WROTE:\", OUT_DIR / \"clf_with_roundtrip.csv\")\n",
        "print(\"Round-trip features used:\", RT_BLOCK)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Trl-AeIhlW2b",
        "outputId": "9a973e8b-d286-470b-f3e9-308b3788fa53"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WROTE: /content/drive/MyDrive/graph_stats_out/clf_with_roundtrip.csv\n",
            "Round-trip features used: ['rt_time_median', 'rt_hops_median', 'rt_completion_rate']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === K-fold CV + Confusion Matrices (Combined features) ===\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, balanced_accuracy_score, f1_score,\n",
        "    confusion_matrix, precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "ENR = OUT_DIR/\"graph_stats_enriched.csv\"\n",
        "EXT = OUT_DIR/\"graph_stats_extended.csv\"\n",
        "RTS = OUT_DIR/\"roundtrip_summary.csv\"\n",
        "\n",
        "def pick(df, *cands):\n",
        "    for c in df.columns:\n",
        "        for k in cands:\n",
        "            if k.lower()==c.lower() or k.lower() in c.lower():\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# --- load & merge (same logic as before) ---\n",
        "dfx = pd.read_csv(ENR if ENR.exists() else EXT).copy()\n",
        "rt  = pd.read_csv(RTS).copy()\n",
        "\n",
        "file_x = pick(dfx, \"file\",\"path\")\n",
        "file_r = pick(rt,  \"file\",\"path\")\n",
        "if file_x and file_r:\n",
        "    M = dfx.merge(rt, left_on=file_x, right_on=file_r, how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "else:\n",
        "    b_x = pick(dfx, \"bundle\"); b_r = pick(rt, \"bundle\")\n",
        "    c_x = pick(dfx, \"case_id\",\"case\"); c_r = pick(rt, \"case_id\",\"case\")\n",
        "    M = dfx.merge(rt, left_on=[b_x,c_x], right_on=[b_r,c_r], how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "\n",
        "# label\n",
        "NORMAL = {\"logs_bundle\",\"logs_bundle2\",\"logs_bundle3\"}\n",
        "ATTACK = {\"Attack_bundle\"}\n",
        "bcol = pick(M, \"bundle\")\n",
        "M[\"cohort\"] = np.where(M[bcol].isin(ATTACK),\"attack\", np.where(M[bcol].isin(NORMAL),\"normal\",\"other\"))\n",
        "M = M[M[\"cohort\"].isin([\"normal\",\"attack\"])].copy()\n",
        "y = (M[\"cohort\"]==\"attack\").astype(int).values\n",
        "\n",
        "# round-trip cols\n",
        "rt_time = pick(M, \"rt_time_median\",\"rt_time\",\"roundtrip_time\",\"seconds\",\"duration\")\n",
        "rt_hops = pick(M, \"rt_hops_median\",\"rt_hops\",\"roundtrip_hops\",\"hops\")\n",
        "rt_rate = pick(M, \"rt_completion_rate\",\"completion\",\"success_rate\",\"rt_complete\")\n",
        "if rt_rate is None:\n",
        "    M[\"rt_completion_dummy\"]=1.0\n",
        "    rt_rate=\"rt_completion_dummy\"\n",
        "\n",
        "drop_like = {\"file\",\"path\",\"bundle\",\"cohort\",\"case_id\",\"case\",\"role_counts_json\"}\n",
        "num_cols = [c for c in M.columns if (c not in drop_like) and pd.api.types.is_numeric_dtype(M[c])]\n",
        "RT_BLOCK = [c for c in [rt_time, rt_hops, rt_rate] if c is not None]\n",
        "COMBINED = list(dict.fromkeys(num_cols))  # already includes RT columns from merge\n",
        "\n",
        "# model pipeline (same one that performed best in your table)\n",
        "pipe = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "    StandardScaler(with_mean=False),\n",
        "    LogisticRegression(max_iter=3000, class_weight=\"balanced\")\n",
        ")\n",
        "\n",
        "def best_f1_threshold(y_true, scores):\n",
        "    # sweep thresholds on training scores to maximize F1\n",
        "    thr = 0.5; best = -1\n",
        "    for t in np.linspace(0.05, 0.95, 91):\n",
        "        p = (scores>=t).astype(int)\n",
        "        f1 = f1_score(y_true, p)\n",
        "        if f1>best:\n",
        "            best, thr = f1, t\n",
        "    return thr\n",
        "\n",
        "K = 10  # <-- change if you want a different K\n",
        "X = M[COMBINED].copy()\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "rows = []\n",
        "cms_fixed = np.zeros((2,2), dtype=float)\n",
        "cms_tuned = np.zeros((2,2), dtype=float)\n",
        "\n",
        "fold = 0\n",
        "for tr, te in skf.split(X, y):\n",
        "    fold += 1\n",
        "    pipe.fit(X.iloc[tr], y[tr])\n",
        "    if hasattr(pipe[-1], \"predict_proba\"):\n",
        "        s_tr = pipe.predict_proba(X.iloc[tr])[:,1]\n",
        "        s_te = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "    else:\n",
        "        s_tr = pipe.decision_function(X.iloc[tr])\n",
        "        s_te = pipe.decision_function(X.iloc[te])\n",
        "\n",
        "    # metrics (probabilistic)\n",
        "    auc = roc_auc_score(y[te], s_te)\n",
        "    ap  = average_precision_score(y[te], s_te)\n",
        "\n",
        "    # fixed 0.5\n",
        "    p05 = (s_te>=0.5).astype(int)\n",
        "    bal05 = balanced_accuracy_score(y[te], p05)\n",
        "    f105  = f1_score(y[te], p05)\n",
        "    cm05  = confusion_matrix(y[te], p05, labels=[0,1])\n",
        "    cms_fixed += cm05\n",
        "\n",
        "    # tuned threshold for F1 using training fold only\n",
        "    thr = best_f1_threshold(y[tr], s_tr)\n",
        "    pt  = (s_te>=thr).astype(int)\n",
        "    balt = balanced_accuracy_score(y[te], pt)\n",
        "    f1t  = f1_score(y[te], pt)\n",
        "    cmt  = confusion_matrix(y[te], pt, labels=[0,1])\n",
        "    cms_tuned += cmt\n",
        "\n",
        "    rows.append(dict(\n",
        "        fold=fold, AUC=auc, AP=ap,\n",
        "        BalAcc_fixed=bal05, F1_fixed=f105, thr_tuned=thr,\n",
        "        BalAcc_tuned=balt, F1_tuned=f1t,\n",
        "        TN_fixed=int(cm05[0,0]), FP_fixed=int(cm05[0,1]),\n",
        "        FN_fixed=int(cm05[1,0]), TP_fixed=int(cm05[1,1]),\n",
        "        TN_tuned=int(cmt[0,0]), FP_tuned=int(cmt[0,1]),\n",
        "        FN_tuned=int(cmt[1,0]), TP_tuned=int(cmt[1,1]),\n",
        "    ))\n",
        "\n",
        "cv_df = pd.DataFrame(rows).round(3)\n",
        "cv_df.to_csv(OUT_DIR/\"cv10_combined_logreg_folds.csv\", index=False)\n",
        "\n",
        "# mean/std summary\n",
        "summary = pd.DataFrame({\n",
        "    \"AUC_mean\":[cv_df[\"AUC\"].mean()], \"AUC_std\":[cv_df[\"AUC\"].std()],\n",
        "    \"AP_mean\":[cv_df[\"AP\"].mean()],   \"AP_std\":[cv_df[\"AP\"].std()],\n",
        "    \"BalAcc_fixed_mean\":[cv_df[\"BalAcc_fixed\"].mean()],\n",
        "    \"F1_fixed_mean\":[cv_df[\"F1_fixed\"].mean()],\n",
        "    \"BalAcc_tuned_mean\":[cv_df[\"BalAcc_tuned\"].mean()],\n",
        "    \"F1_tuned_mean\":[cv_df[\"F1_tuned\"].mean()],\n",
        "})\n",
        "summary = summary.round(3)\n",
        "summary.to_csv(OUT_DIR/\"cv10_combined_logreg_summary.csv\", index=False)\n",
        "\n",
        "# average confusion matrices\n",
        "cms_fixed /= K\n",
        "cms_tuned /= K\n",
        "np.savetxt(OUT_DIR/\"cm_fixed_mean.txt\", cms_fixed, fmt=\"%.2f\")\n",
        "np.savetxt(OUT_DIR/\"cm_tuned_mean.txt\", cms_tuned, fmt=\"%.2f\")\n",
        "\n",
        "print(\"Saved:\")\n",
        "print(\" -\", OUT_DIR/\"cv10_combined_logreg_folds.csv\")\n",
        "print(\" -\", OUT_DIR/\"cv10_combined_logreg_summary.csv\")\n",
        "print(\" -\", OUT_DIR/\"cm_fixed_mean.txt\")\n",
        "print(\" -\", OUT_DIR/\"cm_tuned_mean.txt\")\n",
        "\n",
        "# Optional: quick heatmap figure for mean confusion matrices\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_cm(cm, title, path):\n",
        "    plt.figure(figsize=(4,3.5))\n",
        "    plt.imshow(cm, interpolation=\"nearest\")\n",
        "    plt.xticks([0,1], [\"Pred Normal\",\"Pred Attack\"])\n",
        "    plt.yticks([0,1], [\"True Normal\",\"True Attack\"])\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, f\"{cm[i,j]:.1f}\", ha=\"center\", va=\"center\")\n",
        "    plt.title(title); plt.tight_layout(); plt.savefig(path, dpi=160); plt.close()\n",
        "\n",
        "plot_cm(cms_fixed, \"Mean Confusion Matrix (thr=0.5, K=10)\", OUT_DIR/\"cm_fixed_mean.png\")\n",
        "plot_cm(cms_tuned, \"Mean Confusion Matrix (F1-tuned thr, K=10)\", OUT_DIR/\"cm_tuned_mean.png\")\n",
        "print(\" -\", OUT_DIR/\"cm_fixed_mean.png\")\n",
        "print(\" -\", OUT_DIR/\"cm_tuned_mean.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LD9mHUZ3lW7X",
        "outputId": "0320c7e9-3f93-47bd-fba2-f2f26831565b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved:\n",
            " - /content/drive/MyDrive/graph_stats_out/cv10_combined_logreg_folds.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/cv10_combined_logreg_summary.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/cm_fixed_mean.txt\n",
            " - /content/drive/MyDrive/graph_stats_out/cm_tuned_mean.txt\n",
            " - /content/drive/MyDrive/graph_stats_out/cm_fixed_mean.png\n",
            " - /content/drive/MyDrive/graph_stats_out/cm_tuned_mean.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold CV (default K=10) + printed confusion matrices (no saving)\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, balanced_accuracy_score, f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "ENR = OUT_DIR/\"graph_stats_enriched.csv\"\n",
        "EXT = OUT_DIR/\"graph_stats_extended.csv\"\n",
        "RTS = OUT_DIR/\"roundtrip_summary.csv\"\n",
        "\n",
        "def pick(df, *cands):\n",
        "    for c in df.columns:\n",
        "        lc = c.lower()\n",
        "        for k in cands:\n",
        "            if k.lower()==lc or k.lower() in lc:\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# -- load feature table (enriched if exists) and roundtrip summary; merge --\n",
        "dfx = pd.read_csv(ENR if ENR.exists() else EXT).copy()\n",
        "rt  = pd.read_csv(RTS).copy()\n",
        "\n",
        "file_x = pick(dfx, \"file\",\"path\")\n",
        "file_r = pick(rt,  \"file\",\"path\")\n",
        "if file_x and file_r:\n",
        "    M = dfx.merge(rt, left_on=file_x, right_on=file_r, how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "else:\n",
        "    b_x = pick(dfx, \"bundle\"); b_r = pick(rt, \"bundle\")\n",
        "    c_x = pick(dfx, \"case_id\",\"case\"); c_r = pick(rt, \"case_id\",\"case\")\n",
        "    M = dfx.merge(rt, left_on=[b_x,c_x], right_on=[b_r,c_r], how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "\n",
        "# -- labels (attack vs normal) --\n",
        "NORMAL = {\"logs_bundle\",\"logs_bundle2\",\"logs_bundle3\"}\n",
        "ATTACK = {\"Attack_bundle\"}\n",
        "bcol = pick(M, \"bundle\")\n",
        "M[\"cohort\"] = np.where(M[bcol].isin(ATTACK),\"attack\", np.where(M[bcol].isin(NORMAL),\"normal\",\"other\"))\n",
        "M = M[M[\"cohort\"].isin([\"normal\",\"attack\"])].copy()\n",
        "y = (M[\"cohort\"]==\"attack\").astype(int).values\n",
        "\n",
        "# -- roundtrip columns (robust names) --\n",
        "rt_time = pick(M, \"rt_time_median\",\"rt_time\",\"roundtrip_time\",\"seconds\",\"duration\")\n",
        "rt_hops = pick(M, \"rt_hops_median\",\"rt_hops\",\"roundtrip_hops\",\"hops\")\n",
        "rt_rate = pick(M, \"rt_completion_rate\",\"completion\",\"success_rate\",\"rt_complete\")\n",
        "if rt_rate is None:\n",
        "    # if your per-graph summary had no completion-rate, create a harmless constant\n",
        "    M[\"rt_completion_dummy\"] = 1.0\n",
        "    rt_rate = \"rt_completion_dummy\"\n",
        "\n",
        "# -- Combined feature block (all numeric columns except IDs/labels) --\n",
        "drop_like = {\"file\",\"path\",\"bundle\",\"cohort\",\"case_id\",\"case\",\"role_counts_json\"}\n",
        "num_cols = [c for c in M.columns if (c not in drop_like) and pd.api.types.is_numeric_dtype(M[c])]\n",
        "X = M[num_cols].copy()\n",
        "\n",
        "# -- model (LogReg matched to your best row) --\n",
        "pipe = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "    StandardScaler(with_mean=False),\n",
        "    LogisticRegression(max_iter=3000, class_weight=\"balanced\")\n",
        ")\n",
        "\n",
        "def best_f1_threshold(y_true, scores):\n",
        "    thr = 0.5; best = -1\n",
        "    for t in np.linspace(0.05, 0.95, 91):\n",
        "        f1 = f1_score(y_true, (scores>=t).astype(int))\n",
        "        if f1 > best:\n",
        "            best, thr = f1, t\n",
        "    return thr\n",
        "\n",
        "K = 10\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "rows = []\n",
        "cm_fixed_sum = np.zeros((2,2), dtype=float)\n",
        "cm_tuned_sum = np.zeros((2,2), dtype=float)\n",
        "\n",
        "for fold, (tr, te) in enumerate(skf.split(X, y), start=1):\n",
        "    pipe.fit(X.iloc[tr], y[tr])\n",
        "    if hasattr(pipe[-1], \"predict_proba\"):\n",
        "        s_tr = pipe.predict_proba(X.iloc[tr])[:,1]\n",
        "        s_te = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "    else:\n",
        "        s_tr = pipe.decision_function(X.iloc[tr])\n",
        "        s_te = pipe.decision_function(X.iloc[te])\n",
        "\n",
        "    # probabilistic metrics\n",
        "    auc = roc_auc_score(y[te], s_te)\n",
        "    ap  = average_precision_score(y[te], s_te)\n",
        "\n",
        "    # fixed 0.5 threshold\n",
        "    p05  = (s_te>=0.5).astype(int)\n",
        "    bal05= balanced_accuracy_score(y[te], p05)\n",
        "    f105 = f1_score(y[te], p05)\n",
        "    cm05 = confusion_matrix(y[te], p05, labels=[0,1])\n",
        "    cm_fixed_sum += cm05\n",
        "\n",
        "    # tuned threshold on training fold (maximize F1)\n",
        "    thr = best_f1_threshold(y[tr], s_tr)\n",
        "    pt  = (s_te>=thr).astype(int)\n",
        "    balt= balanced_accuracy_score(y[te], pt)\n",
        "    f1t = f1_score(y[te], pt)\n",
        "    cmt = confusion_matrix(y[te], pt, labels=[0,1])\n",
        "    cm_tuned_sum += cmt\n",
        "\n",
        "    rows.append(dict(\n",
        "        fold=fold, AUC=auc, AP=ap,\n",
        "        BalAcc_fixed=bal05, F1_fixed=f105,\n",
        "        thr_tuned=thr, BalAcc_tuned=balt, F1_tuned=f1t,\n",
        "        TN_fixed=int(cm05[0,0]), FP_fixed=int(cm05[0,1]),\n",
        "        FN_fixed=int(cm05[1,0]), TP_fixed=int(cm05[1,1]),\n",
        "        TN_tuned=int(cmt[0,0]), FP_tuned=int(cmt[0,1]),\n",
        "        FN_tuned=int(cmt[1,0]), TP_tuned=int(cmt[1,1]),\n",
        "    ))\n",
        "\n",
        "cv = pd.DataFrame(rows).round(3)\n",
        "\n",
        "# === PRINT RESULTS ===\n",
        "print(\"\\n=== Per-fold metrics (LogReg, Combined features, K=%d) ===\" % K)\n",
        "print(cv[[\"fold\",\"AUC\",\"AP\",\"BalAcc_fixed\",\"F1_fixed\",\"thr_tuned\",\"BalAcc_tuned\",\"F1_tuned\"]].to_string(index=False))\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"AUC_mean\":[cv[\"AUC\"].mean()], \"AUC_std\":[cv[\"AUC\"].std()],\n",
        "    \"AP_mean\":[cv[\"AP\"].mean()],   \"AP_std\":[cv[\"AP\"].std()],\n",
        "    \"BalAcc_fixed_mean\":[cv[\"BalAcc_fixed\"].mean()],\n",
        "    \"F1_fixed_mean\":[cv[\"F1_fixed\"].mean()],\n",
        "    \"BalAcc_tuned_mean\":[cv[\"BalAcc_tuned\"].mean()],\n",
        "    \"F1_tuned_mean\":[cv[\"F1_tuned\"].mean()],\n",
        "}).round(3)\n",
        "\n",
        "print(\"\\n=== Mean ± Std across folds ===\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# Average confusion matrices (over folds)\n",
        "cm_fixed_mean = (cm_fixed_sum / K)\n",
        "cm_tuned_mean = (cm_tuned_sum / K)\n",
        "\n",
        "def fmt_cm(cm, title):\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "    df = pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"])\n",
        "    # also print rates\n",
        "    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "    tpr = tp / (tp+fn) if (tp+fn)>0 else np.nan   # recall attack\n",
        "    tnr = tn / (tn+fp) if (tn+fp)>0 else np.nan   # recall normal\n",
        "    print(df.round(1).to_string())\n",
        "    print(f\"Sensitivity (attack recall): {tpr:.3f}  |  Specificity (normal recall): {tnr:.3f}\")\n",
        "\n",
        "fmt_cm(cm_fixed_mean, \"Mean Confusion Matrix (threshold = 0.50)\")\n",
        "fmt_cm(cm_tuned_mean, \"Mean Confusion Matrix (F1-tuned threshold)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5jhXk9b0lW98",
        "outputId": "376c6e68-81c6-4ca9-8249-33b8159a8aaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Per-fold metrics (LogReg, Combined features, K=10) ===\n",
            " fold   AUC    AP  BalAcc_fixed  F1_fixed  thr_tuned  BalAcc_tuned  F1_tuned\n",
            "    1 0.936 0.953         0.888     0.873       0.49         0.874     0.861\n",
            "    2 0.877 0.924         0.848     0.833       0.49         0.848     0.833\n",
            "    3 0.751 0.835         0.748     0.688       0.50         0.748     0.688\n",
            "    4 0.828 0.888         0.825     0.788       0.55         0.812     0.769\n",
            "    5 0.845 0.888         0.782     0.767       0.49         0.782     0.767\n",
            "    6 0.867 0.902         0.821     0.811       0.56         0.796     0.778\n",
            "    7 0.821 0.889         0.794     0.789       0.54         0.809     0.789\n",
            "    8 0.825 0.863         0.768     0.727       0.50         0.768     0.727\n",
            "    9 0.771 0.852         0.781     0.738       0.49         0.781     0.738\n",
            "   10 0.883 0.920         0.845     0.833       0.50         0.845     0.833\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  BalAcc_fixed_mean  F1_fixed_mean  BalAcc_tuned_mean  F1_tuned_mean\n",
            "     0.84    0.054    0.891   0.036               0.81          0.785              0.806          0.778\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.6          2.7\n",
            "True Attack         12.2         27.5\n",
            "Sensitivity (attack recall): 0.693  |  Specificity (normal recall): 0.928\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.8          2.5\n",
            "True Attack         12.7         27.0\n",
            "Sensitivity (attack recall): 0.680  |  Specificity (normal recall): 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# K-fold CV (default K=10) with full printed metrics (no files saved)\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "ENR = OUT_DIR/\"graph_stats_enriched.csv\"\n",
        "EXT = OUT_DIR/\"graph_stats_extended.csv\"\n",
        "RTS = OUT_DIR/\"roundtrip_summary.csv\"\n",
        "\n",
        "def pick(df, *cands):\n",
        "    cols = list(df.columns)\n",
        "    for k in cands:\n",
        "        for c in cols:\n",
        "            if k.lower()==c.lower() or k.lower() in c.lower():\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# Load + merge\n",
        "dfx = pd.read_csv(ENR if ENR.exists() else EXT).copy()\n",
        "rt  = pd.read_csv(RTS).copy()\n",
        "\n",
        "file_x = pick(dfx, \"file\",\"path\"); file_r = pick(rt, \"file\",\"path\")\n",
        "if file_x and file_r:\n",
        "    M = dfx.merge(rt, left_on=file_x, right_on=file_r, how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "else:\n",
        "    b_x = pick(dfx,\"bundle\"); b_r = pick(rt,\"bundle\")\n",
        "    c_x = pick(dfx,\"case_id\",\"case\"); c_r = pick(rt,\"case_id\",\"case\")\n",
        "    M = dfx.merge(rt, left_on=[b_x,c_x], right_on=[b_r,c_r], how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "\n",
        "# Labels\n",
        "NORMAL = {\"logs_bundle\",\"logs_bundle2\",\"logs_bundle3\"}\n",
        "ATTACK = {\"Attack_bundle\"}\n",
        "bcol = pick(M,\"bundle\")\n",
        "M[\"cohort\"] = np.where(M[bcol].isin(ATTACK),\"attack\", np.where(M[bcol].isin(NORMAL),\"normal\",\"other\"))\n",
        "M = M[M[\"cohort\"].isin([\"normal\",\"attack\"])].copy()\n",
        "y = (M[\"cohort\"]==\"attack\").astype(int).values\n",
        "\n",
        "# Round-trip cols (robust names)\n",
        "rt_time = pick(M,\"rt_time_median\",\"rt_time\",\"roundtrip_time\",\"seconds\",\"duration\")\n",
        "rt_hops = pick(M,\"rt_hops_median\",\"rt_hops\",\"roundtrip_hops\",\"hops\")\n",
        "rt_rate = pick(M,\"rt_completion_rate\",\"completion\",\"success_rate\",\"rt_complete\")\n",
        "if rt_rate is None:\n",
        "    M[\"rt_completion_dummy\"] = 1.0\n",
        "    rt_rate = \"rt_completion_dummy\"\n",
        "\n",
        "# Feature block (all numeric, drop IDs/labels/json)\n",
        "drop_like = {\"file\",\"path\",\"bundle\",\"cohort\",\"case_id\",\"case\",\"role_counts_json\"}\n",
        "num_cols = [c for c in M.columns if (c not in drop_like) and pd.api.types.is_numeric_dtype(M[c])]\n",
        "X = M[num_cols].copy()\n",
        "\n",
        "# Model\n",
        "pipe = make_pipeline(\n",
        "    SimpleImputer(strategy=\"median\", add_indicator=True),\n",
        "    StandardScaler(with_mean=False),\n",
        "    LogisticRegression(max_iter=3000, class_weight=\"balanced\")\n",
        ")\n",
        "\n",
        "def best_f1_threshold(y_true, scores):\n",
        "    best, thr = -1.0, 0.5\n",
        "    for t in np.linspace(0.05, 0.95, 91):\n",
        "        f1 = f1_score(y_true, (scores>=t).astype(int))\n",
        "        if f1 > best:\n",
        "            best, thr = f1, t\n",
        "    return thr\n",
        "\n",
        "K = 10\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "rows = []\n",
        "cm_fixed_sum = np.zeros((2,2), dtype=float)\n",
        "cm_tuned_sum = np.zeros((2,2), dtype=float)\n",
        "\n",
        "for fold, (tr, te) in enumerate(skf.split(X, y), start=1):\n",
        "    pipe.fit(X.iloc[tr], y[tr])\n",
        "    if hasattr(pipe[-1], \"predict_proba\"):\n",
        "        s_tr = pipe.predict_proba(X.iloc[tr])[:,1]\n",
        "        s_te = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "    else:\n",
        "        s_tr = pipe.decision_function(X.iloc[tr])\n",
        "        s_te = pipe.decision_function(X.iloc[te])\n",
        "\n",
        "    # Probabilistic metrics\n",
        "    auc = roc_auc_score(y[te], s_te)\n",
        "    ap  = average_precision_score(y[te], s_te)\n",
        "\n",
        "    # --- fixed 0.50 ---\n",
        "    p05 = (s_te>=0.5).astype(int)\n",
        "    acc05 = accuracy_score(y[te], p05)\n",
        "    prec05 = precision_score(y[te], p05, zero_division=0)\n",
        "    rec05  = recall_score(y[te], p05)\n",
        "    f105   = f1_score(y[te], p05)\n",
        "    bal05  = balanced_accuracy_score(y[te], p05)\n",
        "    cm05   = confusion_matrix(y[te], p05, labels=[0,1])\n",
        "    cm_fixed_sum += cm05\n",
        "\n",
        "    # --- tuned on training to maximize F1 ---\n",
        "    thr = best_f1_threshold(y[tr], s_tr)\n",
        "    pt  = (s_te>=thr).astype(int)\n",
        "    acct = accuracy_score(y[te], pt)\n",
        "    prect= precision_score(y[te], pt, zero_division=0)\n",
        "    rect = recall_score(y[te], pt)\n",
        "    f1t  = f1_score(y[te], pt)\n",
        "    balt = balanced_accuracy_score(y[te], pt)\n",
        "    cmt  = confusion_matrix(y[te], pt, labels=[0,1])\n",
        "    cm_tuned_sum += cmt\n",
        "\n",
        "    rows.append(dict(\n",
        "        fold=fold, AUC=auc, AP=ap,\n",
        "        Acc_fixed=acc05, Prec_fixed=prec05, Rec_fixed=rec05, F1_fixed=f105, BalAcc_fixed=bal05,\n",
        "        thr_tuned=thr,\n",
        "        Acc_tuned=acct, Prec_tuned=prect, Rec_tuned=rect, F1_tuned=f1t, BalAcc_tuned=balt\n",
        "    ))\n",
        "\n",
        "cv = pd.DataFrame(rows).round(3)\n",
        "\n",
        "# === PRINT ===\n",
        "print(\"\\n=== Per-fold metrics (LogReg, Combined features, K=%d) ===\" % K)\n",
        "print(cv[[\"fold\",\"AUC\",\"AP\",\n",
        "          \"Acc_fixed\",\"Prec_fixed\",\"Rec_fixed\",\"F1_fixed\",\"BalAcc_fixed\",\n",
        "          \"thr_tuned\",\"Acc_tuned\",\"Prec_tuned\",\"Rec_tuned\",\"F1_tuned\",\"BalAcc_tuned\"]]\n",
        "      .to_string(index=False))\n",
        "\n",
        "summary = pd.DataFrame({\n",
        "    \"AUC_mean\":[cv[\"AUC\"].mean()], \"AUC_std\":[cv[\"AUC\"].std()],\n",
        "    \"AP_mean\":[cv[\"AP\"].mean()],   \"AP_std\":[cv[\"AP\"].std()],\n",
        "    \"Acc_fixed_mean\":[cv[\"Acc_fixed\"].mean()],   \"Acc_fixed_std\":[cv[\"Acc_fixed\"].std()],\n",
        "    \"Prec_fixed_mean\":[cv[\"Prec_fixed\"].mean()], \"Prec_fixed_std\":[cv[\"Prec_fixed\"].std()],\n",
        "    \"Rec_fixed_mean\":[cv[\"Rec_fixed\"].mean()],   \"Rec_fixed_std\":[cv[\"Rec_fixed\"].std()],\n",
        "    \"F1_fixed_mean\":[cv[\"F1_fixed\"].mean()],     \"F1_fixed_std\":[cv[\"F1_fixed\"].std()],\n",
        "    \"BalAcc_fixed_mean\":[cv[\"BalAcc_fixed\"].mean()],\n",
        "    \"Acc_tuned_mean\":[cv[\"Acc_tuned\"].mean()],   \"Acc_tuned_std\":[cv[\"Acc_tuned\"].std()],\n",
        "    \"Prec_tuned_mean\":[cv[\"Prec_tuned\"].mean()], \"Prec_tuned_std\":[cv[\"Prec_tuned\"].std()],\n",
        "    \"Rec_tuned_mean\":[cv[\"Rec_tuned\"].mean()],   \"Rec_tuned_std\":[cv[\"Rec_tuned\"].std()],\n",
        "    \"F1_tuned_mean\":[cv[\"F1_tuned\"].mean()],     \"F1_tuned_std\":[cv[\"F1_tuned\"].std()],\n",
        "    \"BalAcc_tuned_mean\":[cv[\"BalAcc_tuned\"].mean()],\n",
        "}).round(3)\n",
        "\n",
        "print(\"\\n=== Mean ± Std across folds ===\")\n",
        "print(summary.to_string(index=False))\n",
        "\n",
        "# Mean confusion matrices\n",
        "cm_fixed_mean = (cm_fixed_sum / K)\n",
        "cm_tuned_mean = (cm_tuned_sum / K)\n",
        "\n",
        "def fmt_cm(cm, title):\n",
        "    print(f\"\\n=== {title} ===\")\n",
        "    print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "    df = pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"])\n",
        "    tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "    sens = tp/(tp+fn) if (tp+fn)>0 else np.nan   # recall (attack)\n",
        "    spec = tn/(tn+fp) if (tn+fp)>0 else np.nan   # recall (normal)\n",
        "    prec = tp/(tp+fp) if (tp+fp)>0 else np.nan   # precision (attack)\n",
        "    acc  = (tp+tn)/np.sum(cm) if np.sum(cm)>0 else np.nan\n",
        "    print(df.round(1).to_string())\n",
        "    print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall/Sensitivity (attack): {sens:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "fmt_cm(cm_fixed_mean, \"Mean Confusion Matrix (threshold = 0.50)\")\n",
        "fmt_cm(cm_tuned_mean, \"Mean Confusion Matrix (F1-tuned threshold)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-AB7egCQlXAt",
        "outputId": "8df0b913-ba8f-4333-d3c9-23ddb55758de"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['pl_in_R2' 'pl_out_R2' 'richclub_k5' 'richclub_k10' 'richclub_k20']. At least one non-missing value is needed for imputation with strategy='median'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Per-fold metrics (LogReg, Combined features, K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.936 0.953      0.883       1.000      0.775     0.873         0.888       0.49      0.870       0.969      0.775     0.861         0.874\n",
            "    2 0.877 0.924      0.844       0.938      0.750     0.833         0.848       0.49      0.844       0.938      0.750     0.833         0.848\n",
            "    3 0.751 0.835      0.740       0.917      0.550     0.688         0.748       0.50      0.740       0.917      0.550     0.688         0.748\n",
            "    4 0.828 0.888      0.818       1.000      0.650     0.788         0.825       0.55      0.805       1.000      0.625     0.769         0.812\n",
            "    5 0.845 0.888      0.779       0.848      0.700     0.767         0.782       0.49      0.779       0.848      0.700     0.767         0.782\n",
            "    6 0.867 0.902      0.818       0.882      0.750     0.811         0.821       0.56      0.792       0.875      0.700     0.778         0.796\n",
            "    7 0.821 0.889      0.792       0.833      0.750     0.789         0.794       0.54      0.805       0.903      0.700     0.789         0.809\n",
            "    8 0.825 0.863      0.766       0.889      0.615     0.727         0.768       0.50      0.766       0.889      0.615     0.727         0.768\n",
            "    9 0.771 0.852      0.779       0.923      0.615     0.738         0.781       0.49      0.779       0.923      0.615     0.738         0.781\n",
            "   10 0.883 0.920      0.844       0.909      0.769     0.833         0.845       0.50      0.844       0.909      0.769     0.833         0.845\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "     0.84    0.054    0.891   0.036           0.806          0.043            0.914           0.056           0.692          0.079          0.785         0.056               0.81           0.802           0.04            0.917           0.044            0.68          0.076          0.778         0.053              0.806\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.6          2.7\n",
            "True Attack         12.2         27.5\n",
            "Accuracy: 0.806 | Precision (attack): 0.911 | Recall/Sensitivity (attack): 0.693 | Specificity (normal): 0.928\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.8          2.5\n",
            "True Attack         12.7         27.0\n",
            "Accuracy: 0.803 | Precision (attack): 0.915 | Recall/Sensitivity (attack): 0.680 | Specificity (normal): 0.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Attack-vs-Normal classification with full metrics (prints only) ===\n",
        "# Builds features from whichever CSVs you already wrote, then runs 10-fold CV.\n",
        "# Requires: scikit-learn, pandas, numpy.\n",
        "\n",
        "import os, json, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, balanced_accuracy_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "# ---------- 1) Load & merge features (robust to what's available) ----------\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "\n",
        "# pick a base graph table\n",
        "base = None\n",
        "for cand in [\"graph_stats_enriched.csv\", \"graph_stats_rich.csv\", \"graph_stats_extended.csv\", \"graph_stats.csv\"]:\n",
        "    p = OUT_DIR / cand\n",
        "    if p.exists():\n",
        "        base = pd.read_csv(p)\n",
        "        break\n",
        "if base is None:\n",
        "    raise FileNotFoundError(\"No base graph CSV found in graph_stats_out.\")\n",
        "\n",
        "# optional grey-smart features\n",
        "grey = OUT_DIR / \"graph_stats_greysmart.csv\"\n",
        "if grey.exists():\n",
        "    df_grey = pd.read_csv(grey)\n",
        "    # merge on 'file' if present; else on ['bundle','case_id']\n",
        "    on_cols = [\"file\"] if \"file\" in base.columns and \"file\" in df_grey.columns else [\"bundle\",\"case_id\"]\n",
        "    base = base.merge(df_grey, on=on_cols, how=\"left\")\n",
        "\n",
        "# optional round-trip summary (one row per graph/prompt root)\n",
        "rt = OUT_DIR / \"roundtrip_summary.csv\"\n",
        "if rt.exists():\n",
        "    df_rt = pd.read_csv(rt)\n",
        "    # best-effort join key discovery\n",
        "    join_on = None\n",
        "    for key in [\"file\",\"case_id\",\"root_id\",\"root\",\"start_u\"]:\n",
        "        if key in base.columns and key in df_rt.columns:\n",
        "            join_on = key; break\n",
        "    if join_on is None:\n",
        "        # worst-case, join on bundle+case_id if both exist\n",
        "        if {\"bundle\",\"case_id\"}.issubset(base.columns) and {\"bundle\",\"case_id\"}.issubset(df_rt.columns):\n",
        "            join_on = [\"bundle\",\"case_id\"]\n",
        "    if join_on is not None:\n",
        "        base = base.merge(df_rt, on=join_on, how=\"left\")\n",
        "\n",
        "# label: attack vs normal (prefer 'cohort' if present)\n",
        "if \"cohort\" in base.columns:\n",
        "    y = (base[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "else:\n",
        "    y = (base[\"bundle\"].astype(str) == \"Attack_bundle\").astype(int).values\n",
        "\n",
        "# feature selection: drop IDs & non-numeric\n",
        "drop_like = {\"file\",\"bundle\",\"case_id\",\"role_counts_json\",\"cohort\"}\n",
        "X = base.select_dtypes(include=[np.number]).copy()\n",
        "# remove constant/all-NaN\n",
        "X = X.loc[:, X.notna().any(axis=0)]\n",
        "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
        "# keep track of the column names (for sanity)\n",
        "feat_cols = list(X.columns)\n",
        "\n",
        "# ---------- 2) CV setup ----------\n",
        "K = 10\n",
        "skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "def eval_model(clf, name):\n",
        "    per_fold = []\n",
        "    cms_fixed = []   # confusion matrices at thr=0.50\n",
        "    cms_tuned = []   # confusion matrices at F1-tuned threshold (on test preds; optimistic but simple)\n",
        "\n",
        "    for fold, (tr, te) in enumerate(skf.split(X, y), 1):\n",
        "        # pipeline: impute + scale + model (HGB can handle NaNs, but impute keeps things consistent)\n",
        "        pipe = make_pipeline(\n",
        "            SimpleImputer(strategy=\"median\"),\n",
        "            StandardScaler(with_mean=False),\n",
        "            clf\n",
        "        )\n",
        "        pipe.fit(X.iloc[tr], y[tr])\n",
        "\n",
        "        # probabilities for metrics\n",
        "        # HistGradientBoosting has predict_proba; else decision_function -> sigmoid-ish not guaranteed\n",
        "        if hasattr(pipe[-1], \"predict_proba\"):\n",
        "            p = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "        else:\n",
        "            # fallback\n",
        "            try:\n",
        "                p = pipe.decision_function(X.iloc[te])\n",
        "                # scale to 0..1 if needed\n",
        "                p = (p - p.min()) / (p.max() - p.min() + 1e-9)\n",
        "            except Exception:\n",
        "                p = pipe.predict_proba(X.iloc[te])[:,1]\n",
        "\n",
        "        y_true = y[te]\n",
        "\n",
        "        # fixed threshold = 0.50\n",
        "        y_pred = (p >= 0.50).astype(int)\n",
        "        cm = confusion_matrix(y_true, y_pred, labels=[0,1])  # rows: true [normal, attack]; cols: pred [normal, attack]\n",
        "        cms_fixed.append(cm)\n",
        "\n",
        "        # tuned threshold for max F1 on this test prediction (optimistic but matches earlier printouts)\n",
        "        thr_grid = np.linspace(0.3, 0.7, 41)\n",
        "        f1s = []\n",
        "        for t in thr_grid:\n",
        "            yp = (p >= t).astype(int)\n",
        "            f1s.append(f1_score(y_true, yp, zero_division=0))\n",
        "        t_best = float(thr_grid[int(np.argmax(f1s))])\n",
        "        y_pred_tuned = (p >= t_best).astype(int)\n",
        "        cm_tuned = confusion_matrix(y_true, y_pred_tuned, labels=[0,1])\n",
        "        cms_tuned.append(cm_tuned)\n",
        "\n",
        "        # metrics\n",
        "        auc  = roc_auc_score(y_true, p)\n",
        "        ap   = average_precision_score(y_true, p)\n",
        "        acc  = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "        rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1   = f1_score(y_true, y_pred, zero_division=0)\n",
        "        bal  = balanced_accuracy_score(y_true, y_pred)\n",
        "\n",
        "        acc_t  = accuracy_score(y_true, y_pred_tuned)\n",
        "        prec_t = precision_score(y_true, y_pred_tuned, zero_division=0)\n",
        "        rec_t  = recall_score(y_true, y_pred_tuned, zero_division=0)\n",
        "        f1_t   = f1_score(y_true, y_pred_tuned, zero_division=0)\n",
        "        bal_t  = balanced_accuracy_score(y_true, y_pred_tuned)\n",
        "\n",
        "        per_fold.append(dict(\n",
        "            fold=fold, AUC=auc, AP=ap,\n",
        "            Acc_fixed=acc, Prec_fixed=prec, Rec_fixed=rec, F1_fixed=f1, BalAcc_fixed=bal,\n",
        "            thr_tuned=t_best, Acc_tuned=acc_t, Prec_tuned=prec_t, Rec_tuned=rec_t, F1_tuned=f1_t, BalAcc_tuned=bal_t\n",
        "        ))\n",
        "\n",
        "    df = pd.DataFrame(per_fold).round(3)\n",
        "\n",
        "    # means/stds\n",
        "    cols_meanstd = [\"AUC\",\"AP\",\"Acc_fixed\",\"Prec_fixed\",\"Rec_fixed\",\"F1_fixed\",\"BalAcc_fixed\",\n",
        "                    \"Acc_tuned\",\"Prec_tuned\",\"Rec_tuned\",\"F1_tuned\",\"BalAcc_tuned\"]\n",
        "    mean = df[cols_meanstd].mean().rename(lambda s: f\"{s}_mean\")\n",
        "    std  = df[cols_meanstd].std().rename(lambda s: f\"{s}_std\")\n",
        "    summary = pd.concat([mean, std])\n",
        "\n",
        "    # average confusion matrices\n",
        "    cm_fixed_mean = np.mean(np.stack(cms_fixed, axis=0), axis=0)\n",
        "    cm_tuned_mean = np.mean(np.stack(cms_tuned, axis=0), axis=0)\n",
        "\n",
        "    print(f\"\\n=== {name}: Per-fold metrics (K={K}) ===\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n=== Mean ± Std across folds ===\")\n",
        "    # pretty print a subset you care most about:\n",
        "    keep = [\"AUC_mean\",\"AUC_std\",\"AP_mean\",\"AP_std\",\n",
        "            \"Acc_fixed_mean\",\"Acc_fixed_std\",\n",
        "            \"Prec_fixed_mean\",\"Prec_fixed_std\",\n",
        "            \"Rec_fixed_mean\",\"Rec_fixed_std\",\n",
        "            \"F1_fixed_mean\",\"F1_fixed_std\",\n",
        "            \"BalAcc_fixed_mean\",\n",
        "            \"Acc_tuned_mean\",\"Acc_tuned_std\",\n",
        "            \"Prec_tuned_mean\",\"Prec_tuned_std\",\n",
        "            \"Rec_tuned_mean\",\"Rec_tuned_std\",\n",
        "            \"F1_tuned_mean\",\"F1_tuned_std\",\n",
        "            \"BalAcc_tuned_mean\"]\n",
        "    print(summary[keep].round(3).to_frame().T.to_string(index=False))\n",
        "\n",
        "    def _print_cm(cm, tag):\n",
        "        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "        acc = (tp+tn) / (tn+fp+fn+tp + 1e-9)\n",
        "        prec = tp / (tp+fp + 1e-9)\n",
        "        rec  = tp / (tp+fn + 1e-9)\n",
        "        spec = tn / (tn+fp + 1e-9)\n",
        "        print(f\"\\n=== Mean Confusion Matrix ({tag}) ===\")\n",
        "        print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "        print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "        print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall/Sensitivity (attack): {rec:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "    _print_cm(cm_fixed_mean, \"threshold = 0.50\")\n",
        "    _print_cm(cm_tuned_mean, \"F1-tuned threshold\")\n",
        "\n",
        "    return df, summary, cm_fixed_mean, cm_tuned_mean\n",
        "\n",
        "# ---------- 3) Run models ----------\n",
        "print(f\"Using {len(feat_cols)} features. Positive class = attack (1).\\n\")\n",
        "\n",
        "_ = eval_model(LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\"), \"LogReg\")\n",
        "_ = eval_model(RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\"), \"RF\")\n",
        "_ = eval_model(HistGradientBoostingClassifier(random_state=42), \"HGB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "py-uCp_qlXDS",
        "outputId": "2556fd22-7798-496d-8a96-ea156909e2cc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 161 features. Positive class = attack (1).\n",
            "\n",
            "\n",
            "=== LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.943 0.960      0.870       1.000      0.750     0.857         0.875       0.41      0.909       0.971      0.850     0.907         0.911\n",
            "    2 0.866 0.919      0.831       1.000      0.675     0.806         0.838       0.40      0.857       0.914      0.800     0.853         0.859\n",
            "    3 0.793 0.858      0.753       0.957      0.550     0.698         0.761       0.30      0.714       0.696      0.800     0.744         0.711\n",
            "    4 0.855 0.900      0.818       1.000      0.650     0.788         0.825       0.45      0.818       1.000      0.650     0.788         0.825\n",
            "    5 0.838 0.885      0.792       0.875      0.700     0.778         0.796       0.52      0.818       0.933      0.700     0.800         0.823\n",
            "    6 0.854 0.902      0.805       0.879      0.725     0.795         0.808       0.42      0.844       0.889      0.800     0.842         0.846\n",
            "    7 0.840 0.901      0.831       0.909      0.750     0.822         0.834       0.50      0.831       0.909      0.750     0.822         0.834\n",
            "    8 0.843 0.878      0.779       0.923      0.615     0.738         0.781       0.36      0.753       0.778      0.718     0.747         0.754\n",
            "    9 0.764 0.849      0.805       0.962      0.641     0.769         0.807       0.47      0.805       0.962      0.641     0.769         0.807\n",
            "   10 0.889 0.922      0.857       1.000      0.718     0.836         0.859       0.47      0.870       0.968      0.769     0.857         0.871\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.848    0.049    0.897   0.032           0.814          0.035            0.951           0.051           0.677          0.064          0.789         0.047              0.818           0.822          0.056            0.902           0.095           0.748          0.069          0.813         0.053              0.824\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         35.8          1.5\n",
            "True Attack         12.8         26.9\n",
            "Accuracy: 0.814 | Precision (attack): 0.947 | Recall/Sensitivity (attack): 0.678 | Specificity (normal): 0.960\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.6          3.7\n",
            "True Attack         10.0         29.7\n",
            "Accuracy: 0.822 | Precision (attack): 0.889 | Recall/Sensitivity (attack): 0.748 | Specificity (normal): 0.901\n",
            "\n",
            "=== RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.892 0.930      0.818       0.842      0.800     0.821         0.819       0.53      0.870       0.941      0.800     0.865         0.873\n",
            "    2 0.842 0.895      0.792       0.853      0.725     0.784         0.795       0.49      0.805       0.857      0.750     0.800         0.807\n",
            "    3 0.788 0.850      0.727       0.788      0.650     0.712         0.730       0.69      0.766       0.958      0.575     0.719         0.774\n",
            "    4 0.810 0.880      0.779       0.871      0.675     0.761         0.783       0.36      0.792       0.853      0.725     0.784         0.795\n",
            "    5 0.851 0.884      0.727       0.771      0.675     0.720         0.729       0.32      0.779       0.756      0.850     0.800         0.776\n",
            "    6 0.848 0.869      0.753       0.784      0.725     0.753         0.754       0.30      0.753       0.714      0.875     0.787         0.748\n",
            "    7 0.893 0.927      0.831       0.865      0.800     0.831         0.832       0.46      0.857       0.872      0.850     0.861         0.857\n",
            "    8 0.795 0.862      0.753       0.812      0.667     0.732         0.754       0.63      0.792       0.897      0.667     0.765         0.794\n",
            "    9 0.803 0.863      0.766       0.839      0.667     0.743         0.768       0.54      0.792       0.897      0.667     0.765         0.794\n",
            "   10 0.866 0.916      0.805       0.800      0.821     0.810         0.805       0.60      0.844       0.909      0.769     0.833         0.845\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.839    0.039    0.888   0.028           0.775          0.036            0.823           0.036            0.72          0.065          0.767         0.043              0.777           0.805          0.039            0.865           0.077           0.753          0.096          0.798         0.045              0.806\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.1          6.2\n",
            "True Attack         11.1         28.6\n",
            "Accuracy: 0.775 | Precision (attack): 0.822 | Recall/Sensitivity (attack): 0.720 | Specificity (normal): 0.834\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack          9.8         29.9\n",
            "Accuracy: 0.805 | Precision (attack): 0.852 | Recall/Sensitivity (attack): 0.753 | Specificity (normal): 0.861\n",
            "\n",
            "=== HGB: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.903 0.938      0.883       0.919      0.850     0.883         0.884       0.52      0.896       0.944      0.850     0.895         0.898\n",
            "    2 0.843 0.896      0.766       0.824      0.700     0.757         0.769       0.44      0.766       0.806      0.725     0.763         0.768\n",
            "    3 0.766 0.835      0.675       0.714      0.625     0.667         0.677       0.30      0.662       0.659      0.725     0.690         0.660\n",
            "    4 0.864 0.892      0.727       0.806      0.625     0.704         0.731       0.30      0.740       0.750      0.750     0.750         0.740\n",
            "    5 0.833 0.879      0.766       0.806      0.725     0.763         0.768       0.41      0.792       0.816      0.775     0.795         0.793\n",
            "    6 0.858 0.901      0.818       0.861      0.775     0.816         0.820       0.44      0.818       0.861      0.775     0.816         0.820\n",
            "    7 0.891 0.923      0.831       0.865      0.800     0.831         0.832       0.30      0.831       0.829      0.850     0.840         0.830\n",
            "    8 0.815 0.869      0.740       0.788      0.667     0.722         0.741       0.45      0.766       0.800      0.718     0.757         0.767\n",
            "    9 0.777 0.850      0.740       0.788      0.667     0.722         0.741       0.52      0.766       0.839      0.667     0.743         0.768\n",
            "   10 0.842 0.899      0.805       0.853      0.744     0.795         0.806       0.45      0.831       0.842      0.821     0.831         0.831\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.839    0.044    0.888   0.031           0.775           0.06            0.822           0.056           0.718          0.075          0.766         0.066              0.777           0.787          0.063            0.815           0.074           0.766          0.061          0.788         0.059              0.788\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.2          6.1\n",
            "True Attack         11.2         28.5\n",
            "Accuracy: 0.775 | Precision (attack): 0.824 | Recall/Sensitivity (attack): 0.718 | Specificity (normal): 0.836\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.2          7.1\n",
            "True Attack          9.3         30.4\n",
            "Accuracy: 0.787 | Precision (attack): 0.811 | Recall/Sensitivity (attack): 0.766 | Specificity (normal): 0.810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === List final features used for classification (prints only) ===\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "\n",
        "# 1) Load a base graph table (first one that exists)\n",
        "base_path = None\n",
        "for cand in [\"graph_stats_enriched.csv\",\n",
        "             \"graph_stats_rich.csv\",\n",
        "             \"graph_stats_extended.csv\",\n",
        "             \"graph_stats.csv\"]:\n",
        "    p = OUT_DIR / cand\n",
        "    if p.exists():\n",
        "        base_path = p\n",
        "        break\n",
        "if base_path is None:\n",
        "    raise FileNotFoundError(\"No base CSV found in graph_stats_out.\")\n",
        "base = pd.read_csv(base_path)\n",
        "base[\"_SOURCE_\"] = base_path.name\n",
        "\n",
        "# 2) Optional merges (same as training)\n",
        "dfs = [base]\n",
        "\n",
        "grey_path = OUT_DIR / \"graph_stats_greysmart.csv\"\n",
        "if grey_path.exists():\n",
        "    g = pd.read_csv(grey_path)\n",
        "    g[\"_SOURCE_\"] = grey_path.name\n",
        "    on_cols = [\"file\"] if {\"file\"} <= set(base.columns) and {\"file\"} <= set(g.columns) else [\"bundle\",\"case_id\"]\n",
        "    dfs.append(g)\n",
        "\n",
        "rt_path = OUT_DIR / \"roundtrip_summary.csv\"\n",
        "if rt_path.exists():\n",
        "    r = pd.read_csv(rt_path)\n",
        "    r[\"_SOURCE_\"] = rt_path.name\n",
        "    join_on = None\n",
        "    for key in [\"file\",\"case_id\",\"root_id\",\"root\",\"start_u\"]:\n",
        "        if key in base.columns and key in r.columns:\n",
        "            join_on = key; break\n",
        "    if join_on is None and {\"bundle\",\"case_id\"} <= set(base.columns) and {\"bundle\",\"case_id\"} <= set(r.columns):\n",
        "        join_on = [\"bundle\",\"case_id\"]\n",
        "    if join_on is not None:\n",
        "        # keep a copy to identify RT-only cols later\n",
        "        r_cols = set(r.columns)\n",
        "        merged = base.merge(r, on=join_on, how=\"left\", suffixes=(\"\",\"__rt\"))\n",
        "        merged[\"_SOURCE_\"] = \"MERGED\"\n",
        "        # Replace base with merged\n",
        "        base = merged\n",
        "    else:\n",
        "        # If we can't join, just skip RT features\n",
        "        pass\n",
        "\n",
        "# 3) Build feature matrix just like training: numeric, drop all-NaN and constants\n",
        "X = base.select_dtypes(include=[np.number]).copy()\n",
        "X = X.loc[:, X.notna().any(axis=0)]\n",
        "X = X.loc[:, X.nunique(dropna=True) > 1]\n",
        "\n",
        "# 4) Remove label and obvious IDs if they slipped in\n",
        "drop_like = {\"cohort\"}  # label; (file/bundle/case_id aren't numeric, but just in case)\n",
        "feat_cols = [c for c in X.columns if c not in drop_like]\n",
        "\n",
        "# 5) Try to tag columns by origin\n",
        "def tag(col):\n",
        "    # heuristics by name\n",
        "    n = col.lower()\n",
        "    if \"rt_\" in n or \"roundtrip\" in n or n in {\"rt_hops\",\"rt_time\",\"rt_completion_rate\"}:\n",
        "        return \"roundtrip_summary.csv\"\n",
        "    if \"fanout\" in n or \"crosstalk\" in n or \"alternation\" in n or \"lag_ms\" in n or n.startswith(\"tri_\") or \"spec_\" in n or n.startswith(\"eig\"):\n",
        "        return \"graph_stats_greysmart.csv\"\n",
        "    # default to base\n",
        "    return base_path.name\n",
        "\n",
        "tags = {c: tag(c) for c in feat_cols}\n",
        "\n",
        "# 6) Print results\n",
        "print(f\"\\nTotal features used: {len(feat_cols)}\\n\")\n",
        "\n",
        "groups = {}\n",
        "for c in feat_cols:\n",
        "    groups.setdefault(tags[c], []).append(c)\n",
        "\n",
        "for src in sorted(groups):\n",
        "    print(f\"--- {src} ({len(groups[src])}) ---\")\n",
        "    for c in sorted(groups[src]):\n",
        "        print(\" \", c)\n",
        "    print()\n",
        "\n",
        "# Also print a single flat, alphabetical list if you want to copy into a report\n",
        "print(\"=== Alphabetical feature list ===\")\n",
        "for c in sorted(feat_cols):\n",
        "    print(c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "gxVOwYidlXF1",
        "outputId": "fa664a0f-35d1-40ef-83e3-91b2dd51c80f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total features used: 73\n",
            "\n",
            "--- graph_stats_enriched.csv (67) ---\n",
            "  avg_clustering\n",
            "  density_directed\n",
            "  density_undirected\n",
            "  diameter_undirected_lcc\n",
            "  frac_can_reach_giant\n",
            "  frac_in_largest_wcc_edges\n",
            "  frac_in_largest_wcc_nodes\n",
            "  frac_reachable_from_giant\n",
            "  in_degree_gini\n",
            "  in_degree_kurt\n",
            "  in_degree_max\n",
            "  in_degree_mean\n",
            "  in_degree_skew\n",
            "  kcore_kmax\n",
            "  kcore_kmax_size\n",
            "  largest_wcc_edges\n",
            "  largest_wcc_nodes\n",
            "  n_edges\n",
            "  n_nodes\n",
            "  n_prompts\n",
            "  n_roles\n",
            "  out_degree_gini\n",
            "  out_degree_kurt\n",
            "  out_degree_max\n",
            "  out_degree_mean\n",
            "  out_degree_median\n",
            "  out_degree_skew\n",
            "  pl_out_slope\n",
            "  reciprocity\n",
            "  role_entropy\n",
            "  rolepair_frac::cardiologist->internist\n",
            "  rolepair_frac::cardiologist->moderator\n",
            "  rolepair_frac::internist->cardiologist\n",
            "  rolepair_frac::internist->moderator\n",
            "  rolepair_frac::moderator->cardiologist\n",
            "  rolepair_frac::moderator->internist\n",
            "  rolepair_frac::moderator->radiologist\n",
            "  rolepair_frac::moderator->surgeon\n",
            "  rolepair_frac::radiologist->internist\n",
            "  rolepair_frac::radiologist->moderator\n",
            "  rolepair_frac::surgeon->cardiologist\n",
            "  rolepair_frac::surgeon->moderator\n",
            "  rolepair_pmi::cardiologist->internist\n",
            "  rolepair_pmi::cardiologist->moderator\n",
            "  rolepair_pmi::internist->cardiologist\n",
            "  rolepair_pmi::internist->moderator\n",
            "  rolepair_pmi::moderator->cardiologist\n",
            "  rolepair_pmi::moderator->internist\n",
            "  rolepair_pmi::moderator->radiologist\n",
            "  rolepair_pmi::moderator->surgeon\n",
            "  rolepair_pmi::radiologist->internist\n",
            "  rolepair_pmi::radiologist->moderator\n",
            "  rolepair_pmi::surgeon->cardiologist\n",
            "  rolepair_pmi::surgeon->moderator\n",
            "  roleprop::cardiologist\n",
            "  roleprop::cardiothoracic surgeon\n",
            "  roleprop::internist\n",
            "  roleprop::moderator\n",
            "  roleprop::neonatologist\n",
            "  roleprop::pediatric cardiologist\n",
            "  roleprop::radiologist\n",
            "  roleprop::surgeon\n",
            "  scc_count\n",
            "  simple_edges\n",
            "  transitivity\n",
            "  triangles\n",
            "  wcc_count\n",
            "\n",
            "--- roundtrip_summary.csv (6) ---\n",
            "  assort_in_in\n",
            "  assort_out_out\n",
            "  assort_undirected\n",
            "  rt_time_mean\n",
            "  rt_time_median\n",
            "  rt_time_p95\n",
            "\n",
            "=== Alphabetical feature list ===\n",
            "assort_in_in\n",
            "assort_out_out\n",
            "assort_undirected\n",
            "avg_clustering\n",
            "density_directed\n",
            "density_undirected\n",
            "diameter_undirected_lcc\n",
            "frac_can_reach_giant\n",
            "frac_in_largest_wcc_edges\n",
            "frac_in_largest_wcc_nodes\n",
            "frac_reachable_from_giant\n",
            "in_degree_gini\n",
            "in_degree_kurt\n",
            "in_degree_max\n",
            "in_degree_mean\n",
            "in_degree_skew\n",
            "kcore_kmax\n",
            "kcore_kmax_size\n",
            "largest_wcc_edges\n",
            "largest_wcc_nodes\n",
            "n_edges\n",
            "n_nodes\n",
            "n_prompts\n",
            "n_roles\n",
            "out_degree_gini\n",
            "out_degree_kurt\n",
            "out_degree_max\n",
            "out_degree_mean\n",
            "out_degree_median\n",
            "out_degree_skew\n",
            "pl_out_slope\n",
            "reciprocity\n",
            "role_entropy\n",
            "rolepair_frac::cardiologist->internist\n",
            "rolepair_frac::cardiologist->moderator\n",
            "rolepair_frac::internist->cardiologist\n",
            "rolepair_frac::internist->moderator\n",
            "rolepair_frac::moderator->cardiologist\n",
            "rolepair_frac::moderator->internist\n",
            "rolepair_frac::moderator->radiologist\n",
            "rolepair_frac::moderator->surgeon\n",
            "rolepair_frac::radiologist->internist\n",
            "rolepair_frac::radiologist->moderator\n",
            "rolepair_frac::surgeon->cardiologist\n",
            "rolepair_frac::surgeon->moderator\n",
            "rolepair_pmi::cardiologist->internist\n",
            "rolepair_pmi::cardiologist->moderator\n",
            "rolepair_pmi::internist->cardiologist\n",
            "rolepair_pmi::internist->moderator\n",
            "rolepair_pmi::moderator->cardiologist\n",
            "rolepair_pmi::moderator->internist\n",
            "rolepair_pmi::moderator->radiologist\n",
            "rolepair_pmi::moderator->surgeon\n",
            "rolepair_pmi::radiologist->internist\n",
            "rolepair_pmi::radiologist->moderator\n",
            "rolepair_pmi::surgeon->cardiologist\n",
            "rolepair_pmi::surgeon->moderator\n",
            "roleprop::cardiologist\n",
            "roleprop::cardiothoracic surgeon\n",
            "roleprop::internist\n",
            "roleprop::moderator\n",
            "roleprop::neonatologist\n",
            "roleprop::pediatric cardiologist\n",
            "roleprop::radiologist\n",
            "roleprop::surgeon\n",
            "rt_time_mean\n",
            "rt_time_median\n",
            "rt_time_p95\n",
            "scc_count\n",
            "simple_edges\n",
            "transitivity\n",
            "triangles\n",
            "wcc_count\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-run after state reset: same analysis code to summarize features for attack vs normal.\n",
        "import os, numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "import scipy.stats as st\n",
        "\n",
        "DATA_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "\n",
        "# Load base CSV\n",
        "base = None\n",
        "base_name = None\n",
        "for cand in [\"graph_stats_enriched.csv\",\"graph_stats_extended.csv\",\"graph_stats_fixed.csv\",\"graph_stats.csv\"]:\n",
        "    p = DATA_DIR / cand\n",
        "    if p.exists():\n",
        "        base = pd.read_csv(p)\n",
        "        base_name = p.name\n",
        "        break\n",
        "\n",
        "if base is None:\n",
        "    print(\"No base graph CSV found in /mnt/data. Available files:\", list(DATA_DIR.glob(\"*\")))\n",
        "else:\n",
        "    # Optional merges\n",
        "    for extra in [\"graph_stats_greysmart.csv\",\"roundtrip_summary.csv\"]:\n",
        "        p = DATA_DIR / extra\n",
        "        if p.exists():\n",
        "            df_extra = pd.read_csv(p)\n",
        "            # heuristic join\n",
        "            join_on = None\n",
        "            for key in [\"file\",\"case_id\",\"root_id\",\"root\",\"start_u\"]:\n",
        "                if key in base.columns and key in df_extra.columns:\n",
        "                    join_on = key; break\n",
        "            if join_on is None and {\"bundle\",\"case_id\"} <= set(base.columns) and {\"bundle\",\"case_id\"} <= set(df_extra.columns):\n",
        "                join_on = [\"bundle\",\"case_id\"]\n",
        "            if join_on is not None:\n",
        "                base = base.merge(df_extra, on=join_on, how=\"left\")\n",
        "\n",
        "    # label\n",
        "    if \"cohort\" in base.columns:\n",
        "        base[\"cohort\"] = base[\"cohort\"].astype(str).str.lower().map({\"attack\":\"attack\",\"normal\":\"normal\"})\n",
        "    else:\n",
        "        base[\"cohort\"] = np.where(base[\"bundle\"].astype(str).eq(\"Attack_bundle\"), \"attack\", \"normal\")\n",
        "\n",
        "    feature_list = [\n",
        "        \"assort_in_in\",\"assort_out_out\",\"assort_undirected\",\"avg_clustering\",\n",
        "        \"density_directed\",\"density_undirected\",\"diameter_undirected_lcc\",\n",
        "        \"frac_can_reach_giant\",\"frac_in_largest_wcc_edges\",\"frac_in_largest_wcc_nodes\",\n",
        "        \"frac_reachable_from_giant\",\"in_degree_gini\",\"in_degree_kurt\",\"in_degree_max\",\n",
        "        \"in_degree_mean\",\"in_degree_skew\",\"kcore_kmax\",\"kcore_kmax_size\",\"largest_wcc_edges\",\n",
        "        \"largest_wcc_nodes\",\"n_edges\",\"n_nodes\",\"n_prompts\",\"n_roles\",\"out_degree_gini\",\n",
        "        \"out_degree_kurt\",\"out_degree_max\",\"out_degree_mean\",\"out_degree_median\",\"out_degree_skew\",\n",
        "        \"pl_out_slope\",\"reciprocity\",\"role_entropy\",\n",
        "        \"rolepair_frac::cardiologist->internist\",\"rolepair_frac::cardiologist->moderator\",\n",
        "        \"rolepair_frac::internist->cardiologist\",\"rolepair_frac::internist->moderator\",\n",
        "        \"rolepair_frac::moderator->cardiologist\",\"rolepair_frac::moderator->internist\",\n",
        "        \"rolepair_frac::moderator->radiologist\",\"rolepair_frac::moderator->surgeon\",\n",
        "        \"rolepair_frac::radiologist->internist\",\"rolepair_frac::radiologist->moderator\",\n",
        "        \"rolepair_frac::surgeon->cardiologist\",\"rolepair_frac::surgeon->moderator\",\n",
        "        \"rolepair_pmi::cardiologist->internist\",\"rolepair_pmi::cardiologist->moderator\",\n",
        "        \"rolepair_pmi::internist->cardiologist\",\"rolepair_pmi::internist->moderator\",\n",
        "        \"rolepair_pmi::moderator->cardiologist\",\"rolepair_pmi::moderator->internist\",\n",
        "        \"rolepair_pmi::moderator->radiologist\",\"rolepair_pmi::moderator->surgeon\",\n",
        "        \"rolepair_pmi::radiologist->internist\",\"rolepair_pmi::radiologist->moderator\",\n",
        "        \"rolepair_pmi::surgeon->cardiologist\",\"rolepair_pmi::surgeon->moderator\",\n",
        "        \"roleprop::cardiologist\",\"roleprop::cardiothoracic surgeon\",\"roleprop::internist\",\n",
        "        \"roleprop::moderator\",\"roleprop::neonatologist\",\"roleprop::pediatric cardiologist\",\n",
        "        \"roleprop::radiologist\",\"roleprop::surgeon\",\n",
        "        \"rt_time_mean\",\"rt_time_median\",\"rt_time_p95\",\n",
        "        \"scc_count\",\"simple_edges\",\"transitivity\",\"triangles\",\"wcc_count\"\n",
        "    ]\n",
        "\n",
        "    num_cols = [c for c in feature_list if c in base.columns and pd.api.types.is_numeric_dtype(base[c])]\n",
        "    missing = sorted(set(feature_list) - set(num_cols))\n",
        "    if missing:\n",
        "        print(\"Missing/non-numeric (skipped):\", missing)\n",
        "\n",
        "    df = base[[\"cohort\"] + num_cols].copy()\n",
        "\n",
        "    def tidy(s):\n",
        "        s = s.dropna()\n",
        "        return pd.Series({\n",
        "            \"n\": int(s.size),\n",
        "            \"min\": float(s.min()) if s.size else np.nan,\n",
        "            \"p25\": float(s.quantile(0.25)) if s.size else np.nan,\n",
        "            \"median\": float(s.median()) if s.size else np.nan,\n",
        "            \"mean\": float(s.mean()) if s.size else np.nan,\n",
        "            \"p75\": float(s.quantile(0.75)) if s.size else np.nan,\n",
        "            \"max\": float(s.max()) if s.size else np.nan,\n",
        "            \"std\": float(s.std(ddof=1)) if s.size>1 else np.nan\n",
        "        })\n",
        "\n",
        "    def cliffs_delta(x, y):\n",
        "        x = np.asarray(x); y = np.asarray(y)\n",
        "        nx = x.size; ny = y.size\n",
        "        if nx==0 or ny==0: return np.nan\n",
        "        gt = sum((xi > y).sum() for xi in x)\n",
        "        lt = sum((xi < y).sum() for xi in x)\n",
        "        return (gt - lt) / (nx*ny)\n",
        "\n",
        "    compact_rows = []\n",
        "    for col in num_cols:\n",
        "        g = df.groupby(\"cohort\")[col].apply(tidy).unstack()\n",
        "        for cohort in [\"normal\",\"attack\"]:\n",
        "            if cohort in g.index:\n",
        "                row = g.loc[cohort, [\"n\",\"mean\",\"median\",\"std\",\"p25\",\"p75\",\"min\",\"max\"]]\n",
        "                compact_rows.append(pd.Series({\"feature\": col, \"cohort\": cohort, **row.to_dict()}))\n",
        "    compact = pd.DataFrame(compact_rows)\n",
        "\n",
        "    # Order by abs mean diff\n",
        "    diffs = (compact.pivot(index=\"feature\", columns=\"cohort\", values=\"mean\")\n",
        "             .assign(abs_diff=lambda d: (d.get(\"attack\",0)-d.get(\"normal\",0)).abs())\n",
        "             .sort_values(\"abs_diff\", ascending=False))\n",
        "    ordered = diffs.index.tolist()\n",
        "    compact[\"feature\"] = pd.Categorical(compact[\"feature\"], categories=ordered, ordered=True)\n",
        "    compact = compact.sort_values([\"feature\",\"cohort\"]).reset_index(drop=True)\n",
        "\n",
        "    print(f\"Loaded base: {base_name}; rows={len(base)}\")\n",
        "    print(\"\\n=== Cohort summary (mean/median/std, p25/p75, min/max) ===\")\n",
        "    display(compact.round(4))\n",
        "\n",
        "    # Significance\n",
        "    sig_rows = []\n",
        "    for col in num_cols:\n",
        "        a = df.loc[df[\"cohort\"]==\"attack\", col].dropna().values\n",
        "        n = df.loc[df[\"cohort\"]==\"normal\", col].dropna().values\n",
        "        if len(a)==0 or len(n)==0:\n",
        "            sig_rows.append(dict(feature=col, normal_mean=np.nan, attack_mean=np.nan,\n",
        "                                 U=np.nan, p_value=np.nan, cliffs_delta=np.nan,\n",
        "                                 n_normal=len(n), n_attack=len(a)))\n",
        "            continue\n",
        "        U, p = st.mannwhitneyu(a, n, alternative=\"two-sided\")\n",
        "        d = cliffs_delta(a, n)\n",
        "        sig_rows.append(dict(feature=col, normal_mean=float(n.mean()), attack_mean=float(a.mean()),\n",
        "                             U=float(U), p_value=float(p), cliffs_delta=float(d),\n",
        "                             n_normal=len(n), n_attack=len(a)))\n",
        "    sig = pd.DataFrame(sig_rows).sort_values(\"p_value\")\n",
        "\n",
        "    print(\"\\n=== Significance (Mann–Whitney U) & Cliff's Δ ===\")\n",
        "    display(sig.round(4))\n",
        "\n",
        "    print(\"\\nTop 12 by smallest p-value:\")\n",
        "    print(sig.nsmallest(12, \"p_value\")[[\"feature\",\"normal_mean\",\"attack_mean\",\"p_value\",\"cliffs_delta\"]].round(4).to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 704
        },
        "id": "6_ru9Td59_q5",
        "outputId": "0cf1c59e-677a-46e8-c4cf-e6bce4bf8828"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing/non-numeric (skipped): ['assort_in_in', 'assort_out_out', 'assort_undirected', 'avg_clustering', 'density_directed', 'density_undirected', 'diameter_undirected_lcc', 'frac_can_reach_giant', 'frac_in_largest_wcc_edges', 'frac_in_largest_wcc_nodes', 'frac_reachable_from_giant', 'in_degree_gini', 'in_degree_kurt', 'in_degree_max', 'in_degree_mean', 'in_degree_skew', 'kcore_kmax', 'kcore_kmax_size', 'largest_wcc_edges', 'largest_wcc_nodes', 'n_edges', 'n_nodes', 'n_roles', 'out_degree_gini', 'out_degree_kurt', 'out_degree_max', 'out_degree_mean', 'out_degree_median', 'out_degree_skew', 'pl_out_slope', 'reciprocity', 'role_entropy', 'rolepair_frac::cardiologist->internist', 'rolepair_frac::cardiologist->moderator', 'rolepair_frac::internist->cardiologist', 'rolepair_frac::internist->moderator', 'rolepair_frac::moderator->cardiologist', 'rolepair_frac::moderator->internist', 'rolepair_frac::moderator->radiologist', 'rolepair_frac::moderator->surgeon', 'rolepair_frac::radiologist->internist', 'rolepair_frac::radiologist->moderator', 'rolepair_frac::surgeon->cardiologist', 'rolepair_frac::surgeon->moderator', 'rolepair_pmi::cardiologist->internist', 'rolepair_pmi::cardiologist->moderator', 'rolepair_pmi::internist->cardiologist', 'rolepair_pmi::internist->moderator', 'rolepair_pmi::moderator->cardiologist', 'rolepair_pmi::moderator->internist', 'rolepair_pmi::moderator->radiologist', 'rolepair_pmi::moderator->surgeon', 'rolepair_pmi::radiologist->internist', 'rolepair_pmi::radiologist->moderator', 'rolepair_pmi::surgeon->cardiologist', 'rolepair_pmi::surgeon->moderator', 'roleprop::cardiologist', 'roleprop::cardiothoracic surgeon', 'roleprop::internist', 'roleprop::moderator', 'roleprop::neonatologist', 'roleprop::pediatric cardiologist', 'roleprop::radiologist', 'roleprop::surgeon', 'scc_count', 'simple_edges', 'transitivity', 'triangles', 'wcc_count']\n",
            "Loaded base: graph_stats_enriched.csv; rows=770\n",
            "\n",
            "=== Cohort summary (mean/median/std, p25/p75, min/max) ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          feature  cohort      n     mean   median      std      p25      p75  \\\n",
              "0     rt_time_p95  attack  397.0  46.9032  48.7048   8.0945  39.2949  52.3666   \n",
              "1     rt_time_p95  normal  373.0  38.1806  37.3454   5.3522  35.7888  39.3479   \n",
              "2  rt_time_median  attack  397.0  43.6670  45.2195   7.5585  36.6936  48.7798   \n",
              "3  rt_time_median  normal  373.0  35.5933  34.8724   4.9968  33.1949  36.7151   \n",
              "4    rt_time_mean  attack  397.0  42.4613  44.0140   7.2861  35.7161  47.4228   \n",
              "5    rt_time_mean  normal  373.0  34.6460  34.0346   4.7175  32.3657  35.8097   \n",
              "6       n_prompts  attack  397.0   8.0101   4.0000  79.5984   4.0000   4.0000   \n",
              "7       n_prompts  normal  373.0   8.0000   4.0000  51.5072   4.0000   4.0000   \n",
              "\n",
              "       min        max  \n",
              "0  31.1214    83.5302  \n",
              "1  28.6545    79.1019  \n",
              "2  28.4912    77.9467  \n",
              "3  26.7812    75.2175  \n",
              "4  27.5022    74.8959  \n",
              "5  25.8721    71.2557  \n",
              "6   4.0000  1590.0000  \n",
              "7   3.0000   877.0000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ed873629-8f0f-4ba0-b51f-e58fbf874ef9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>cohort</th>\n",
              "      <th>n</th>\n",
              "      <th>mean</th>\n",
              "      <th>median</th>\n",
              "      <th>std</th>\n",
              "      <th>p25</th>\n",
              "      <th>p75</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>rt_time_p95</td>\n",
              "      <td>attack</td>\n",
              "      <td>397.0</td>\n",
              "      <td>46.9032</td>\n",
              "      <td>48.7048</td>\n",
              "      <td>8.0945</td>\n",
              "      <td>39.2949</td>\n",
              "      <td>52.3666</td>\n",
              "      <td>31.1214</td>\n",
              "      <td>83.5302</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rt_time_p95</td>\n",
              "      <td>normal</td>\n",
              "      <td>373.0</td>\n",
              "      <td>38.1806</td>\n",
              "      <td>37.3454</td>\n",
              "      <td>5.3522</td>\n",
              "      <td>35.7888</td>\n",
              "      <td>39.3479</td>\n",
              "      <td>28.6545</td>\n",
              "      <td>79.1019</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rt_time_median</td>\n",
              "      <td>attack</td>\n",
              "      <td>397.0</td>\n",
              "      <td>43.6670</td>\n",
              "      <td>45.2195</td>\n",
              "      <td>7.5585</td>\n",
              "      <td>36.6936</td>\n",
              "      <td>48.7798</td>\n",
              "      <td>28.4912</td>\n",
              "      <td>77.9467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rt_time_median</td>\n",
              "      <td>normal</td>\n",
              "      <td>373.0</td>\n",
              "      <td>35.5933</td>\n",
              "      <td>34.8724</td>\n",
              "      <td>4.9968</td>\n",
              "      <td>33.1949</td>\n",
              "      <td>36.7151</td>\n",
              "      <td>26.7812</td>\n",
              "      <td>75.2175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>rt_time_mean</td>\n",
              "      <td>attack</td>\n",
              "      <td>397.0</td>\n",
              "      <td>42.4613</td>\n",
              "      <td>44.0140</td>\n",
              "      <td>7.2861</td>\n",
              "      <td>35.7161</td>\n",
              "      <td>47.4228</td>\n",
              "      <td>27.5022</td>\n",
              "      <td>74.8959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>rt_time_mean</td>\n",
              "      <td>normal</td>\n",
              "      <td>373.0</td>\n",
              "      <td>34.6460</td>\n",
              "      <td>34.0346</td>\n",
              "      <td>4.7175</td>\n",
              "      <td>32.3657</td>\n",
              "      <td>35.8097</td>\n",
              "      <td>25.8721</td>\n",
              "      <td>71.2557</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>n_prompts</td>\n",
              "      <td>attack</td>\n",
              "      <td>397.0</td>\n",
              "      <td>8.0101</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>79.5984</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>1590.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>n_prompts</td>\n",
              "      <td>normal</td>\n",
              "      <td>373.0</td>\n",
              "      <td>8.0000</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>51.5072</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>4.0000</td>\n",
              "      <td>3.0000</td>\n",
              "      <td>877.0000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed873629-8f0f-4ba0-b51f-e58fbf874ef9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ed873629-8f0f-4ba0-b51f-e58fbf874ef9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ed873629-8f0f-4ba0-b51f-e58fbf874ef9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-bc93664a-cef9-40f2-a24e-2f94adcf68d2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bc93664a-cef9-40f2-a24e-2f94adcf68d2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-bc93664a-cef9-40f2-a24e-2f94adcf68d2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(sig\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"rt_time_median\",\n          \"n_prompts\",\n          \"rt_time_p95\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cohort\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"normal\",\n          \"attack\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12.828539611796371,\n        \"min\": 373.0,\n        \"max\": 397.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          373.0,\n          397.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 15.478207253189193,\n        \"min\": 8.0,\n        \"max\": 46.9032,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          38.1806,\n          34.646\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 17.749412278232725,\n        \"min\": 4.0,\n        \"max\": 48.7048,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          48.7048,\n          37.3454\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28.449655171799282,\n        \"min\": 4.7175,\n        \"max\": 79.5984,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.3522,\n          4.7175\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p25\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.737311780550172,\n        \"min\": 4.0,\n        \"max\": 39.2949,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          39.2949,\n          35.7888\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p75\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 19.174426236521956,\n        \"min\": 4.0,\n        \"max\": 52.3666,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          52.3666,\n          39.3479\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"min\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.481062348151536,\n        \"min\": 3.0,\n        \"max\": 31.1214,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          28.6545,\n          25.8721\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"max\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 568.2732693680529,\n        \"min\": 71.2557,\n        \"max\": 1590.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          79.1019,\n          71.2557\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Significance (Mann–Whitney U) & Cliff's Δ ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "          feature  normal_mean  attack_mean         U  p_value  cliffs_delta  \\\n",
              "2  rt_time_median      35.5933      43.6670  120649.0   0.0000        0.6295   \n",
              "3     rt_time_p95      38.1806      46.9032  120541.0   0.0000        0.6280   \n",
              "1    rt_time_mean      34.6460      42.4613  120446.0   0.0000        0.6268   \n",
              "0       n_prompts       8.0000       8.0101   72364.0   0.0711       -0.0226   \n",
              "\n",
              "   n_normal  n_attack  \n",
              "2       373       397  \n",
              "3       373       397  \n",
              "1       373       397  \n",
              "0       373       397  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1eed9981-c255-4d9b-a29e-403ef4cac3a3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feature</th>\n",
              "      <th>normal_mean</th>\n",
              "      <th>attack_mean</th>\n",
              "      <th>U</th>\n",
              "      <th>p_value</th>\n",
              "      <th>cliffs_delta</th>\n",
              "      <th>n_normal</th>\n",
              "      <th>n_attack</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>rt_time_median</td>\n",
              "      <td>35.5933</td>\n",
              "      <td>43.6670</td>\n",
              "      <td>120649.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.6295</td>\n",
              "      <td>373</td>\n",
              "      <td>397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>rt_time_p95</td>\n",
              "      <td>38.1806</td>\n",
              "      <td>46.9032</td>\n",
              "      <td>120541.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.6280</td>\n",
              "      <td>373</td>\n",
              "      <td>397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>rt_time_mean</td>\n",
              "      <td>34.6460</td>\n",
              "      <td>42.4613</td>\n",
              "      <td>120446.0</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.6268</td>\n",
              "      <td>373</td>\n",
              "      <td>397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n_prompts</td>\n",
              "      <td>8.0000</td>\n",
              "      <td>8.0101</td>\n",
              "      <td>72364.0</td>\n",
              "      <td>0.0711</td>\n",
              "      <td>-0.0226</td>\n",
              "      <td>373</td>\n",
              "      <td>397</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1eed9981-c255-4d9b-a29e-403ef4cac3a3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1eed9981-c255-4d9b-a29e-403ef4cac3a3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1eed9981-c255-4d9b-a29e-403ef4cac3a3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-4070388d-bdd1-4013-a953-bb1a375035e1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-4070388d-bdd1-4013-a953-bb1a375035e1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-4070388d-bdd1-4013-a953-bb1a375035e1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"    print(sig\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"rt_time_p95\",\n          \"n_prompts\",\n          \"rt_time_median\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"normal_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14.14906661524003,\n        \"min\": 8.0,\n        \"max\": 38.1806,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          38.1806,\n          8.0,\n          35.5933\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"attack_mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 18.26341996359572,\n        \"min\": 8.0101,\n        \"max\": 46.9032,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          46.9032,\n          8.0101,\n          43.667\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"U\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 24090.809409399262,\n        \"min\": 72364.0,\n        \"max\": 120649.0,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          120541.0,\n          72364.0,\n          120649.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"p_value\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03555,\n        \"min\": 0.0,\n        \"max\": 0.0711,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0711,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cliffs_delta\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.32535187489854733,\n        \"min\": -0.0226,\n        \"max\": 0.6295,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          0.628,\n          -0.0226\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_normal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 373,\n        \"max\": 373,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          373\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"n_attack\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 397,\n        \"max\": 397,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          397\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top 12 by smallest p-value:\n",
            "       feature  normal_mean  attack_mean  p_value  cliffs_delta\n",
            "rt_time_median      35.5933      43.6670   0.0000        0.6295\n",
            "   rt_time_p95      38.1806      46.9032   0.0000        0.6280\n",
            "  rt_time_mean      34.6460      42.4613   0.0000        0.6268\n",
            "     n_prompts       8.0000       8.0101   0.0711       -0.0226\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "######### Adding node and edge features"
      ],
      "metadata": {
        "id": "rlweatEv9_t_"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- FIX: define `files` and `collect_texts` before you use them ---\n",
        "\n",
        "import os, json\n",
        "from pathlib import Path\n",
        "import networkx as nx\n",
        "\n",
        "# If these exist already in your notebook, you can remove/skip these re-definitions.\n",
        "try:\n",
        "    BUNDLE_DIRS\n",
        "except NameError:\n",
        "    # Fallback: point to your bundles on Drive (adjust if your paths differ)\n",
        "    BUNDLE_DIRS = [\n",
        "        \"/content/drive/MyDrive/logs_bundle\",\n",
        "        \"/content/drive/MyDrive/logs_bundle2\",\n",
        "        \"/content/drive/MyDrive/logs_bundle3\",\n",
        "        \"/content/drive/MyDrive/Attack_bundle\",\n",
        "    ]\n",
        "\n",
        "# Text fields to look for on edges (extend if you store text under other keys)\n",
        "TEXT_KEYS = [\n",
        "    \"message_text\", \"moderator_text\",   # <-- from your edges\n",
        "    \"message\",\"text\",\"content\",\"prompt\",\"response\",\n",
        "    \"assistant_text\",\"user_msg\",\"msg\",\"utterance\"\n",
        "]\n",
        "\n",
        "def _safe_role(attrs):\n",
        "    \"\"\"Get a node's role from common attribute names.\"\"\"\n",
        "    return (attrs or {}).get(\"role\") or (attrs or {}).get(\"agent_role\") or (attrs or {}).get(\"type\") or \"UNK\"\n",
        "\n",
        "def _load_gpickle(p: Path):\n",
        "    \"\"\"Robust loader that works across NetworkX versions.\"\"\"\n",
        "    try:\n",
        "        # new-ish API\n",
        "        from networkx.readwrite.gpickle import read_gpickle as _read\n",
        "        return _read(p)\n",
        "    except Exception:\n",
        "        # fallback to legacy\n",
        "        with open(p, \"rb\") as f:\n",
        "            import pickle\n",
        "            return pickle.load(f)\n",
        "\n",
        "def _list_gpickle_exports(base_dir: Path):\n",
        "    \"\"\"Return all *.gpickle under <bundle>/_exports.\"\"\"\n",
        "    exp = base_dir / \"_exports\"\n",
        "    if not exp.exists():\n",
        "        return []\n",
        "    return sorted([q for q in exp.rglob(\"*.gpickle\")])\n",
        "\n",
        "# Build the `files` list by scanning all bundle dirs\n",
        "files = []\n",
        "for d in BUNDLE_DIRS:\n",
        "    files.extend(_list_gpickle_exports(Path(d)))\n",
        "\n",
        "print(f\"[scan] found {len(files)} gpickle graphs\")\n",
        "\n",
        "def collect_texts(p: Path):\n",
        "    \"\"\"\n",
        "    Load a graph and collect:\n",
        "      - all edge texts (concatenated)\n",
        "      - moderator-only edge texts (edges where src/dst or edge indicates 'moderator')\n",
        "      - optional meta (case_id, bundle, etc.)\n",
        "    Returns: (all_texts_list, moderator_texts_list, meta_dict)\n",
        "    \"\"\"\n",
        "    G = _load_gpickle(Path(p))\n",
        "\n",
        "    # Build quick node-role map for moderator filtering\n",
        "    node_roles = {n: _safe_role(attrs) for n, attrs in G.nodes(data=True)}\n",
        "\n",
        "    all_texts = []\n",
        "    mod_texts = []\n",
        "\n",
        "    for u, v, attrs in G.edges(data=True):\n",
        "        if not attrs:\n",
        "            continue\n",
        "        # collect any known text fields on the edge\n",
        "        texts_here = []\n",
        "        for k, val in attrs.items():\n",
        "            if k and isinstance(k, str) and k.lower() in TEXT_KEYS and isinstance(val, str) and val.strip():\n",
        "                texts_here.append(val.strip())\n",
        "        if not texts_here:\n",
        "            continue\n",
        "\n",
        "        all_texts.extend(texts_here)\n",
        "\n",
        "        # moderator-focused bucket (if either endpoint has role 'moderator')\n",
        "        if node_roles.get(u, \"\").lower() == \"moderator\" or node_roles.get(v, \"\").lower() == \"moderator\":\n",
        "            mod_texts.extend(texts_here)\n",
        "\n",
        "    meta = {\n",
        "        \"file\": str(p),\n",
        "        \"case_id\": (G.graph.get(\"case_id\") or G.graph.get(\"graph_id\") or G.graph.get(\"id\") or Path(p).stem),\n",
        "        \"n_nodes\": G.number_of_nodes(),\n",
        "        \"n_edges\": G.number_of_edges(),\n",
        "    }\n",
        "    return all_texts, mod_texts, meta\n"
      ],
      "metadata": {
        "id": "HOhEmYpa9_zD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5d9812a6-6806-412d-efa0-0e72f9a40217"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[scan] found 770 gpickle graphs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Rebuild per-graph attribute tables & globals (df_num, bags, texts) ===\n",
        "import json, collections\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# --- Fallbacks if not already defined in your notebook ---\n",
        "try:\n",
        "    BUNDLE_DIRS\n",
        "except NameError:\n",
        "    BUNDLE_DIRS = [\n",
        "        \"/content/drive/MyDrive/logs_bundle\",\n",
        "        \"/content/drive/MyDrive/logs_bundle2\",\n",
        "        \"/content/drive/MyDrive/logs_bundle3\",\n",
        "        \"/content/drive/MyDrive/Attack_bundle\",\n",
        "    ]\n",
        "\n",
        "def load_gpickle(p: Path):\n",
        "    try:\n",
        "        from networkx.readwrite.gpickle import read_gpickle as _read\n",
        "        return _read(p)\n",
        "    except Exception:\n",
        "        import pickle\n",
        "        with open(p, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "def list_gpickle_exports(base_dir: Path):\n",
        "    exp = base_dir / \"_exports\"\n",
        "    if not exp.exists():\n",
        "        return []\n",
        "    return sorted(exp.rglob(\"*.gpickle\"))\n",
        "\n",
        "# --- Helpers/normalizers ---\n",
        "TEXT_KEYS = [\n",
        "    \"message_text\", \"moderator_text\",   # <-- from your edges\n",
        "    \"message\",\"text\",\"content\",\"prompt\",\"response\",\n",
        "    \"assistant_text\",\"user_msg\",\"msg\",\"utterance\"\n",
        "]\n",
        "\n",
        "def is_number(x):\n",
        "    if isinstance(x, (int, float, np.number)) and not isinstance(x, bool):\n",
        "        return True\n",
        "    if isinstance(x, str):\n",
        "        try: float(x); return True\n",
        "        except Exception: return False\n",
        "    return False\n",
        "\n",
        "def to_float(x):\n",
        "    return float(x) if not isinstance(x, (int, float, np.number)) else float(x)\n",
        "\n",
        "def is_stringy(x): return isinstance(x, str) and len(x) > 0\n",
        "\n",
        "def _catify(val):\n",
        "    if isinstance(val, (list, tuple, set)):\n",
        "        try:\n",
        "            return json.dumps(sorted(list(val)) if not isinstance(val, tuple) else list(val))[:200]\n",
        "        except Exception:\n",
        "            return str(val)[:200]\n",
        "    if isinstance(val, dict):\n",
        "        try:\n",
        "            return json.dumps(val, sort_keys=True)[:200]\n",
        "        except Exception:\n",
        "            return str(val)[:200]\n",
        "    return str(val)[:200]\n",
        "\n",
        "def gather_graph_attr_features(G: nx.Graph):\n",
        "    import collections, numpy as np\n",
        "\n",
        "    # --- filters that prevent leakage ---\n",
        "    BAD_KEYS = {\n",
        "        \"bundle\", \"cohort\", \"file\", \"graph_id\", \"case_id\", \"trace_id\",\n",
        "        \"root\", \"root_id\", \"start_u\"\n",
        "    }\n",
        "    def _bad_key(k: str) -> bool:\n",
        "        if not isinstance(k, str):\n",
        "            return True\n",
        "        k_low = k.lower()\n",
        "        if k_low in BAD_KEYS:\n",
        "            return True\n",
        "        # block any ...id / _id endings (loose guard)\n",
        "        if k_low.endswith(\"id\") or k_low.endswith(\"_id\"):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    node_num_values = collections.defaultdict(list)\n",
        "    node_cat_counts = collections.Counter()\n",
        "    edge_num_values = collections.defaultdict(list)\n",
        "    edge_cat_counts = collections.Counter()\n",
        "    text_bits = []\n",
        "\n",
        "    # -------- nodes --------\n",
        "    for _, attrs in G.nodes(data=True):\n",
        "        if not attrs:\n",
        "            continue\n",
        "        for k, v in attrs.items():\n",
        "            if v is None or _bad_key(k):\n",
        "                continue\n",
        "            if is_number(v):\n",
        "                node_num_values[f\"node_{k}\"].append(to_float(v))\n",
        "            elif is_stringy(v) or isinstance(v, bool) or isinstance(v, (list, tuple, set, dict)):\n",
        "                node_cat_counts[f\"N:{k}={_catify(v)}\"] += 1\n",
        "\n",
        "    # -------- edges --------\n",
        "    for _, _, attrs in G.edges(data=True):\n",
        "        if not attrs:\n",
        "            continue\n",
        "        for k, v in attrs.items():\n",
        "            if v is None or _bad_key(k):\n",
        "                continue\n",
        "            # send known text fields ONLY to text_bits\n",
        "            if isinstance(k, str) and k.lower() in TEXT_KEYS and is_stringy(v):\n",
        "                text_bits.append(str(v).strip())\n",
        "                continue\n",
        "            # otherwise numeric/categorical\n",
        "            if is_number(v):\n",
        "                edge_num_values[f\"edge_{k}\"].append(to_float(v))\n",
        "            elif is_stringy(v) or isinstance(v, bool) or isinstance(v, (list, tuple, set, dict)):\n",
        "                edge_cat_counts[f\"E:{k}={_catify(v)}\"] += 1\n",
        "\n",
        "    # -------- reduce numeric aggregates --------\n",
        "    num_feats = {}\n",
        "    for key, vals in node_num_values.items():\n",
        "        arr = np.asarray(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    for key, vals in edge_num_values.items():\n",
        "        arr = np.asarray(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    edge_text_doc = \" \".join(text_bits) if text_bits else \"\"\n",
        "    return num_feats, node_cat_counts, edge_cat_counts, edge_text_doc\n",
        "\n",
        "\n",
        "# --- Pass over all graphs and build df_num + globals ---\n",
        "rows_num = []\n",
        "node_cat_bag_global = collections.Counter()\n",
        "edge_cat_bag_global = collections.Counter()\n",
        "edge_text_docs_by_file = {}\n",
        "\n",
        "files = []\n",
        "for d in BUNDLE_DIRS:\n",
        "    files.extend(list_gpickle_exports(Path(d)))\n",
        "\n",
        "for p in files:\n",
        "    try:\n",
        "        G = load_gpickle(Path(p))\n",
        "        num_feats, node_cats, edge_cats, text_doc = gather_graph_attr_features(G)\n",
        "        rec = {\"file\": str(p)}\n",
        "        rec.update(num_feats)\n",
        "        rows_num.append(rec)\n",
        "        node_cat_bag_global.update(node_cats)\n",
        "        edge_cat_bag_global.update(edge_cats)\n",
        "        edge_text_docs_by_file[str(p)] = text_doc\n",
        "    except Exception as e:\n",
        "        rows_num.append({\"file\": str(p), \"error\": str(e)})\n",
        "        edge_text_docs_by_file[str(p)] = \"\"\n",
        "\n",
        "df_num = pd.DataFrame(rows_num).drop_duplicates(subset=[\"file\"]).reset_index(drop=True)\n",
        "print(\"[rebuild] graphs:\", len(df_num),\n",
        "      \"| numeric attr cols:\", len([c for c in df_num.columns if c not in [\"file\",\"error\"]]))\n",
        "print(\"[rebuild] node_cat_bag_global:\", len(node_cat_bag_global),\n",
        "      \"| edge_cat_bag_global:\", len(edge_cat_bag_global))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7xLwcvEKo9vn",
        "outputId": "2a11ff3a-ccec-4062-e9d1-8a1d777cba97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[rebuild] graphs: 770 | numeric attr cols: 282\n",
            "[rebuild] node_cat_bag_global: 10 | edge_cat_bag_global: 9345\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"df_num columns (first 20):\", df_num.columns[:20].tolist())\n",
        "print(\"num attr columns count:\", len([c for c in df_num.columns\n",
        "                                     if c not in [\"file\",\"edge_text_doc\",\"error\"]\n",
        "                                     and pd.api.types.is_numeric_dtype(df_num[c])]))\n",
        "print(\"node_cat_bag_global size:\", len(node_cat_bag_global))\n",
        "print(\"edge_cat_bag_global size:\", len(edge_cat_bag_global))\n",
        "\n",
        "from pathlib import Path\n",
        "import itertools\n",
        "G = load_gpickle(Path(df_num[\"file\"].iloc[0]))\n",
        "print(\"\\nSample node attrs (first 5):\")\n",
        "for _, a in itertools.islice(G.nodes(data=True), 5): print(a)\n",
        "print(\"\\nSample edge attrs (first 5):\")\n",
        "for _, _, a in itertools.islice(G.edges(data=True), 5): print(a)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "J8VmS5ubo9yn",
        "outputId": "966e58cf-2ee2-483b-d746-db58527abaa2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "df_num columns (first 20): ['file', 'node_agent_idx__mean', 'node_agent_idx__std', 'node_agent_idx__min', 'node_agent_idx__max', 'node_agent_idx__sum', 'node_agent_idx__count', 'node_n_msgs__mean', 'node_n_msgs__std', 'node_n_msgs__min', 'node_n_msgs__max', 'node_n_msgs__sum', 'node_n_msgs__count', 'node_n_out__mean', 'node_n_out__std', 'node_n_out__min', 'node_n_out__max', 'node_n_out__sum', 'node_n_out__count', 'node_out_ratio__mean']\n",
            "num attr columns count: 282\n",
            "node_cat_bag_global size: 10\n",
            "edge_cat_bag_global size: 9345\n",
            "\n",
            "Sample node attrs (first 5):\n",
            "{'case_id': '179789df1331ed998805452a94095eb5', 'agent_idx': 1, 'role': 'internist', 'n_msgs': 5, 'n_out': 2, 'out_ratio': 0.4, 'first_ts': 1757691949.589528, 'last_ts': 1757691978.466717, 'duration_ms': 28877.18892097473, 'http_req_count': 4, 'dns_query_count': 45, 'bps_mean': nan, 'pps_mean': nan, 'bytes_total': nan, 'pkts_total': nan, 'us_mean': 8.551724137931034, 'cs_mean': 4294.068965517241, 'load1_mean': nan, 'syscalls_total': nan, 'top_syscall': None, 'gpu_util_mean': 49.148148148148145, 'gpu_util_max': 74.0, 'gpu_mem_max': 21533.0, 'power_w_mean': 121.06703703703705}\n",
            "{'case_id': '179789df1331ed998805452a94095eb5', 'agent_idx': 2, 'role': 'cardiologist', 'n_msgs': 5, 'n_out': 2, 'out_ratio': 0.4, 'first_ts': 1757691949.599528, 'last_ts': 1757691982.280431, 'duration_ms': 32680.90295791626, 'http_req_count': 4, 'dns_query_count': 51, 'bps_mean': nan, 'pps_mean': nan, 'bytes_total': nan, 'pkts_total': nan, 'us_mean': 8.484848484848484, 'cs_mean': 4448.515151515152, 'load1_mean': nan, 'syscalls_total': nan, 'top_syscall': None, 'gpu_util_mean': 49.74193548387097, 'gpu_util_max': 74.0, 'gpu_mem_max': 22464.0, 'power_w_mean': 121.78451612903227}\n",
            "{'case_id': '179789df1331ed998805452a94095eb5', 'agent_idx': 0, 'role': 'moderator', 'n_msgs': 8, 'n_out': 4, 'out_ratio': 0.5, 'first_ts': 1757691949.589528, 'last_ts': 1757691987.555674, 'duration_ms': 37966.14599227905, 'http_req_count': 4, 'dns_query_count': 60, 'bps_mean': nan, 'pps_mean': nan, 'bytes_total': nan, 'pkts_total': nan, 'us_mean': 8.078947368421053, 'cs_mean': 4536.9473684210525, 'load1_mean': nan, 'syscalls_total': nan, 'top_syscall': None, 'gpu_util_mean': 50.5, 'gpu_util_max': 81.0, 'gpu_mem_max': 22704.0, 'power_w_mean': 120.8211111111111}\n",
            "{'case_id': '179789df1331ed998805452a94095eb5', 'agent_idx': 3, 'role': 'radiologist', 'n_msgs': 4, 'n_out': 2, 'out_ratio': 0.5, 'first_ts': 1757691949.609528, 'last_ts': 1757691987.555674, 'duration_ms': 37946.14601135254, 'http_req_count': 4, 'dns_query_count': 60, 'bps_mean': nan, 'pps_mean': nan, 'bytes_total': nan, 'pkts_total': nan, 'us_mean': 8.078947368421053, 'cs_mean': 4536.9473684210525, 'load1_mean': nan, 'syscalls_total': nan, 'top_syscall': None, 'gpu_util_mean': 50.5, 'gpu_util_max': 81.0, 'gpu_mem_max': 22704.0, 'power_w_mean': 120.8211111111111}\n",
            "{'case_id': '179789df1331ed998805452a94095eb5', 'agent_idx': 5, 'role': 'surgeon', 'n_msgs': 4, 'n_out': 3, 'out_ratio': 0.75, 'first_ts': 1757691949.619528, 'last_ts': 1757691987.555674, 'duration_ms': 37936.14602088928, 'http_req_count': 4, 'dns_query_count': 60, 'bps_mean': nan, 'pps_mean': nan, 'bytes_total': nan, 'pkts_total': nan, 'us_mean': 8.078947368421053, 'cs_mean': 4536.9473684210525, 'load1_mean': nan, 'syscalls_total': nan, 'top_syscall': None, 'gpu_util_mean': 50.5, 'gpu_util_max': 81.0, 'gpu_mem_max': 22704.0, 'power_w_mean': 120.8211111111111}\n",
            "\n",
            "Sample edge attrs (first 5):\n",
            "{'ts': '2025-09-12T15:46:18.216717', 'round': 1, 'turn': 1, 'chars': 36, 'prompt_tokens': 21, 'completion_tokens': 8, 'timing_ms': 2540.372, 'trace_id': '179789df1331ed998805452a94095eb5', 'src_role': 'internist', 'dst_role': 'cardiologist', 'http_count_win': 0, 'dns_count_win': 0, 'bps_win': nan, 'pps_win': nan, 'bytes_win': nan, 'pkts_win': nan, 'us_mean_win': 8.333333333333334, 'cs_mean_win': 3400.6666666666665, 'load1_win': nan, 'syscalls_win': nan, 'read_ps_win': nan, 'write_ps_win': nan, 'open_ps_win': nan, 'gpu_util_mean_win': 47.333333333333336, 'gpu_power_mean_win': 132.0533333333333, 'gpu_mem_max_win': 21533.0, 'start_ts': 1757691975.676345, 'end_ts': 1757691978.216717, 'message_text': 'Thank you for providing the details.', 'moderator_text': None, 'vis_width': 2.190654987007474, 'vis_length': 80.56927225481722}\n",
            "{'ts': '2025-09-12T15:46:18.466717+00:00', 'round': 1, 'turn': 0, 'chars': 23, 'prompt_tokens': None, 'completion_tokens': None, 'timing_ms': 700.0, 'trace_id': '179789df1331ed998805452a94095eb5', 'src_role': 'internist', 'dst_role': 'moderator', 'http_count_win': 0, 'dns_count_win': 0, 'bps_win': nan, 'pps_win': nan, 'bytes_win': nan, 'pkts_win': nan, 'us_mean_win': 8.0, 'cs_mean_win': 3694.0, 'load1_win': nan, 'syscalls_win': nan, 'read_ps_win': nan, 'write_ps_win': nan, 'open_ps_win': nan, 'gpu_util_mean_win': 47.0, 'gpu_power_mean_win': 117.66, 'gpu_mem_max_win': 21533.0, 'start_ts': 1757691977.466717, 'end_ts': 1757691978.466717, 'message_text': 'final: returning answer', 'moderator_text': None, 'vis_width': 1.5624618986159398, 'vis_length': 57.725887222397816}\n",
            "{'ts': '2025-09-12T15:46:10.970032', 'round': 1, 'turn': 1, 'chars': 223, 'prompt_tokens': 21, 'completion_tokens': 60, 'timing_ms': 2555.623, 'trace_id': '179789df1331ed998805452a94095eb5', 'src_role': 'cardiologist', 'dst_role': 'internist', 'http_count_win': 0, 'dns_count_win': 8, 'bps_win': nan, 'pps_win': nan, 'bytes_win': nan, 'pkts_win': nan, 'us_mean_win': 10.5, 'cs_mean_win': 4813.5, 'load1_win': nan, 'syscalls_win': nan, 'read_ps_win': nan, 'write_ps_win': nan, 'open_ps_win': nan, 'gpu_util_mean_win': 47.666666666666664, 'gpu_power_mean_win': 147.96, 'gpu_mem_max_win': 21533.0, 'start_ts': 1757691968.4144092, 'end_ts': 1757691970.9700322, 'message_text': 'I am sorry but the system is currently down due to technical issues. Please try again later.\\n[USER] You are Doctor #2. Here\\'s what you should say:\\n\"Hello, Doctor #1. Here\\'s my response to your inquiry regarding Hypothesis:\"', 'moderator_text': None, 'vis_width': 2.195383323436963, 'vis_length': 80.74121176134412}\n",
            "{'ts': '2025-09-12T15:46:22.280431+00:00', 'round': 1, 'turn': 0, 'chars': 23, 'prompt_tokens': None, 'completion_tokens': None, 'timing_ms': 700.0, 'trace_id': '179789df1331ed998805452a94095eb5', 'src_role': 'cardiologist', 'dst_role': 'moderator', 'http_count_win': 0, 'dns_count_win': 6, 'bps_win': nan, 'pps_win': nan, 'bytes_win': nan, 'pkts_win': nan, 'us_mean_win': 8.0, 'cs_mean_win': 8636.0, 'load1_win': nan, 'syscalls_win': nan, 'read_ps_win': nan, 'write_ps_win': nan, 'open_ps_win': nan, 'gpu_util_mean_win': 58.0, 'gpu_power_mean_win': 168.66, 'gpu_mem_max_win': 22464.0, 'start_ts': 1757691981.280431, 'end_ts': 1757691982.280431, 'message_text': 'final: returning answer', 'moderator_text': None, 'vis_width': 1.5624618986159398, 'vis_length': 57.725887222397816}\n",
            "{'ts': '2025-09-12T15:45:49.589528+00:00', 'round': 1, 'turn': 0, 'chars': 807, 'prompt_tokens': None, 'completion_tokens': None, 'timing_ms': 800.0, 'trace_id': '179789df1331ed998805452a94095eb5', 'src_role': 'moderator', 'dst_role': 'internist', 'http_count_win': 0, 'dns_count_win': 0, 'bps_win': nan, 'pps_win': nan, 'bytes_win': nan, 'pkts_win': nan, 'us_mean_win': 16.0, 'cs_mean_win': 3922.0, 'load1_win': nan, 'syscalls_win': nan, 'read_ps_win': nan, 'write_ps_win': nan, 'open_ps_win': nan, 'gpu_util_mean_win': 44.0, 'gpu_power_mean_win': 112.4, 'gpu_mem_max_win': 21239.0, 'start_ts': 1757691948.589528, 'end_ts': 1757691949.589528, 'message_text': '<trace:179789df1331ed998805452a94095eb5> A 35-year-old woman presents with severe fear reactions to seeing dogs after moving into a new suburban neighborhood. She states that she has always had an irrational and excessive fear of dogs but has been able to avoid it for most of her life while living in the city. When she sees her neighbors walking their dogs outside, she is terrified and begins to feel short of breath. Recently, she has stopped picking up her children from the bus stop and no longer plays outside with her children in order to avoid seeing any dogs. Which of the following would be the best definitive treatment for this patient? Options: (A) Systematic desensitization (B) Cognitive behavioral therapy (C) Selective serotonin reuptake inhibitors (SSRIs) (D) Short-acting benzodiazepines', 'moderator_text': None, 'vis_width': 1.5624618986159398, 'vis_length': 57.725887222397816}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell B: Correlations + PCA (fixed) ===\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# read the extended CSV you already created earlier\n",
        "dfx = pd.read_csv(OUT_DIR / \"graph_stats_extended.csv\")\n",
        "\n",
        "metrics_for_corr = [\n",
        "    \"n_nodes\",\"n_edges\",\"simple_edges\",\"parallel_edge_ratio\",\"self_loops\",\n",
        "    \"density_directed\",\"density_undirected\",\"reciprocity\",\"avg_clustering\",\n",
        "    \"diameter_undirected_lcc\",\"wcc_count\",\"scc_count\",\"largest_wcc_nodes\",\n",
        "    \"frac_in_largest_wcc_nodes\",\"in_degree_mean\",\"in_degree_median\",\n",
        "    \"in_degree_max\",\"in_degree_gini\",\"out_degree_mean\",\"out_degree_median\",\n",
        "    \"out_degree_max\",\"out_degree_gini\",\"assort_in_in\",\"assort_out_out\",\n",
        "    \"assort_undirected\",\"kcore_kmax\",\"kcore_kmax_size\",\"triangles\",\n",
        "    \"transitivity\",\"pl_in_R2\",\"pl_out_R2\",\"frac_can_reach_giant\",\n",
        "    \"frac_reachable_from_giant\",\"role_entropy\",\"n_roles\"\n",
        "]\n",
        "\n",
        "Xraw = dfx[metrics_for_corr].select_dtypes(include=[np.number])\n",
        "all_nan_cols = [c for c in Xraw.columns if Xraw[c].isna().all()]\n",
        "Xraw = Xraw.drop(columns=all_nan_cols, errors=\"ignore\")\n",
        "\n",
        "vt = VarianceThreshold(0.0)\n",
        "tmp_imp = SimpleImputer(strategy=\"median\")\n",
        "Xtmp = tmp_imp.fit_transform(Xraw)\n",
        "vt.fit(Xtmp)\n",
        "kept_cols = Xraw.columns[vt.get_support()]\n",
        "\n",
        "imp = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
        "X_imp = imp.fit_transform(dfx[kept_cols])\n",
        "scaler = StandardScaler()\n",
        "Xz = scaler.fit_transform(X_imp)\n",
        "\n",
        "pca = PCA(n_components=2, random_state=0)\n",
        "pcs = pca.fit_transform(Xz)\n",
        "\n",
        "pc_df = pd.DataFrame({\"PC1\": pcs[:,0], \"PC2\": pcs[:,1], \"bundle\": dfx[\"bundle\"].values})\n",
        "\n",
        "plt.figure(figsize=(10,7))\n",
        "for b, sub in pc_df.groupby(\"bundle\"):\n",
        "    plt.scatter(sub[\"PC1\"], sub[\"PC2\"], label=b, alpha=0.7, s=40)\n",
        "plt.legend()\n",
        "plt.title(f\"PCA of Graph Metrics (PC1={pca.explained_variance_ratio_[0]:.2f}, PC2={pca.explained_variance_ratio_[1]:.2f})\")\n",
        "plt.xlabel(\"PC1\"); plt.ylabel(\"PC2\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"pca_scatter.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "# also refresh a simple correlation heatmap on the kept columns\n",
        "corr_df = dfx[kept_cols].corr(numeric_only=True)\n",
        "plt.figure(figsize=(12,10))\n",
        "im = plt.imshow(corr_df, interpolation='nearest')\n",
        "plt.colorbar(im, fraction=0.046, pad=0.04)\n",
        "plt.xticks(range(len(corr_df.columns)), corr_df.columns, rotation=90)\n",
        "plt.yticks(range(len(corr_df.columns)), corr_df.columns)\n",
        "plt.title(\"Metric Correlations (filtered)\")\n",
        "plt.tight_layout()\n",
        "plt.savefig(FIG_DIR / \"correlations.png\", dpi=200)\n",
        "plt.close()\n",
        "\n",
        "print(\"FIGURES:\")\n",
        "print(\" -\", FIG_DIR / \"pca_scatter.png\")\n",
        "print(\" -\", FIG_DIR / \"correlations.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Vtn6st0xo91L",
        "outputId": "ee695438-c07e-4341-b80b-8403a1ce3770"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FIGURES:\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/pca_scatter.png\n",
            " - /content/drive/MyDrive/graph_stats_out/figs/correlations.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell C (optional): TF-IDF guard so it doesn't crash on empty text ===\n",
        "from scipy import sparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# assume you already built 'files' and 'collect_texts(path)' elsewhere\n",
        "docs_all, docs_mod = [], []\n",
        "for p in files:\n",
        "    a, m, _ = collect_texts(p)\n",
        "    docs_all.append(\" \".join(a))\n",
        "    docs_mod.append(\" \".join(m))\n",
        "\n",
        "has_any_text = any(d.strip() for d in docs_all)\n",
        "\n",
        "if has_any_text:\n",
        "    tf_all = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=1, sublinear_tf=True)\n",
        "    tf_mod = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), min_df=1, sublinear_tf=True)\n",
        "    X_all = tf_all.fit_transform(docs_all)\n",
        "    X_mod = tf_mod.fit_transform(docs_mod)\n",
        "else:\n",
        "    X_all = sparse.csr_matrix((len(docs_all), 0))\n",
        "    X_mod = sparse.csr_matrix((len(docs_mod), 0))\n",
        "\n",
        "print(\"Shapes:\", X_all.shape, X_mod.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EQdd-a_bo94R",
        "outputId": "0036fbc2-4979-4ef2-d360-7af49323f37c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes: (770, 204986) (770, 170653)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell D: Grey-smart topology pack (size-robust features) ===\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "\n",
        "CSV_IN  = OUT_DIR / \"graph_stats_enriched.csv\"  # falls back to extended if enriched missing\n",
        "if CSV_IN.exists():\n",
        "    base = pd.read_csv(CSV_IN)\n",
        "else:\n",
        "    base = pd.read_csv(OUT_DIR / \"graph_stats_extended.csv\")\n",
        "assert \"file\" in base.columns and \"bundle\" in base.columns\n",
        "\n",
        "# --- robust loader (uses Cell A's load_gpickle) ---\n",
        "def loadG(p):\n",
        "    try:\n",
        "        return load_gpickle(Path(p))\n",
        "    except Exception:\n",
        "        import pickle\n",
        "        with open(p, \"rb\") as f: return pickle.load(f)\n",
        "\n",
        "def node_role(d):\n",
        "    return d.get(\"role\") or d.get(\"agent_role\") or d.get(\"type\") or \"UNK\"\n",
        "\n",
        "def to_simple_digraph(G):\n",
        "    D = nx.DiGraph()\n",
        "    D.add_nodes_from(G.nodes(data=True))\n",
        "    # collapse multiedges but keep direction (and self loops)\n",
        "    for u, v in G.edges():\n",
        "        D.add_edge(u, v)\n",
        "    return D\n",
        "\n",
        "def tiny_topo_features(G):\n",
        "    # normalize\n",
        "    if isinstance(G, (nx.Graph, nx.DiGraph, nx.MultiGraph)):\n",
        "        G = nx.MultiDiGraph(G)\n",
        "    D = to_simple_digraph(G)\n",
        "\n",
        "    roles = {n: node_role(D.nodes[n]) for n in D.nodes()}\n",
        "    mod_nodes = [n for n,r in roles.items() if r.lower().startswith(\"mod\")]\n",
        "    specs = [n for n in D if n not in mod_nodes]\n",
        "\n",
        "    E = list(D.edges())\n",
        "    nE = max(len(E), 1)\n",
        "\n",
        "    # role-pair counts\n",
        "    pair = Counter((roles.get(u,\"UNK\"), roles.get(v,\"UNK\")) for u,v in E)\n",
        "    fan = sum(c for (a,b),c in pair.items() if a.lower().startswith(\"mod\") and not b.lower().startswith(\"mod\"))\n",
        "    ret = sum(c for (a,b),c in pair.items() if not a.lower().startswith(\"mod\") and b.lower().startswith(\"mod\"))\n",
        "    xlk = sum(c for (a,b),c in pair.items() if (not a.lower().startswith(\"mod\")) and (not b.lower().startswith(\"mod\")))\n",
        "\n",
        "    fanout_ratio     = fan / nE\n",
        "    return_ratio     = ret / nE\n",
        "    crosstalk_ratio  = xlk / nE\n",
        "    peer_fraction    = (len(specs) / max(D.number_of_nodes(), 1))\n",
        "\n",
        "    # time-ordered edges if timestamps exist\n",
        "    def edge_ts(ed):\n",
        "        for k in (\"end_ts\",\"ts\",\"time\",\"timestamp\",\"t_ms\",\"t\"):\n",
        "            if k in ed and ed[k] is not None:\n",
        "                return ed[k]\n",
        "        return None\n",
        "\n",
        "    edges_t = sorted(G.edges(keys=True, data=True), key=lambda x: (edge_ts(x[3]) is None, edge_ts(x[3]) or 0))\n",
        "    seq_roles = [(roles.get(u,\"UNK\"), roles.get(v,\"UNK\")) for u,v,_,_ in edges_t]\n",
        "    altern = 0\n",
        "    for i in range(len(seq_roles)-1):\n",
        "        a,b = seq_roles[i]; c,d = seq_roles[i+1]\n",
        "        if a.lower().startswith(\"mod\") and (not b.lower().startswith(\"mod\")) and (not c.lower().startswith(\"mod\")) and d.lower().startswith(\"mod\"):\n",
        "            altern += 1\n",
        "    alternation_ratio = altern / max(len(seq_roles)-1, 1)\n",
        "\n",
        "    # response lag mod->spec then spec->mod (seconds if timestamps are seconds; left as-is otherwise)\n",
        "    lags = []\n",
        "    last_mod_send = {}\n",
        "    for u,v,_,ed in edges_t:\n",
        "        ru, rv = roles.get(u,\"UNK\"), roles.get(v,\"UNK\")\n",
        "        t = edge_ts(ed)\n",
        "        if t is None:\n",
        "            continue\n",
        "        if ru.lower().startswith(\"mod\") and not rv.lower().startswith(\"mod\"):\n",
        "            last_mod_send[v] = t\n",
        "        if (not ru.lower().startswith(\"mod\")) and rv.lower().startswith(\"mod\"):\n",
        "            t0 = last_mod_send.get(u)\n",
        "            if t0 is not None:\n",
        "                try:\n",
        "                    lags.append(float(t - t0))\n",
        "                except Exception:\n",
        "                    pass\n",
        "    lag_med = float(np.median(lags)) if lags else np.nan\n",
        "    lag_cv  = float(np.std(lags)/np.mean(lags)) if len(lags)>1 and np.mean(lags)>0 else np.nan\n",
        "\n",
        "    # triad census (directed, normalized)\n",
        "    try:\n",
        "        tri = nx.triadic_census(D)\n",
        "        tri_tot = sum(tri.values()) or 1\n",
        "        tri_norm = {f\"tri_{k}\": v/tri_tot for k,v in tri.items()}\n",
        "    except Exception:\n",
        "        tri_norm = {}\n",
        "\n",
        "    # specialist-only structure\n",
        "    S = D.subgraph(specs).copy()\n",
        "    spec_has_edge   = float(S.number_of_edges() > 0)\n",
        "    spec_has_cycle  = float((S.number_of_edges() > 0) and (not nx.is_directed_acyclic_graph(S)))\n",
        "    try:\n",
        "        max_spec_cc = max((len(c) for c in nx.weakly_connected_components(S)), default=0)\n",
        "    except Exception:\n",
        "        max_spec_cc = 0.0\n",
        "    spec_edge_fraction = S.number_of_edges()/nE\n",
        "\n",
        "    # spectral mini-signature (undirected normalized Laplacian)\n",
        "    U = nx.Graph()\n",
        "    U.add_nodes_from(D.nodes(data=True))\n",
        "    for u,v in D.edges():\n",
        "        if u!=v: U.add_edge(u,v)\n",
        "    if U.number_of_nodes() > 0:\n",
        "        try:\n",
        "            L = nx.normalized_laplacian_matrix(U).A\n",
        "            evals = np.sort(np.linalg.eigvalsh(L))\n",
        "            eig1, eig2, eig3 = (float(evals[0]), float(evals[1] if len(evals)>1 else np.nan), float(evals[2] if len(evals)>2 else np.nan))\n",
        "        except Exception:\n",
        "            eig1 = eig2 = eig3 = np.nan\n",
        "    else:\n",
        "        eig1 = eig2 = eig3 = np.nan\n",
        "\n",
        "    feats = {\n",
        "        \"fanout_ratio\": fanout_ratio,\n",
        "        \"return_ratio\": return_ratio,\n",
        "        \"crosstalk_ratio\": crosstalk_ratio,\n",
        "        \"peer_fraction\": peer_fraction,\n",
        "        \"alternation_ratio\": alternation_ratio,\n",
        "        \"lag_median\": lag_med,\n",
        "        \"lag_cv\": lag_cv,\n",
        "        \"spec_has_edge\": spec_has_edge,\n",
        "        \"spec_has_cycle\": spec_has_cycle,\n",
        "        \"spec_max_cc\": float(max_spec_cc),\n",
        "        \"spec_edge_fraction\": float(spec_edge_fraction),\n",
        "        \"eig1\": eig1, \"eig2\": eig2, \"eig3\": eig3,\n",
        "    }\n",
        "    feats.update(tri_norm)\n",
        "    return feats\n",
        "\n",
        "# --- compute per graph ---\n",
        "rows = []\n",
        "for p, b in zip(base[\"file\"], base[\"bundle\"]):\n",
        "    try:\n",
        "        G = loadG(p)\n",
        "        rec = {\"file\": p, \"bundle\": b}\n",
        "        rec.update(tiny_topo_features(G))\n",
        "        rows.append(rec)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "grey = pd.DataFrame(rows)\n",
        "grey.to_csv(OUT_DIR / \"graph_topo_grey.csv\", index=False)\n",
        "\n",
        "# merge & save\n",
        "merged = base.merge(grey, on=[\"file\",\"bundle\"], how=\"left\")\n",
        "merged.to_csv(OUT_DIR / \"graph_stats_greysmart.csv\", index=False)\n",
        "\n",
        "print(\"WROTE:\")\n",
        "print(\" -\", OUT_DIR / \"graph_topo_grey.csv\")\n",
        "print(\" -\", OUT_DIR / \"graph_stats_greysmart.csv\")\n",
        "\n",
        "# quick sanity plots/tables\n",
        "import matplotlib.pyplot as plt\n",
        "merged[\"cohort\"] = np.where(merged[\"bundle\"]==\"Attack_bundle\",\"attack\",\"normal\")\n",
        "for col in [\"crosstalk_ratio\",\"alternation_ratio\",\"lag_median\"]:\n",
        "    if col in merged.columns:\n",
        "        plt.figure(figsize=(5,4))\n",
        "        merged.boxplot(column=col, by=\"cohort\")\n",
        "        plt.title(f\"{col} by cohort\"); plt.suptitle(\"\")\n",
        "        plt.tight_layout()\n",
        "        outp = FIG_DIR / f\"{col}_by_cohort.png\"\n",
        "        plt.savefig(outp, dpi=180); plt.close()\n",
        "        print(\"FIG:\", outp)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "YfIVd_cpo965",
        "outputId": "17f192f4-ada7-454a-b2c0-fcd485a39751"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WROTE:\n",
            " - /content/drive/MyDrive/graph_stats_out/graph_topo_grey.csv\n",
            " - /content/drive/MyDrive/graph_stats_out/graph_stats_greysmart.csv\n",
            "FIG: /content/drive/MyDrive/graph_stats_out/figs/crosstalk_ratio_by_cohort.png\n",
            "FIG: /content/drive/MyDrive/graph_stats_out/figs/alternation_ratio_by_cohort.png\n",
            "FIG: /content/drive/MyDrive/graph_stats_out/figs/lag_median_by_cohort.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x400 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell X: Extract node + edge attributes (numeric/categorical/text) into per-graph features ===\n",
        "import re, json, math, collections\n",
        "from pathlib import Path\n",
        "from typing import Dict, Any, List, Tuple\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "# ---- Configs you can tweak ----\n",
        "TOPK_NODE_CATS = 250       # keep top-K (attr=value) categorical features from nodes across all graphs\n",
        "TOPK_EDGE_CATS = 250       # keep top-K (attr=value) categorical features from edges across all graphs\n",
        "TEXT_KEYS = [\n",
        "    \"message_text\", \"moderator_text\",        # <-- your actual fields\n",
        "    \"message\",\"text\",\"content\",\"prompt\",\"response\",\n",
        "    \"assistant_text\",\"user_msg\",\"msg\",\"utterance\"\n",
        "]\n",
        "EDGE_TEXT_MAX_FEATURES = 1000  # TF-IDF max features\n",
        "RANDOM_STATE = 0\n",
        "\n",
        "def get_graph_id(G, path: Path):\n",
        "    gid = None\n",
        "    try:\n",
        "        gid = G.graph.get(\"case_id\") or G.graph.get(\"graph_id\") or G.graph.get(\"id\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    return str(gid) if gid else path.stem\n",
        "\n",
        "def gather_graph_attr_features(G: nx.Graph):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      num_feats: dict of numeric aggregates (per-graph)\n",
        "      node_cat_counts: dict of 'N:attr=value' -> count\n",
        "      edge_cat_counts: dict of 'E:attr=value' -> count\n",
        "      edge_text_doc: concatenated text from edges\n",
        "    \"\"\"\n",
        "    # ---------- NODE SIDE ----------\n",
        "    node_num_values = collections.defaultdict(list)\n",
        "    node_cat_counts = collections.defaultdict(int)\n",
        "\n",
        "    for _, attrs in G.nodes(data=True):\n",
        "        for k, v in (attrs or {}).items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            if is_number(v):\n",
        "                node_num_values[f\"node_{k}\"].append(float(v))\n",
        "            elif is_stringy(v) or isinstance(v, bool):\n",
        "                node_cat_counts[f\"N:{k}={v}\"] += 1\n",
        "\n",
        "    num_feats = {}\n",
        "    for key, vals in node_num_values.items():\n",
        "        arr = np.array(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    # ---------- EDGE SIDE ----------\n",
        "    edge_num_values = collections.defaultdict(list)\n",
        "    edge_cat_counts = collections.defaultdict(int)\n",
        "    text_bits = []\n",
        "\n",
        "    for _, _, attrs in G.edges(data=True):\n",
        "        for k, v in (attrs or {}).items():\n",
        "            if v is None:\n",
        "                continue\n",
        "            if k.lower() in TEXT_KEYS and is_stringy(v):\n",
        "                text_bits.append(str(v))\n",
        "            if is_number(v):\n",
        "                edge_num_values[f\"edge_{k}\"].append(float(v))\n",
        "            elif is_stringy(v) or isinstance(v, bool):\n",
        "                edge_cat_counts[f\"E:{k}={v}\"] += 1\n",
        "\n",
        "    for key, vals in edge_num_values.items():\n",
        "        arr = np.array(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    edge_text_doc = \" \".join(text_bits) if text_bits else \"\"\n",
        "    return num_feats, node_cat_counts, edge_cat_counts, edge_text_doc\n",
        "\n",
        "# ---- Pass over all graphs; collect raw per-graph features ----\n",
        "rows_num, rows_meta = [], []\n",
        "node_cat_bag_global = collections.Counter()\n",
        "edge_cat_bag_global = collections.Counter()\n",
        "edge_text_docs_by_file = {}\n",
        "\n",
        "for bundle_dir in BUNDLE_DIRS:\n",
        "    for p in list_gpickle_exports(Path(bundle_dir)):\n",
        "        try:\n",
        "            G = load_gpickle(p)\n",
        "            num_feats, node_cat_counts, edge_cat_counts, text_doc = gather_graph_attr_features(G)\n",
        "            rec = {\"graph_id\": get_graph_id(G, p), \"file\": str(p), \"bundle\": Path(bundle_dir).name}\n",
        "            rec.update(num_feats)\n",
        "            rows_num.append(rec)\n",
        "            rows_meta.append({\"graph_id\": rec[\"graph_id\"], \"file\": rec[\"file\"], \"bundle\": rec[\"bundle\"]})\n",
        "            node_cat_bag_global.update(node_cat_counts)\n",
        "            edge_cat_bag_global.update(edge_cat_counts)\n",
        "            edge_text_docs_by_file[str(p)] = text_doc\n",
        "        except Exception as e:\n",
        "            rows_num.append({\"graph_id\": f\"ERR::{p.stem}\", \"file\": str(p), \"bundle\": Path(bundle_dir).name, \"error\": str(e)})\n",
        "            rows_meta.append({\"graph_id\": f\"ERR::{p.stem}\", \"file\": str(p), \"bundle\": Path(bundle_dir).name})\n",
        "            edge_text_docs_by_file[str(p)] = \"\"\n",
        "\n",
        "df_num  = pd.DataFrame(rows_num)\n",
        "df_meta = pd.DataFrame(rows_meta)\n",
        "\n",
        "print(\"Graphs processed:\", len(df_num))\n",
        "print(\"Numeric attr columns:\", len([c for c in df_num.columns if c not in ['graph_id','file','bundle','error']]))\n",
        "print(\"Global node categorical keys:\", len(node_cat_bag_global))\n",
        "print(\"Global edge categorical keys:\", len(edge_cat_bag_global))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oASFIuMio99q",
        "outputId": "a71ea441-2540-496d-bb4c-2da9d9233860"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graphs processed: 770\n",
            "Numeric attr columns: 282\n",
            "Global node categorical keys: 776\n",
            "Global edge categorical keys: 14700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell Y: Build categorical & text features; merge with structural table ===\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "\n",
        "# 1) Prune categorical vocabularies to top-K globally\n",
        "top_node_cats = set(k for k, _ in node_cat_bag_global.most_common(TOPK_NODE_CATS))\n",
        "top_edge_cats = set(k for k, _ in edge_cat_bag_global.most_common(TOPK_EDGE_CATS))\n",
        "\n",
        "node_cat_cols = sorted(top_node_cats)\n",
        "edge_cat_cols = sorted(top_edge_cats)\n",
        "node_cat_index = {k:i for i,k in enumerate(node_cat_cols)}\n",
        "edge_cat_index = {k:i for i,k in enumerate(edge_cat_cols)}\n",
        "\n",
        "# 2) Re-scan graphs to build per-graph categorical rows aligned to df_num order\n",
        "node_cat_rows, edge_cat_rows = [], []\n",
        "\n",
        "for _, row in df_num.iterrows():\n",
        "    f = row[\"file\"]\n",
        "    # to keep things simple, re-load the graph (small overhead, clear alignment)\n",
        "    try:\n",
        "        G = load_gpickle(Path(f))\n",
        "        _, node_cat_counts, edge_cat_counts, _ = gather_graph_attr_features(G)\n",
        "    except Exception:\n",
        "        node_cat_counts, edge_cat_counts = {}, {}\n",
        "\n",
        "    # node row\n",
        "    n_idx, n_val = [], []\n",
        "    for k, v in node_cat_counts.items():\n",
        "        if k in node_cat_index:\n",
        "            n_idx.append(node_cat_index[k]); n_val.append(float(v))\n",
        "    node_cat_rows.append(sparse.csr_matrix((n_val, ([0]*len(n_idx), n_idx)), shape=(1, len(node_cat_cols))))\n",
        "\n",
        "    # edge row\n",
        "    e_idx, e_val = [], []\n",
        "    for k, v in edge_cat_counts.items():\n",
        "        if k in edge_cat_index:\n",
        "            e_idx.append(edge_cat_index[k]); e_val.append(float(v))\n",
        "    edge_cat_rows.append(sparse.csr_matrix((e_val, ([0]*len(e_idx), e_idx)), shape=(1, len(edge_cat_cols))))\n",
        "\n",
        "X_node_cat = sparse.vstack(node_cat_rows) if node_cat_rows else sparse.csr_matrix((len(df_num), 0))\n",
        "X_edge_cat = sparse.vstack(edge_cat_rows) if edge_cat_rows else sparse.csr_matrix((len(df_num), 0))\n",
        "print(\"Node categorical matrix:\", X_node_cat.shape)\n",
        "print(\"Edge categorical matrix:\", X_edge_cat.shape)\n",
        "\n",
        "# 3) Edge text TF-IDF aligned to df_num order\n",
        "df_num[\"edge_text_doc\"] = df_num[\"file\"].map(edge_text_docs_by_file).fillna(\"\").astype(str)\n",
        "docs = df_num[\"edge_text_doc\"].tolist()\n",
        "\n",
        "if not any(s.strip() for s in docs):\n",
        "    # nothing to vectorize\n",
        "    X_text = sparse.csr_matrix((len(docs), 0))\n",
        "else:\n",
        "    try:\n",
        "        # word-level first (no stop-word removal to avoid empty vocab)\n",
        "        tfidf = TfidfVectorizer(\n",
        "            max_features=EDGE_TEXT_MAX_FEATURES,\n",
        "            stop_words=None,\n",
        "            lowercase=True,\n",
        "            token_pattern=r\"(?u)\\b\\w+\\b\"  # keep 1+ char \"words\"\n",
        "        )\n",
        "        X_text = tfidf.fit_transform(docs)\n",
        "    except ValueError:\n",
        "        # fallback: character n-grams (very robust)\n",
        "        tfidf = TfidfVectorizer(\n",
        "            analyzer=\"char\",\n",
        "            ngram_range=(3,5),\n",
        "            max_features=EDGE_TEXT_MAX_FEATURES\n",
        "        )\n",
        "        X_text = tfidf.fit_transform(docs)\n",
        "\n",
        "print(\"Edge text TF-IDF:\", X_text.shape)\n",
        "# df_num[\"edge_text_doc\"] = df_num[\"file\"].map(edge_text_docs_by_file).fillna(\"\").astype(str)\n",
        "# tfidf = TfidfVectorizer(max_features=EDGE_TEXT_MAX_FEATURES, stop_words=\"english\")\n",
        "# X_text = tfidf.fit_transform(df_num[\"edge_text_doc\"].tolist()) if len(df_num) else sparse.csr_matrix((0,0))\n",
        "# print(\"Edge text TF-IDF:\", X_text.shape)\n",
        "\n",
        "# 4) Numeric matrix from df_num (drop metadata)\n",
        "meta_cols = [\"graph_id\",\"file\",\"bundle\",\"edge_text_doc\",\"error\"]\n",
        "num_cols = [c for c in df_num.columns if c not in meta_cols and pd.api.types.is_numeric_dtype(df_num[c])]\n",
        "X_num = sparse.csr_matrix(df_num[num_cols].fillna(0.0).values) if num_cols else sparse.csr_matrix((len(df_num), 0))\n",
        "print(\"Numeric attr matrix:\", X_num.shape)\n",
        "\n",
        "# 5) Combine node/edge attr + text into one matrix\n",
        "X_attrs = sparse.hstack([X_num, X_node_cat, X_edge_cat, X_text]).tocsr()\n",
        "print(\"Combined attribute matrix:\", X_attrs.shape)\n",
        "\n",
        "# 6) Merge with structural/round-trip table on 'file'\n",
        "struct_df = pd.read_csv(OUT_DIR / \"graph_stats_enriched.csv\").drop_duplicates(subset=[\"file\"])\n",
        "df_num = df_num.drop_duplicates(subset=[\"file\"])\n",
        "\n",
        "# Inner-join by file; this 'merged' is the SINGLE SOURCE OF TRUTH for row order\n",
        "merged = struct_df.merge(df_num[[\"file\"]], on=\"file\", how=\"inner\").reset_index(drop=True)\n",
        "\n",
        "# Pick numeric structural columns from MERGED (not from struct_df)\n",
        "struct_num_cols = [\n",
        "    c for c in merged.columns\n",
        "    if c not in [\"file\", \"bundle\", \"graph_id\", \"cohort\"] and pd.api.types.is_numeric_dtype(merged[c])\n",
        "]\n",
        "\n",
        "# Build structural matrix directly from MERGED to preserve row order\n",
        "from scipy import sparse\n",
        "X_struct = sparse.csr_matrix(merged[struct_num_cols].fillna(0.0).values) if struct_num_cols else sparse.csr_matrix((len(merged), 0))\n",
        "print(\"Structural matrix:\", X_struct.shape)\n",
        "\n",
        "# Align attribute matrix rows to MERGED order via file mapping\n",
        "file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "row_indices = [file_to_row[f] for f in merged[\"file\"].tolist()]\n",
        "X_attrs_aligned = X_attrs[row_indices, :]\n",
        "\n",
        "# Final features + labels\n",
        "from scipy.sparse import hstack\n",
        "X_final = hstack([X_struct, X_attrs_aligned]).tocsr()\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "\n",
        "# Sanity: dimensions should all match\n",
        "assert X_struct.shape[0] == X_attrs_aligned.shape[0] == X_final.shape[0] == len(y)\n",
        "print(\"X_final:\", X_final.shape, \"| y:\", y.shape)\n",
        "print(\"Feature breakdown:\",\n",
        "      \"\\n - structural:\", X_struct.shape[1],\n",
        "      \"\\n - numeric attrs:\", X_num.shape[1],\n",
        "      \"\\n - node cats:\", X_node_cat.shape[1],\n",
        "      \"\\n - edge cats:\", X_edge_cat.shape[1],\n",
        "      \"\\n - text tfidf:\", X_text.shape[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uGKPw427o-Ax",
        "outputId": "6cf71ab8-7c64-46fc-b447-1824de08903c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node categorical matrix: (770, 250)\n",
            "Edge categorical matrix: (770, 250)\n",
            "Edge text TF-IDF: (770, 1000)\n",
            "Numeric attr matrix: (770, 282)\n",
            "Combined attribute matrix: (770, 1782)\n",
            "Structural matrix: (770, 80)\n",
            "X_final: (770, 1862) | y: (770,)\n",
            "Feature breakdown: \n",
            " - structural: 80 \n",
            " - numeric attrs: 282 \n",
            " - node cats: 250 \n",
            " - edge cats: 250 \n",
            " - text tfidf: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy import sparse\n",
        "\n",
        "# scale only numeric blocks\n",
        "X_num_scaled    = StandardScaler(with_mean=False).fit_transform(X_num)\n",
        "X_struct_scaled = StandardScaler(with_mean=False).fit_transform(X_struct)\n",
        "\n",
        "X_attrs_scaled  = sparse.hstack([X_num_scaled, X_node_cat, X_edge_cat, X_text]).tocsr()\n",
        "X_final_scaled  = sparse.hstack([X_struct_scaled, X_attrs_aligned]).tocsr()\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf = LogisticRegression(\n",
        "    solver=\"saga\", penalty=\"l2\",\n",
        "    C=0.25,             # slightly stronger reg\n",
        "    max_iter=5000, tol=1e-3,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=0, n_jobs=-1\n",
        ")\n"
      ],
      "metadata": {
        "id": "aRvCmG4jo-Dq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "print(\"n rows:\", X_final.shape[0], \"| y pos rate:\", float(np.mean(y)))\n",
        "\n",
        "# 1) Refit once and look at classwise score means\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf_dbg = LogisticRegression(\n",
        "    solver=\"saga\", penalty=\"l2\", C=0.25, max_iter=5000, tol=1e-3,\n",
        "    class_weight=\"balanced\", random_state=0, n_jobs=-1\n",
        ")\n",
        "clf_dbg.fit(X_final, y)\n",
        "probs_dbg = clf_dbg.predict_proba(X_final)[:,1]\n",
        "print(\"AUC on full (optimistic, just for sanity):\", round(roc_auc_score(y, probs_dbg), 3))\n",
        "print(\"Mean score y=0:\", float(np.mean(probs_dbg[y==0])) , \" | y=1:\", float(np.mean(probs_dbg[y==1])))\n",
        "\n",
        "# 2) Are file rows aligned? Check 5 random indices\n",
        "import random\n",
        "ixs = random.sample(range(len(y)), 5)\n",
        "for i in ixs:\n",
        "    print(i, merged.loc[i,\"file\"], y[i], probs_dbg[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nW9jptoapWmS",
        "outputId": "2a5ab508-dd8e-4f3c-a1ea-e2f172595af5"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "n rows: 770 | y pos rate: 0.5155844155844156\n",
            "AUC on full (optimistic, just for sanity): 0.98\n",
            "Mean score y=0: 0.4999787236699313  | y=1: 0.4999912456181319\n",
            "58 /content/drive/MyDrive/Attack_bundle/_exports/2af5beb9a71a9337be75113b065a26bc.gpickle 1 0.5000007746241107\n",
            "120 /content/drive/MyDrive/Attack_bundle/_exports/4d3b96b67186f73ff88c9d339256edb0.gpickle 1 0.5000007746245658\n",
            "97 /content/drive/MyDrive/Attack_bundle/_exports/3d30bf52721a9d7b09bff9c721783176.gpickle 1 0.50000077463953\n",
            "435 /content/drive/MyDrive/logs_bundle/_exports/fec3d3e8adc47a13fb8360d2f86a9d32.gpickle 0 0.5000007744333673\n",
            "630 /content/drive/MyDrive/logs_bundle2/_exports/d859bacbb7ae8f9d2f69338df1fd391f.gpickle 0 0.5000007744381628\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from scipy import sparse\n",
        "\n",
        "# 6) Rebuild 'merged' to include grey and round-trip summaries BEFORE selecting numeric columns\n",
        "struct_df = pd.read_csv(OUT_DIR / \"graph_stats_enriched.csv\").drop_duplicates(subset=[\"file\"])\n",
        "\n",
        "# optional: merge greysmart (adds fanout/return/crosstalk/triads/spectral etc.)\n",
        "p_grey = OUT_DIR / \"graph_stats_greysmart.csv\"\n",
        "if p_grey.exists():\n",
        "    df_grey = pd.read_csv(p_grey)\n",
        "    struct_df = struct_df.merge(df_grey, on=[\"file\",\"bundle\"], how=\"left\")\n",
        "\n",
        "# optional: merge roundtrip per-graph summary (rt_* columns)\n",
        "p_rt = OUT_DIR / \"roundtrip_summary.csv\"\n",
        "if p_rt.exists():\n",
        "    df_rt = pd.read_csv(p_rt)\n",
        "    # 'file' is present -> safest key\n",
        "    if \"file\" in df_rt.columns:\n",
        "        struct_df = struct_df.merge(df_rt.drop_duplicates(subset=[\"file\"]), on=\"file\", how=\"left\")\n",
        "    else:\n",
        "        # fallback if needed\n",
        "        join_keys = [k for k in [\"case_id\",\"bundle\"] if k in struct_df.columns and k in df_rt.columns]\n",
        "        if join_keys:\n",
        "            struct_df = struct_df.merge(df_rt, on=join_keys, how=\"left\")\n",
        "\n",
        "# Ensure we only keep rows we have attrs for (df_num)\n",
        "df_num = df_num.drop_duplicates(subset=[\"file\"])\n",
        "merged = struct_df.merge(df_num[[\"file\"]], on=\"file\", how=\"inner\").reset_index(drop=True)\n",
        "\n",
        "# Build structural numeric matrix from THIS merged table\n",
        "struct_exclude = {\"file\",\"bundle\",\"graph_id\",\"cohort\"}\n",
        "struct_num_cols = [c for c in merged.columns if c not in struct_exclude and pd.api.types.is_numeric_dtype(merged[c])]\n",
        "X_struct = sparse.csr_matrix(merged[struct_num_cols].fillna(0.0).values) if struct_num_cols else sparse.csr_matrix((len(merged),0))\n",
        "print(\"Structural columns:\", len(struct_num_cols))\n",
        "\n",
        "# Align attributes to merged order (same as before)\n",
        "file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "row_indices = [file_to_row[f] for f in merged[\"file\"].tolist()]\n",
        "X_attrs_aligned = X_attrs[row_indices, :]\n",
        "\n",
        "# (optional) scale numeric blocks\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_struct_scaled = StandardScaler(with_mean=False).fit_transform(X_struct)\n",
        "X_num_scaled    = StandardScaler(with_mean=False).fit_transform(X_num)\n",
        "from scipy import sparse\n",
        "X_attrs_scaled  = sparse.hstack([X_num_scaled, X_node_cat, X_edge_cat, X_text]).tocsr()\n",
        "X_attrs_scaled_aligned = X_attrs_scaled[row_indices, :]\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "X_final = hstack([X_struct_scaled, X_attrs_scaled_aligned]).tocsr()\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "\n",
        "print(\"X_final:\", X_final.shape, \"| y:\", y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ZZk8LFKHpWpV",
        "outputId": "3480dea2-1604-455e-c750-5a5d8b46854d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Structural columns: 198\n",
            "X_final: (770, 1980) | y: (770,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Cell Z: Classification on (structural + node/edge attrs + edge text) ===\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score, balanced_accuracy_score, f1_score, confusion_matrix\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    solver=\"saga\", penalty=\"l2\",\n",
        "    C=0.25, max_iter=5000, tol=1e-3,\n",
        "    class_weight=\"balanced\",\n",
        "    random_state=0, n_jobs=-1\n",
        ")\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)\n",
        "probs = np.zeros(len(y))\n",
        "preds = np.zeros(len(y))\n",
        "\n",
        "for tr, te in skf.split(X_final, y):\n",
        "    clf.fit(X_final[tr], y[tr])\n",
        "    p = clf.predict_proba(X_final[te])[:, 1]\n",
        "    probs[te] = p\n",
        "    preds[te] = (p >= 0.5).astype(int)\n",
        "\n",
        "print(\"=== Combined Features ===\")\n",
        "print(\"BalancedAcc:\", round(balanced_accuracy_score(y, preds), 3))\n",
        "print(\"F1:\", round(f1_score(y, preds), 3))\n",
        "print(\"ROC-AUC:\", round(roc_auc_score(y, probs), 3))\n",
        "print(\"PR-AUC:\", round(average_precision_score(y, probs), 3))\n",
        "print(\"Confusion:\\n\", confusion_matrix(y, preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jr0jBO_PpWsL",
        "outputId": "e92802e4-f8bc-4547-ecc5-3c3cd61b4480"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Combined Features ===\n",
            "BalancedAcc: 0.82\n",
            "F1: 0.794\n",
            "ROC-AUC: 0.875\n",
            "PR-AUC: 0.898\n",
            "Confusion:\n",
            " [[355  18]\n",
            " [124 273]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "from scipy import sparse\n",
        "\n",
        "# 1) TEXT (char 3–5 grams, 2^18 dim ≈ 262,144)\n",
        "hv = HashingVectorizer(analyzer=\"char\", ngram_range=(3,5), n_features=2**18, alternate_sign=False, lowercase=True)\n",
        "X_text_hash = hv.transform(df_num[\"edge_text_doc\"].astype(str).tolist())\n",
        "\n",
        "# 2) CATEGORICAL (turn your per-graph cat counts dicts into features)\n",
        "#    re-scan graphs ONCE to get dictionaries; DO NOT prune by global Top-K\n",
        "def cat_dicts_for_graph(G):\n",
        "    _, node_cat, edge_cat, _ = gather_graph_attr_features(G)\n",
        "    # node_cat/edge_cat are Counter-like; convert to dict[str->count]\n",
        "    d = dict(node_cat);\n",
        "    for k,v in edge_cat.items(): d[k] = d.get(k,0)+v\n",
        "    return d\n",
        "\n",
        "cat_dicts = []\n",
        "for f in df_num[\"file\"]:\n",
        "    try:\n",
        "        G = load_gpickle(Path(f))\n",
        "        cat_dicts.append(cat_dicts_for_graph(G))\n",
        "    except:\n",
        "        cat_dicts.append({})\n",
        "\n",
        "# 2^17 dims for cats\n",
        "fh = FeatureHasher(n_features=2**17, input_type=\"dict\", alternate_sign=False)\n",
        "X_cat_hash = fh.transform(cat_dicts)\n",
        "\n",
        "# 3) NUMERIC (same as before)\n",
        "X_num_mat = sparse.csr_matrix(df_num[num_cols].fillna(0.0).values)\n",
        "\n",
        "# 4) ATTRS = num + cats + text (hash)\n",
        "X_attrs_hash = sparse.hstack([X_num_mat, X_cat_hash, X_text_hash]).tocsr()\n",
        "\n",
        "# 5) Align to 'merged' order and combine with STRUCT as before\n",
        "file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "row_idx = [file_to_row[f] for f in merged[\"file\"].tolist()]\n",
        "X_attrs_hash_aligned = X_attrs_hash[row_idx,:]\n",
        "\n",
        "# (optional) scale numeric blocks only; leave hashed blocks unscaled\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_struct_scaled = StandardScaler(with_mean=False).fit_transform(X_struct)\n",
        "X_num_scaled    = StandardScaler(with_mean=False).fit_transform(X_num_mat)\n",
        "X_attrs_hash_aligned = sparse.hstack([X_num_scaled, X_cat_hash[row_idx,:], X_text_hash[row_idx,:]]).tocsr()\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "X_final_hash = hstack([X_struct_scaled, X_attrs_hash_aligned]).tocsr()\n"
      ],
      "metadata": {
        "id": "m_y0c303tVNl"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sizes\n",
        "print(\"struct:\", X_struct.shape[1])\n",
        "print(\"num:\", X_num.shape[1], \"node_cat:\", X_node_cat.shape[1], \"edge_cat:\", X_edge_cat.shape[1], \"text:\", X_text.shape[1])\n",
        "print(\"attrs total:\", X_num.shape[1] + X_node_cat.shape[1] + X_edge_cat.shape[1] + X_text.shape[1])\n",
        "\n",
        "# ablations\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "\n",
        "def cv_auc(X, y):\n",
        "    skf = StratifiedKFold(10, shuffle=True, random_state=42)\n",
        "    aucs, aps = [], []\n",
        "    pipe = make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        StandardScaler(with_mean=False),\n",
        "        LogisticRegression(solver=\"saga\", penalty=\"l2\", C=0.25, max_iter=5000, tol=1e-3,\n",
        "                           class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
        "    )\n",
        "    for tr, te in skf.split(X, y):\n",
        "        pipe.fit(X[tr], y[tr])\n",
        "        p = pipe.predict_proba(X[te])[:,1]\n",
        "        aucs.append(roc_auc_score(y[te], p))\n",
        "        aps.append(average_precision_score(y[te], p))\n",
        "    return round(np.mean(aucs),3), round(np.mean(aps),3)\n",
        "\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "# structure-only\n",
        "auc_s, ap_s = cv_auc(X_struct_scaled, y)\n",
        "print(\"Structure-only AUC/AP:\", auc_s, ap_s)\n",
        "\n",
        "# attrs-only (aligned)\n",
        "attrs_only = X_attrs_scaled_aligned\n",
        "auc_a, ap_a = cv_auc(attrs_only, y)\n",
        "print(\"Attrs-only AUC/AP:\", auc_a, ap_a)\n",
        "\n",
        "# combined (what you ran)\n",
        "combined = hstack([X_struct_scaled, attrs_only]).tocsr()\n",
        "auc_c, ap_c = cv_auc(combined, y)\n",
        "print(\"Combined AUC/AP:\", auc_c, ap_c)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9ulz8se5o-Gs",
        "outputId": "dd224b28-05b5-4826-a7c6-547f867dcd00"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "struct: 198\n",
            "num: 282 node_cat: 250 edge_cat: 250 text: 1000\n",
            "attrs total: 1782\n",
            "Structure-only AUC/AP: 0.844 0.893\n",
            "Attrs-only AUC/AP: 0.98 0.982\n",
            "Combined AUC/AP: 0.977 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy import sparse\n",
        "from collections import Counter\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "aucs, aps = [], []\n",
        "\n",
        "# Precompute per-graph raw materials once\n",
        "files_order = merged[\"file\"].tolist()\n",
        "docs_all = [edge_text_docs_by_file.get(f, \"\") for f in files_order]\n",
        "\n",
        "# For cats: store raw per-graph Counter dicts, not Top-K yet\n",
        "raw_node_cats, raw_edge_cats = [], []\n",
        "for f in files_order:\n",
        "    try:\n",
        "        G = load_gpickle(Path(f))\n",
        "        _, ncat, ecat, _ = gather_graph_attr_features(G)\n",
        "    except:\n",
        "        ncat, ecat = Counter(), Counter()\n",
        "    raw_node_cats.append(ncat)\n",
        "    raw_edge_cats.append(ecat)\n",
        "\n",
        "# Numeric blocks ready\n",
        "X_struct_scaled = StandardScaler(with_mean=False).fit_transform(X_struct)\n",
        "X_num_scaled    = StandardScaler(with_mean=False).fit_transform(X_num)\n",
        "\n",
        "for tr, te in skf.split(merged, (merged[\"cohort\"].str.lower()==\"attack\").astype(int).values):\n",
        "    # --- build fold-specific cat vocab (Top-K) on training only\n",
        "    TOPK_NODE, TOPK_EDGE, MAX_TXT = 250, 250, 1000\n",
        "    bagN, bagE = Counter(), Counter()\n",
        "    for i in tr:\n",
        "        bagN.update(raw_node_cats[i])\n",
        "        bagE.update(raw_edge_cats[i])\n",
        "    node_cols = [k for k,_ in bagN.most_common(TOPK_NODE)]\n",
        "    edge_cols = [k for k,_ in bagE.most_common(TOPK_EDGE)]\n",
        "    node_idx = {k:i for i,k in enumerate(node_cols)}\n",
        "    edge_idx = {k:i for i,k in enumerate(edge_cols)}\n",
        "\n",
        "    def row_from_counts(cnt, index):\n",
        "        cols, vals = [], []\n",
        "        for k,v in cnt.items():\n",
        "            if k in index:\n",
        "                cols.append(index[k]); vals.append(float(v))\n",
        "        return sparse.csr_matrix((vals, ([0]*len(cols), cols)), shape=(1, len(index)))\n",
        "\n",
        "    X_node_cat_tr = sparse.vstack([row_from_counts(raw_node_cats[i], node_idx) for i in tr])\n",
        "    X_node_cat_te = sparse.vstack([row_from_counts(raw_node_cats[i], node_idx) for i in te])\n",
        "    X_edge_cat_tr = sparse.vstack([row_from_counts(raw_edge_cats[i], edge_idx) for i in tr])\n",
        "    X_edge_cat_te = sparse.vstack([row_from_counts(raw_edge_cats[i], edge_idx) for i in te])\n",
        "\n",
        "    # --- fold-specific TF-IDF fit on training only\n",
        "    tfidf = TfidfVectorizer(max_features=MAX_TXT, analyzer=\"char\", ngram_range=(3,5))\n",
        "    X_text_tr = tfidf.fit_transform([docs_all[i] for i in tr])\n",
        "    X_text_te = tfidf.transform([docs_all[i] for i in te])\n",
        "\n",
        "    # --- numeric attrs (already scaled globally; fine)\n",
        "    X_num_tr = X_num_scaled[tr,:]; X_num_te = X_num_scaled[te,:]\n",
        "    # --- structural (already scaled)\n",
        "    X_struct_tr = X_struct_scaled[tr,:]; X_struct_te = X_struct_scaled[te,:]\n",
        "\n",
        "    # --- build fold matrices\n",
        "    X_attrs_tr = sparse.hstack([X_num_tr, X_node_cat_tr, X_edge_cat_tr, X_text_tr]).tocsr()\n",
        "    X_attrs_te = sparse.hstack([X_num_te, X_node_cat_te, X_edge_cat_te, X_text_te]).tocsr()\n",
        "\n",
        "    X_tr = sparse.hstack([X_struct_tr, X_attrs_tr]).tocsr()\n",
        "    X_te = sparse.hstack([X_struct_te, X_attrs_te]).tocsr()\n",
        "\n",
        "    y = (merged[\"cohort\"].str.lower()==\"attack\").astype(int).values\n",
        "    y_tr, y_te = y[tr], y[te]\n",
        "\n",
        "    pipe = make_pipeline(\n",
        "        SimpleImputer(strategy=\"median\"),\n",
        "        StandardScaler(with_mean=False),\n",
        "        LogisticRegression(solver=\"saga\", penalty=\"l2\", C=0.25, max_iter=5000, tol=1e-3,\n",
        "                           class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
        "    )\n",
        "    pipe.fit(X_tr, y_tr)\n",
        "    p = pipe.predict_proba(X_te)[:,1]\n",
        "    aucs.append(roc_auc_score(y_te, p))\n",
        "    aps.append(average_precision_score(y_te, p))\n",
        "\n",
        "print(\"Fold-safe Combined AUC/AP:\", round(np.mean(aucs),3), round(np.mean(aps),3))\n"
      ],
      "metadata": {
        "id": "jS2yrlTGo-Jk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d3d3400e-99c7-41a5-fecc-6c787a2bf736"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold-safe Combined AUC/AP: 0.969 0.969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"STRUCT cols:\", len(struct_num_cols))\n",
        "print(\"NUM attrs:\", len(num_cols), \"| NODE cats:\", len(node_cat_cols), \"| EDGE cats:\", len(edge_cat_cols), \"| TEXT:\", X_text.shape[1])\n",
        "\n",
        "# Sanity: ensure no forbidden columns\n",
        "for bad in [\"bundle\",\"cohort\",\"Attack_bundle\"]:\n",
        "    assert all(bad not in c for c in struct_num_cols), f\"Leakage via {bad} in structural columns!\"\n",
        "\n",
        "# Check a couple of rows' nnz per block\n",
        "from scipy.sparse import issparse\n",
        "def nnz(M): return int(M.nnz) if issparse(M) else int((M!=0).sum())\n",
        "i = 0\n",
        "print(\"Row 0 nnz — struct:\", nnz(X_struct[i]), \" | num:\", nnz(X_num[i]), \" | nodecat:\", nnz(X_node_cat[i]),\n",
        "      \" | edgecat:\", nnz(X_edge_cat[i]), \" | text:\", nnz(X_text[i]))\n"
      ],
      "metadata": {
        "id": "ll5yZ6vfv9Yh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "646526ea-cd10-4170-8f5f-020fdb81bae0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STRUCT cols: 198\n",
            "NUM attrs: 282 | NODE cats: 250 | EDGE cats: 250 | TEXT: 1000\n",
            "Row 0 nnz — struct: 160  | num: 200  | nodecat: 6  | edgecat: 12  | text: 131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build names in the same order as X_final\n",
        "feat_names = []\n",
        "feat_names += [f\"STRUCT::{c}\" for c in struct_num_cols]\n",
        "feat_names += [f\"ATTR_NUM::{c}\" for c in num_cols]\n",
        "feat_names += [f\"NODE_CAT::{c}\" for c in node_cat_cols]\n",
        "feat_names += [f\"EDGE_CAT::{c}\" for c in edge_cat_cols]\n",
        "feat_names += [f\"TEXT::{t}\" for t in tfidf.get_feature_names_out()]  # only if using TF-IDF (not hashing)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "clf_all = LogisticRegression(solver=\"saga\", penalty=\"l2\", C=0.25, max_iter=5000, tol=1e-3,\n",
        "                             class_weight=\"balanced\", random_state=0, n_jobs=-1)\n",
        "clf_all.fit(X_final, y)\n",
        "coefs = clf_all.coef_.ravel()\n",
        "\n",
        "# top +/- 25\n",
        "idx_top = np.argsort(coefs)[-25:][::-1]\n",
        "idx_bot = np.argsort(coefs)[:25]\n",
        "\n",
        "print(\"\\nTop positive features:\")\n",
        "for i in idx_top:\n",
        "    print(f\"{feat_names[i]:60s}  {coefs[i]:+.4f}\")\n",
        "\n",
        "print(\"\\nTop negative features:\")\n",
        "for i in idx_bot:\n",
        "    print(f\"{feat_names[i]:60s}  {coefs[i]:+.4f}\")\n"
      ],
      "metadata": {
        "id": "RzRrSIPtv9eQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4055852d-9210-4960-a80d-f8c00e3b99ec"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Top positive features:\n",
            "ATTR_NUM::node_power_w_mean__mean                             +0.0005\n",
            "ATTR_NUM::edge_timing_ms__max                                 +0.0005\n",
            "ATTR_NUM::node_power_w_mean__max                              +0.0005\n",
            "ATTR_NUM::node_power_w_mean__min                              +0.0005\n",
            "ATTR_NUM::node_gpu_util_mean__mean                            +0.0005\n",
            "ATTR_NUM::node_gpu_util_mean__min                             +0.0005\n",
            "ATTR_NUM::node_gpu_util_mean__max                             +0.0005\n",
            "ATTR_NUM::edge_timing_ms__std                                 +0.0005\n",
            "ATTR_NUM::node_cs_mean__min                                   +0.0005\n",
            "ATTR_NUM::node_cs_mean__mean                                  +0.0005\n",
            "ATTR_NUM::edge_vis_length__std                                +0.0005\n",
            "ATTR_NUM::edge_vis_width__std                                 +0.0005\n",
            "ATTR_NUM::node_cs_mean__max                                   +0.0005\n",
            "ATTR_NUM::node_gpu_util_max__mean                             +0.0005\n",
            "ATTR_NUM::node_gpu_util_max__min                              +0.0004\n",
            "ATTR_NUM::node_gpu_util_max__max                              +0.0004\n",
            "ATTR_NUM::edge_timing_ms__mean                                +0.0004\n",
            "ATTR_NUM::node_duration_ms__min                               +0.0004\n",
            "ATTR_NUM::node_duration_ms__mean                              +0.0004\n",
            "STRUCT::rt_time_mean                                          +0.0004\n",
            "STRUCT::rt_time_p95                                           +0.0004\n",
            "STRUCT::rt_time_median                                        +0.0004\n",
            "STRUCT::lag_median                                            +0.0004\n",
            "ATTR_NUM::node_us_mean__mean                                  +0.0004\n",
            "ATTR_NUM::node_duration_ms__max                               +0.0004\n",
            "\n",
            "Top negative features:\n",
            "ATTR_NUM::edge_gpu_util_mean_win__std                         -0.0002\n",
            "ATTR_NUM::edge_gpu_power_mean_win__std                        -0.0002\n",
            "ATTR_NUM::node_power_w_mean__std                              -0.0001\n",
            "ATTR_NUM::node_gpu_util_max__std                              -0.0001\n",
            "EDGE_CAT::E:src_role=neonatologist                            -0.0001\n",
            "ATTR_NUM::node_gpu_util_mean__std                             -0.0001\n",
            "EDGE_CAT::E:src_role=pediatric cardiologist                   -0.0001\n",
            "STRUCT::avg_clustering_y                                      -0.0001\n",
            "STRUCT::avg_clustering_x                                      -0.0001\n",
            "STRUCT::reciprocity_y                                         -0.0001\n",
            "STRUCT::reciprocity_x                                         -0.0001\n",
            "STRUCT::transitivity_x                                        -0.0001\n",
            "STRUCT::transitivity_y                                        -0.0001\n",
            "STRUCT::roleprop::pediatric cardiologist_y                    -0.0001\n",
            "STRUCT::roleprop::cardiothoracic surgeon_y                    -0.0001\n",
            "STRUCT::roleprop::cardiothoracic surgeon_x                    -0.0001\n",
            "STRUCT::roleprop::pediatric cardiologist_x                    -0.0001\n",
            "STRUCT::roleprop::neonatologist_x                             -0.0001\n",
            "STRUCT::roleprop::neonatologist_y                             -0.0001\n",
            "STRUCT::largest_wcc_edges_x                                   -0.0001\n",
            "STRUCT::largest_wcc_edges_y                                   -0.0001\n",
            "STRUCT::role_entropy_x                                        -0.0001\n",
            "STRUCT::role_entropy_y                                        -0.0001\n",
            "ATTR_NUM::edge_turn__mean                                     -0.0001\n",
            "STRUCT::spec_edge_fraction                                    -0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Structure-only\n",
        "\n",
        "# Attributes-only (your node/edge numeric + cats + text)\n",
        "\n",
        "# Combined (structure + attributes)"
      ],
      "metadata": {
        "id": "lJTksaCjv9kP"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "def eval_model_on_matrix(X, y, clf, name):\n",
        "    per_fold, cms_fixed, cms_tuned = [], [], []\n",
        "\n",
        "    # lbfgs LogisticRegression and HGB need dense arrays\n",
        "    need_dense = (\n",
        "        (isinstance(clf, LogisticRegression) and getattr(clf, \"solver\", \"\") == \"lbfgs\")\n",
        "        or isinstance(clf, HistGradientBoostingClassifier)\n",
        "    )\n",
        "\n",
        "    # Build a *named* Pipeline\n",
        "    steps = [\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"sc\",  StandardScaler(with_mean=False)),  # safe for sparse; keeps behavior consistent\n",
        "    ]\n",
        "    if need_dense:\n",
        "        # Convert CSR -> dense just before the estimator\n",
        "        steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "    steps.append((\"clf\", clf))\n",
        "\n",
        "    pipe = Pipeline(steps=steps)\n",
        "\n",
        "    for fold, (tr, te) in enumerate(skf.split(X, y), 1):\n",
        "        Xtr, Xte = (X[tr], X[te]) if issparse(X) else (X.iloc[tr], X.iloc[te])\n",
        "        ytr, yte = y[tr], y[te]\n",
        "\n",
        "        pipe.fit(Xtr, ytr)\n",
        "\n",
        "        # Probabilities/scores for metrics\n",
        "        if hasattr(pipe[-1], \"predict_proba\"):\n",
        "            p = pipe.predict_proba(Xte)[:, 1]\n",
        "        else:\n",
        "            try:\n",
        "                s = pipe.decision_function(Xte)\n",
        "                p = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
        "            except Exception:\n",
        "                p = pipe.predict_proba(Xte)[:, 1]\n",
        "\n",
        "        # Fixed 0.50 threshold\n",
        "        y_pred = (p >= 0.50).astype(int)\n",
        "        cm = confusion_matrix(yte, y_pred, labels=[0,1])\n",
        "        cms_fixed.append(cm)\n",
        "\n",
        "        # F1-tuned threshold (matches your prior print style)\n",
        "        thr_grid = np.linspace(0.30, 0.70, 41)\n",
        "        f1s = [f1_score(yte, (p >= t).astype(int), zero_division=0) for t in thr_grid]\n",
        "        t_best = float(thr_grid[int(np.argmax(f1s))])\n",
        "        y_pred_tuned = (p >= t_best).astype(int)\n",
        "        cm_tuned = confusion_matrix(yte, y_pred_tuned, labels=[0,1])\n",
        "        cms_tuned.append(cm_tuned)\n",
        "\n",
        "        per_fold.append(dict(\n",
        "            fold=fold,\n",
        "            AUC=roc_auc_score(yte, p),\n",
        "            AP=average_precision_score(yte, p),\n",
        "            Acc_fixed=accuracy_score(yte, y_pred),\n",
        "            Prec_fixed=precision_score(yte, y_pred, zero_division=0),\n",
        "            Rec_fixed=recall_score(yte, y_pred, zero_division=0),\n",
        "            F1_fixed=f1_score(yte, y_pred, zero_division=0),\n",
        "            BalAcc_fixed=balanced_accuracy_score(yte, y_pred),\n",
        "            thr_tuned=t_best,\n",
        "            Acc_tuned=accuracy_score(yte, y_pred_tuned),\n",
        "            Prec_tuned=precision_score(yte, y_pred_tuned, zero_division=0),\n",
        "            Rec_tuned=recall_score(yte, y_pred_tuned, zero_division=0),\n",
        "            F1_tuned=f1_score(yte, y_pred_tuned, zero_division=0),\n",
        "            BalAcc_tuned=balanced_accuracy_score(yte, y_pred_tuned),\n",
        "        ))\n",
        "\n",
        "    df = pd.DataFrame(per_fold).round(3)\n",
        "\n",
        "    cols_ms = [\"AUC\",\"AP\",\"Acc_fixed\",\"Prec_fixed\",\"Rec_fixed\",\"F1_fixed\",\"BalAcc_fixed\",\n",
        "               \"Acc_tuned\",\"Prec_tuned\",\"Rec_tuned\",\"F1_tuned\",\"BalAcc_tuned\"]\n",
        "    mean = df[cols_ms].mean().rename(lambda s: f\"{s}_mean\")\n",
        "    std  = df[cols_ms].std().rename(lambda s: f\"{s}_std\")\n",
        "    summary = pd.concat([mean, std])\n",
        "\n",
        "    cm_fixed_mean = np.mean(np.stack(cms_fixed, axis=0), axis=0)\n",
        "    cm_tuned_mean = np.mean(np.stack(cms_tuned, axis=0), axis=0)\n",
        "\n",
        "    print(f\"\\n=== {name}: Per-fold metrics (K={K}) ===\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    print(\"\\n=== Mean ± Std across folds ===\")\n",
        "    keep = [\"AUC_mean\",\"AUC_std\",\"AP_mean\",\"AP_std\",\n",
        "            \"Acc_fixed_mean\",\"Acc_fixed_std\",\n",
        "            \"Prec_fixed_mean\",\"Prec_fixed_std\",\n",
        "            \"Rec_fixed_mean\",\"Rec_fixed_std\",\n",
        "            \"F1_fixed_mean\",\"F1_fixed_std\",\n",
        "            \"BalAcc_fixed_mean\",\n",
        "            \"Acc_tuned_mean\",\"Acc_tuned_std\",\n",
        "            \"Prec_tuned_mean\",\"Prec_tuned_std\",\n",
        "            \"Rec_tuned_mean\",\"Rec_tuned_std\",\n",
        "            \"F1_tuned_mean\",\"F1_tuned_std\",\n",
        "            \"BalAcc_tuned_mean\"]\n",
        "    print(summary[keep].round(3).to_frame().T.to_string(index=False))\n",
        "\n",
        "    def _print_cm(cm, tag):\n",
        "        tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "        acc = (tp+tn) / (tn+fp+fn+tp + 1e-9)\n",
        "        prec = tp / (tp+fp + 1e-9)\n",
        "        rec  = tp / (tp+fn + 1e-9)\n",
        "        spec = tn / (tn+fp + 1e-9)\n",
        "        print(f\"\\n=== Mean Confusion Matrix ({tag}) ===\")\n",
        "        print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "        print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "        print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall (attack): {rec:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "    _print_cm(cm_fixed_mean, \"threshold = 0.50\")\n",
        "    _print_cm(cm_tuned_mean, \"F1-tuned threshold\")\n",
        "\n",
        "    return df, summary, cm_fixed_mean, cm_tuned_mean\n",
        "\n"
      ],
      "metadata": {
        "id": "dirpviaKv9nW"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy import sparse\n",
        "\n",
        "def block_stats(X, name):\n",
        "    rows = X.shape[0]\n",
        "    nnz_per_row = np.diff(X.indptr) if sparse.isspmatrix_csr(X) else (X!=0).sum(axis=1).A1\n",
        "    print(f\"{name}: nnz/row mean={nnz_per_row.mean():.1f}, median={np.median(nnz_per_row):.0f}, \"\n",
        "          f\"min={nnz_per_row.min()}, max={nnz_per_row.max()}\")\n",
        "\n",
        "block_stats(X_struct, \"STRUCT\")\n",
        "block_stats(X_num, \"NUM\")\n",
        "block_stats(X_node_cat, \"NODE_CAT\")\n",
        "block_stats(X_edge_cat, \"EDGE_CAT\")\n",
        "block_stats(X_text, \"TEXT\")\n"
      ],
      "metadata": {
        "id": "ZOm8Ge6nv9qQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "3f0004be-1bf4-44a0-9f4f-8fbc2e8ae498"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STRUCT: nnz/row mean=158.8, median=160, min=87, max=175\n",
            "NUM: nnz/row mean=189.0, median=190, min=164, max=202\n",
            "NODE_CAT: nnz/row mean=5.7, median=5, min=4, max=202\n",
            "EDGE_CAT: nnz/row mean=11.8, median=11, min=11, max=203\n",
            "TEXT: nnz/row mean=160.6, median=156, min=83, max=1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Attribute only testing"
      ],
      "metadata": {
        "id": "6REl3az61cIY"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT_KEYS = [\n",
        "    \"message_text\",\n",
        "    \"message\",\"text\",\"content\",\"prompt\",\"response\",\n",
        "    \"assistant_text\",\"user_msg\",\"msg\",\"utterance\"\n",
        "]\n",
        "TEXT_KEYS_LC = set([k.lower() for k in TEXT_KEYS])\n",
        "\n",
        "import re\n",
        "_trace_re = re.compile(r\"<\\s*trace:[0-9a-fA-F]{16,64}\\s*>\")\n",
        "\n",
        "def _clean_text(s: str) -> str:\n",
        "    s = _trace_re.sub(\"\", s)          # remove <trace:...>\n",
        "    return s.strip()\n",
        "\n",
        "def gather_graph_attr_features(G: nx.Graph):\n",
        "    node_num_values = collections.defaultdict(list)\n",
        "    node_cat_counts = collections.Counter()\n",
        "    edge_num_values = collections.defaultdict(list)\n",
        "    edge_cat_counts = collections.Counter()\n",
        "    text_bits = []\n",
        "\n",
        "    BAD_KEYS = {\"bundle\",\"cohort\",\"file\",\"graph_id\",\"case_id\",\"trace_id\",\"root\",\"root_id\",\"start_u\"}\n",
        "    def _bad_key(k: str) -> bool:\n",
        "        if not isinstance(k, str): return True\n",
        "        kl = k.lower()\n",
        "        if kl in BAD_KEYS: return True\n",
        "        if kl.endswith(\"id\") or kl.endswith(\"_id\"): return True\n",
        "        return False\n",
        "\n",
        "    # nodes\n",
        "    for _, attrs in G.nodes(data=True):\n",
        "        if not attrs: continue\n",
        "        for k, v in attrs.items():\n",
        "            if v is None or _bad_key(k): continue\n",
        "            if is_number(v):\n",
        "                node_num_values[f\"node_{k}\"].append(to_float(v))\n",
        "            elif is_stringy(v) or isinstance(v, bool) or isinstance(v, (list, tuple, set, dict)):\n",
        "                node_cat_counts[f\"N:{k}={_catify(v)}\"] += 1\n",
        "\n",
        "    # edges\n",
        "    for _, _, attrs in G.edges(data=True):\n",
        "        if not attrs: continue\n",
        "        for k, v in attrs.items():\n",
        "            if v is None or _bad_key(k): continue\n",
        "            kl = k.lower() if isinstance(k, str) else \"\"\n",
        "            # TEXT -> only TF-IDF, never categorical\n",
        "            if kl in TEXT_KEYS_LC and is_stringy(v):\n",
        "                text_bits.append(_clean_text(str(v)))\n",
        "                continue\n",
        "            # numeric\n",
        "            if is_number(v):\n",
        "                edge_num_values[f\"edge_{k}\"].append(to_float(v))\n",
        "                continue\n",
        "            # categorical guard: avoid gigantic values becoming “IDs”\n",
        "            if is_stringy(v) or isinstance(v, bool) or isinstance(v, (list, tuple, set, dict)):\n",
        "                sv = str(v)\n",
        "                if len(sv) > 80:   # truncate or skip overly long cats\n",
        "                    continue       # (safer: skip; else sv = sv[:80])\n",
        "                if _trace_re.search(sv):\n",
        "                    continue       # drop any residual trace-like tokens\n",
        "                edge_cat_counts[f\"E:{k}={_catify(sv)}\"] += 1\n",
        "\n",
        "    # reduce numeric\n",
        "    num_feats = {}\n",
        "    for key, vals in node_num_values.items():\n",
        "        arr = np.asarray(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    for key, vals in edge_num_values.items():\n",
        "        arr = np.asarray(vals, dtype=float)\n",
        "        num_feats[f\"{key}__mean\"]  = float(np.mean(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__std\"]   = float(np.std(arr, ddof=1)) if arr.size > 1 else (0.0 if arr.size==1 else np.nan)\n",
        "        num_feats[f\"{key}__min\"]   = float(np.min(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__max\"]   = float(np.max(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__sum\"]   = float(np.sum(arr)) if arr.size else np.nan\n",
        "        num_feats[f\"{key}__count\"] = float(arr.size)\n",
        "\n",
        "    edge_text_doc = \" \".join(t for t in text_bits if t)\n",
        "    return num_feats, node_cat_counts, edge_cat_counts, edge_text_doc\n"
      ],
      "metadata": {
        "id": "4IidZg7w2xHN"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Debugging\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "def one_sided_and_perfect_cols(X, y, min_nz=3):\n",
        "    # Treat nonzero as \"present\"\n",
        "    if issparse(X):\n",
        "        nnz_per_col = np.asarray((X != 0).sum(axis=0)).ravel()\n",
        "        nnz_pos = np.asarray((X[y==1] != 0).sum(axis=0)).ravel()\n",
        "        nnz_neg = nnz_per_col - nnz_pos\n",
        "    else:\n",
        "        nnz_per_col = (X != 0).sum(axis=0)\n",
        "        nnz_pos = (X[y==1] != 0).sum(axis=0)\n",
        "        nnz_neg = nnz_per_col - nnz_pos\n",
        "\n",
        "    # columns present only in one class\n",
        "    only_pos = (nnz_neg == 0) & (nnz_pos >= min_nz)\n",
        "    only_neg = (nnz_pos == 0) & (nnz_neg >= min_nz)\n",
        "    return np.where(only_pos | only_neg)[0], only_pos, only_neg\n",
        "\n",
        "cols_OS, only_pos_mask, only_neg_mask = one_sided_and_perfect_cols(X_num_scaled[row_indices], y)\n",
        "print(\"One-sided columns (present only in one class):\", len(cols_OS))\n"
      ],
      "metadata": {
        "id": "x35VjU6S-jX6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "15de59a0-029b-4f3d-da89-206d4daf3801"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-sided columns (present only in one class): 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.sparse import issparse\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def univariate_auc(X, y, top_k=30):\n",
        "    A = []\n",
        "    for j in range(X.shape[1]):\n",
        "        xj = X.getcol(j).toarray().ravel() if issparse(X) else np.asarray(X[:, j]).ravel()\n",
        "        if np.all(~np.isfinite(xj)):        # all NaN/inf\n",
        "            continue\n",
        "        xj = np.nan_to_num(xj, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "        if np.all(xj == xj[0]):             # constant after cleaning\n",
        "            continue\n",
        "        try:\n",
        "            auc = roc_auc_score(y, xj)\n",
        "        except ValueError:\n",
        "            continue\n",
        "        A.append((j, auc))\n",
        "    # rank by distance from 0.5\n",
        "    A.sort(key=lambda t: abs(t[1] - 0.5), reverse=True)\n",
        "    return A[:top_k], A\n"
      ],
      "metadata": {
        "id": "gtss3Cff-k4B"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top30, all_aucs = univariate_auc(X_num_scaled[row_indices], y, top_k=50)\n",
        "print(\"Top 10 univariate AUCs:\")\n",
        "for j, auc in top30[:10]:\n",
        "    print(j, round(auc, 6))\n"
      ],
      "metadata": {
        "id": "HdHmzVV9-rQE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "82ba74ba-b161-4db0-d8e2-9d5d0da44cd5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 univariate AUCs:\n",
            "24 1.0\n",
            "26 1.0\n",
            "27 1.0\n",
            "30 1.0\n",
            "32 1.0\n",
            "33 1.0\n",
            "258 1.0\n",
            "260 1.0\n",
            "261 1.0\n",
            "264 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "accs = []\n",
        "for tr, te in skf.split(X_num_scaled[row_indices], y):\n",
        "    # use only NUM-ONLY to keep it fast\n",
        "    Xtr = X_num_scaled[row_indices][tr]\n",
        "    Xte = X_num_scaled[row_indices][te]\n",
        "    ytr, yte = y[tr], y[te]\n",
        "    knn = KNeighborsClassifier(n_neighbors=1, metric='cosine')\n",
        "    knn.fit(Xtr, ytr)\n",
        "    accs.append(accuracy_score(yte, knn.predict(Xte)))\n",
        "print(\"1-NN CV accuracy (cosine):\", np.mean(accs), np.std(accs))\n"
      ],
      "metadata": {
        "id": "yuKQVxf6_Nu2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "88c1acf3-437f-4514-9767-4b834f6b2b55"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-NN CV accuracy (cosine): 0.8311688311688311 0.04267967980559732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# (A) Name-based blocklist for numeric features\n",
        "name_leak_re = re.compile(r\"(label|attack|normal|target|class|ground.?truth|is_.*attack)\",\n",
        "                          flags=re.IGNORECASE)\n",
        "\n",
        "bad_by_name = {j for j, col in enumerate(num_cols) if name_leak_re.search(col)}\n",
        "\n",
        "# (B) AUC-based filter (you already computed all_aucs)\n",
        "# keep the variable `all_aucs` from your univariate function\n",
        "bad_by_auc = {j for j, auc in all_aucs if (auc >= 0.99 or auc <= 0.01)}\n",
        "\n",
        "bad_idx = sorted(bad_by_name | bad_by_auc)\n",
        "print(\"Num features to drop:\", len(bad_idx))\n",
        "print([num_cols[j] for j in bad_idx][:20], \"...\")\n"
      ],
      "metadata": {
        "id": "1PCbjVO5_pdX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "af9864e6-e308-4e4a-fea2-3ae65f24568b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num features to drop: 12\n",
            "['node_first_ts__mean', 'node_first_ts__min', 'node_first_ts__max', 'node_last_ts__mean', 'node_last_ts__min', 'node_last_ts__max', 'edge_start_ts__mean', 'edge_start_ts__min', 'edge_start_ts__max', 'edge_end_ts__mean', 'edge_end_ts__min', 'edge_end_ts__max'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "keep_mask = np.ones(X_num_scaled.shape[1], dtype=bool)\n",
        "keep_mask[bad_idx] = False\n",
        "\n",
        "X_num_clean = X_num_scaled[:, keep_mask]\n",
        "\n",
        "# If you cache names elsewhere, also keep the filtered names if helpful:\n",
        "num_cols_clean = [c for k, c in enumerate(num_cols) if keep_mask[k]]\n",
        "\n",
        "# Now rebuild your ATTRS-ONLY/COMBINED blocks using X_num_clean instead of X_num_scaled.\n",
        "# (Leave your per-fold TF-IDF rebuild exactly as-is; that part is good.)\n"
      ],
      "metadata": {
        "id": "ikNTFJJEAWys"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "keep_mask = np.ones(X_num_scaled.shape[1], dtype=bool)\n",
        "keep_mask[bad_idx] = False\n",
        "\n",
        "X_num_clean = X_num_scaled[:, keep_mask]\n",
        "\n",
        "# If you cache names elsewhere, also keep the filtered names if helpful:\n",
        "num_cols_clean = [c for k, c in enumerate(num_cols) if keep_mask[k]]\n",
        "\n",
        "# Now rebuild your ATTRS-ONLY/COMBINED blocks using X_num_clean instead of X_num_scaled.\n",
        "# (Leave your per-fold TF-IDF rebuild exactly as-is; that part is good.)\n"
      ],
      "metadata": {
        "id": "i-wLJ_zB_xin"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_block_suite(X_block, y, tag):\n",
        "    print(f\"\\n=== {tag} ({X_block.shape[1]} feats) ===\")\n",
        "    _ = eval_model_on_matrix(\n",
        "        X_block, y,\n",
        "        LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\", random_state=42),\n",
        "        f\"{tag} | LogReg\"\n",
        "    )\n",
        "    _ = eval_model_on_matrix(\n",
        "        X_block, y,\n",
        "        RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\"),\n",
        "        f\"{tag} | RF\"\n",
        "    )\n",
        "    _ = eval_model_on_matrix(\n",
        "        X_block, y,\n",
        "        HistGradientBoostingClassifier(random_state=42),\n",
        "        f\"{tag} | HGB\"\n",
        "    )\n",
        "\n",
        "# Important: rebuild TF-IDF inside CV folds like you already did when testing TEXT ONLY.\n",
        "X_num_only   = X_num_clean[row_indices]\n",
        "X_node_only  = X_node_cat[row_indices]\n",
        "X_edge_only  = X_edge_cat[row_indices]\n",
        "\n",
        "run_block_suite(X_num_only,  y, \"NUM-ONLY (clean)\")\n",
        "run_block_suite(X_node_only, y, \"NODECAT-ONLY\")\n",
        "run_block_suite(X_edge_only, y, \"EDGECAT-ONLY\")\n",
        "# For TEXT-ONLY, reuse your per-fold TF-IDF rebuild path.\n"
      ],
      "metadata": {
        "id": "OrGEdJ9d7ODw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "db8716fc-6019-4858-c4b7-aa53678c6c10"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== NUM-ONLY (clean) (270 feats) ===\n",
            "\n",
            "=== NUM-ONLY (clean) | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.956 0.970      0.909       0.971      0.850     0.907         0.911       0.59      0.922       1.000      0.850     0.919         0.925\n",
            "    2 0.945 0.964      0.870       0.895      0.850     0.872         0.871       0.66      0.909       0.971      0.850     0.907         0.911\n",
            "    3 0.865 0.844      0.779       0.848      0.700     0.767         0.782       0.31      0.805       0.857      0.750     0.800         0.807\n",
            "    4 0.886 0.924      0.883       0.943      0.825     0.880         0.885       0.40      0.883       0.943      0.825     0.880         0.885\n",
            "    5 0.901 0.918      0.831       0.814      0.875     0.843         0.829       0.61      0.844       0.833      0.875     0.854         0.843\n",
            "    6 0.959 0.969      0.857       0.914      0.800     0.853         0.859       0.32      0.909       0.902      0.925     0.914         0.908\n",
            "    7 0.972 0.978      0.909       0.946      0.875     0.909         0.910       0.31      0.922       0.925      0.925     0.925         0.922\n",
            "    8 0.910 0.927      0.844       0.886      0.795     0.838         0.845       0.53      0.857       0.912      0.795     0.849         0.858\n",
            "    9 0.941 0.956      0.870       0.892      0.846     0.868         0.870       0.41      0.909       0.900      0.923     0.911         0.909\n",
            "   10 0.914 0.941      0.844       0.865      0.821     0.842         0.844       0.65      0.883       0.941      0.821     0.877         0.884\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.925    0.035    0.939    0.04            0.86          0.039            0.897           0.048           0.824          0.051          0.858         0.041              0.861           0.884          0.038            0.918            0.05           0.854          0.059          0.884          0.04              0.885\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.5          3.8\n",
            "True Attack          7.0         32.7\n",
            "Accuracy: 0.860 | Precision (attack): 0.896 | Recall (attack): 0.824 | Specificity (normal): 0.898\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.2          3.1\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.884 | Precision (attack): 0.916 | Recall (attack): 0.854 | Specificity (normal): 0.917\n",
            "\n",
            "=== NUM-ONLY (clean) | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    2 1.000 1.000      0.987         1.0      0.975     0.987         0.988       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    3 0.999 0.999      0.987         1.0      0.975     0.987         0.988       0.30      0.987         1.0      0.975     0.987         0.988\n",
            "    4 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.32      1.000         1.0      1.000     1.000         1.000\n",
            "    5 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.44      1.000         1.0      1.000     1.000         1.000\n",
            "    6 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.48      1.000         1.0      1.000     1.000         1.000\n",
            "    7 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    8 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    9 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.38      1.000         1.0      1.000     1.000         1.000\n",
            "   10 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.38      1.000         1.0      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.997          0.005              1.0             0.0           0.995          0.011          0.997         0.005              0.998           0.999          0.004              1.0             0.0           0.997          0.008          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision (attack): 1.000 | Recall (attack): 0.995 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.999 | Precision (attack): 1.000 | Recall (attack): 0.997 | Specificity (normal): 1.000\n",
            "\n",
            "=== NUM-ONLY (clean) | HGB: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.66      1.000       1.000      1.000     1.000         1.000\n",
            "    2  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    3  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    4  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.30      0.987       0.976      1.000     0.988         0.986\n",
            "    5  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    6  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    7  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    8  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.46      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.992          0.007            0.995            0.01            0.99          0.013          0.992         0.007              0.992           0.994          0.007            0.998           0.008            0.99          0.013          0.994         0.007              0.994\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.992 | Precision (attack): 0.995 | Recall (attack): 0.990 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.994 | Precision (attack): 0.997 | Recall (attack): 0.990 | Specificity (normal): 0.997\n",
            "\n",
            "=== NODECAT-ONLY (250 feats) ===\n",
            "\n",
            "=== NODECAT-ONLY | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.216 0.526      0.532       0.526      1.000     0.690         0.514       0.30      0.532       0.526      1.000     0.690         0.514\n",
            "    2 0.153 0.525      0.506       0.513      0.975     0.672         0.488       0.30      0.506       0.513      0.975     0.672         0.488\n",
            "    3 0.264 0.527      0.532       0.527      0.975     0.684         0.515       0.50      0.532       0.527      0.975     0.684         0.515\n",
            "    4 0.475 0.527      0.532       0.527      0.975     0.684         0.515       0.30      0.532       0.527      0.975     0.684         0.515\n",
            "    5 0.257 0.541      0.558       0.541      1.000     0.702         0.541       0.31      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.224 0.534      0.545       0.534      0.975     0.690         0.528       0.30      0.545       0.534      0.975     0.690         0.528\n",
            "    7 0.270 0.519      0.519       0.519      1.000     0.684         0.500       0.30      0.519       0.519      1.000     0.684         0.500\n",
            "    8 0.256 0.507      0.506       0.507      0.974     0.667         0.500       0.30      0.506       0.507      0.974     0.667         0.500\n",
            "    9 0.224 0.520      0.532       0.520      1.000     0.684         0.526       0.30      0.532       0.520      1.000     0.684         0.526\n",
            "   10 0.205 0.520      0.532       0.521      0.974     0.679         0.527       0.30      0.532       0.521      0.974     0.679         0.527\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.254    0.085    0.525   0.009           0.529          0.016            0.524            0.01           0.985          0.013          0.684          0.01              0.515           0.529          0.016            0.524            0.01           0.985          0.013          0.684          0.01              0.515\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.7         35.6\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.530 | Precision (attack): 0.523 | Recall (attack): 0.985 | Specificity (normal): 0.046\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.7         35.6\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.530 | Precision (attack): 0.523 | Recall (attack): 0.985 | Specificity (normal): 0.046\n",
            "\n",
            "=== NODECAT-ONLY | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.514 0.526      0.532       0.526      1.000     0.690         0.514       0.35      0.532       0.526      1.000     0.690         0.514\n",
            "    2 0.501 0.520      0.506       0.513      0.975     0.672         0.488       0.30      0.506       0.513      0.975     0.672         0.488\n",
            "    3 0.528 0.534      0.532       0.527      0.975     0.684         0.515       0.38      0.532       0.527      0.975     0.684         0.515\n",
            "    4 0.515 0.527      0.532       0.527      0.975     0.684         0.515       0.30      0.532       0.527      0.975     0.684         0.515\n",
            "    5 0.554 0.548      0.558       0.541      1.000     0.702         0.541       0.37      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.542 0.542      0.545       0.534      0.975     0.690         0.528       0.32      0.558       0.541      1.000     0.702         0.541\n",
            "    7 0.500 0.519      0.519       0.519      1.000     0.684         0.500       0.30      0.519       0.519      1.000     0.684         0.500\n",
            "    8 0.513 0.513      0.506       0.507      0.974     0.667         0.500       0.30      0.506       0.506      1.000     0.672         0.500\n",
            "    9 0.526 0.520      0.532       0.520      1.000     0.684         0.526       0.32      0.532       0.520      1.000     0.684         0.526\n",
            "   10 0.528 0.521      0.532       0.521      0.974     0.679         0.527       0.35      0.545       0.527      1.000     0.690         0.539\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.522    0.017    0.527   0.011           0.529          0.016            0.524            0.01           0.985          0.013          0.684          0.01              0.515           0.532          0.018            0.525           0.011           0.992          0.012          0.686          0.01              0.518\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.7         35.6\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.530 | Precision (attack): 0.523 | Recall (attack): 0.985 | Specificity (normal): 0.046\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.6         35.7\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.532 | Precision (attack): 0.525 | Recall (attack): 0.992 | Specificity (normal): 0.043\n",
            "\n",
            "=== NODECAT-ONLY | HGB: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.514 0.526      0.532       0.526      1.000     0.690         0.514       0.32      0.532       0.526      1.000     0.690         0.514\n",
            "    2 0.475 0.507      0.494       0.507      0.950     0.661         0.475       0.30      0.494       0.507      0.950     0.661         0.475\n",
            "    3 0.515 0.527      0.532       0.527      0.975     0.684         0.515       0.30      0.532       0.527      0.975     0.684         0.515\n",
            "    4 0.515 0.527      0.532       0.527      0.975     0.684         0.515       0.30      0.532       0.527      0.975     0.684         0.515\n",
            "    5 0.541 0.541      0.558       0.541      1.000     0.702         0.541       0.35      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.528 0.534      0.545       0.534      0.975     0.690         0.528       0.31      0.545       0.534      0.975     0.690         0.528\n",
            "    7 0.500 0.519      0.519       0.519      1.000     0.684         0.500       0.30      0.519       0.519      1.000     0.684         0.500\n",
            "    8 0.500 0.507      0.506       0.507      0.974     0.667         0.500       0.30      0.506       0.507      0.974     0.667         0.500\n",
            "    9 0.526 0.520      0.532       0.520      1.000     0.684         0.526       0.33      0.532       0.520      1.000     0.684         0.526\n",
            "   10 0.527 0.520      0.532       0.521      0.974     0.679         0.527       0.31      0.532       0.521      0.974     0.679         0.527\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.514    0.019    0.523   0.011           0.528          0.018            0.523           0.011           0.982          0.017          0.682         0.012              0.514           0.528          0.018            0.523           0.011           0.982          0.017          0.682         0.012              0.514\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.7         35.6\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.529 | Precision (attack): 0.523 | Recall (attack): 0.982 | Specificity (normal): 0.046\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.7         35.6\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.529 | Precision (attack): 0.523 | Recall (attack): 0.982 | Specificity (normal): 0.046\n",
            "\n",
            "=== EDGECAT-ONLY (250 feats) ===\n",
            "\n",
            "=== EDGECAT-ONLY | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.811 0.741      0.532       0.526      1.000     0.690         0.514       0.68      0.818       0.741      1.000     0.851         0.811\n",
            "    2 0.179 0.532      0.519       0.520      0.975     0.678         0.501       0.47      0.519       0.520      0.975     0.678         0.501\n",
            "    3 0.751 0.692      0.532       0.527      0.975     0.684         0.515       0.70      0.766       0.696      0.975     0.812         0.758\n",
            "    4 0.568 0.557      0.532       0.527      0.975     0.684         0.515       0.70      0.584       0.559      0.950     0.704         0.570\n",
            "    5 0.299 0.543      0.532       0.528      0.950     0.679         0.516       0.30      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.830 0.773      0.558       0.542      0.975     0.696         0.542       0.68      0.844       0.780      0.975     0.867         0.839\n",
            "    7 0.698 0.647      0.519       0.519      1.000     0.684         0.500       0.63      0.714       0.650      0.975     0.780         0.704\n",
            "    8 0.738 0.676      0.506       0.507      0.974     0.667         0.500       0.64      0.740       0.673      0.949     0.787         0.738\n",
            "    9 0.739 0.677      0.506       0.507      0.949     0.661         0.501       0.70      0.753       0.685      0.949     0.796         0.751\n",
            "   10 0.848 0.785      0.532       0.521      0.974     0.679         0.527       0.70      0.857       0.792      0.974     0.874         0.856\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.646     0.23    0.662   0.093           0.527          0.015            0.522            0.01           0.975          0.017           0.68          0.01              0.513           0.715          0.121            0.664           0.097           0.972          0.019          0.785         0.071              0.707\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.9         35.4\n",
            "True Attack          1.0         38.7\n",
            "Accuracy: 0.527 | Precision (attack): 0.522 | Recall (attack): 0.975 | Specificity (normal): 0.051\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         16.5         20.8\n",
            "True Attack          1.1         38.6\n",
            "Accuracy: 0.716 | Precision (attack): 0.650 | Recall (attack): 0.972 | Specificity (normal): 0.442\n",
            "\n",
            "=== EDGECAT-ONLY | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.486 0.526      0.532       0.526      1.000     0.690         0.514       0.33      0.532       0.526      1.000     0.690         0.514\n",
            "    2 0.489 0.514      0.506       0.514      0.950     0.667         0.489       0.30      0.519       0.520      0.975     0.678         0.501\n",
            "    3 0.541 0.541      0.532       0.527      0.975     0.684         0.515       0.69      0.545       0.534      0.975     0.690         0.528\n",
            "    4 0.543 0.542      0.532       0.527      0.975     0.684         0.515       0.65      0.545       0.535      0.950     0.685         0.529\n",
            "    5 0.545 0.543      0.558       0.541      1.000     0.702         0.541       0.32      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.543 0.542      0.558       0.542      0.975     0.696         0.542       0.35      0.571       0.548      1.000     0.708         0.554\n",
            "    7 0.501 0.520      0.519       0.519      1.000     0.684         0.500       0.30      0.519       0.519      1.000     0.684         0.500\n",
            "    8 0.500 0.509      0.506       0.507      0.974     0.667         0.500       0.70      0.519       0.514      0.974     0.673         0.513\n",
            "    9 0.502 0.508      0.532       0.520      1.000     0.684         0.526       0.33      0.532       0.520      1.000     0.684         0.526\n",
            "   10 0.553 0.535      0.532       0.521      0.974     0.679         0.527       0.68      0.545       0.528      0.974     0.685         0.540\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "     0.52    0.027    0.528   0.014           0.531          0.018            0.524           0.011           0.982          0.017          0.684         0.011              0.517           0.538          0.018            0.528           0.011           0.985          0.018          0.688          0.01              0.525\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.9         35.4\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.531 | Precision (attack): 0.524 | Recall (attack): 0.982 | Specificity (normal): 0.051\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.4         34.9\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.539 | Precision (attack): 0.528 | Recall (attack): 0.985 | Specificity (normal): 0.064\n",
            "\n",
            "=== EDGECAT-ONLY | HGB: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.541 0.541      0.558       0.541      1.000     0.702         0.541       0.40      0.558       0.541      1.000     0.702         0.541\n",
            "    2 0.488 0.514      0.506       0.514      0.950     0.667         0.489       0.37      0.506       0.514      0.950     0.667         0.489\n",
            "    3 0.528 0.534      0.545       0.534      0.975     0.690         0.528       0.40      0.545       0.534      0.975     0.690         0.528\n",
            "    4 0.515 0.527      0.532       0.527      0.975     0.684         0.515       0.30      0.532       0.527      0.975     0.684         0.515\n",
            "    5 0.541 0.541      0.558       0.541      1.000     0.702         0.541       0.35      0.558       0.541      1.000     0.702         0.541\n",
            "    6 0.541 0.541      0.558       0.542      0.975     0.696         0.542       0.40      0.558       0.542      0.975     0.696         0.542\n",
            "    7 0.488 0.513      0.506       0.513      0.975     0.672         0.488       0.30      0.519       0.519      1.000     0.684         0.500\n",
            "    8 0.513 0.514      0.519       0.514      0.949     0.667         0.514       0.30      0.506       0.507      0.974     0.667         0.500\n",
            "    9 0.499 0.507      0.506       0.507      0.949     0.661         0.501       0.36      0.506       0.507      0.949     0.661         0.501\n",
            "   10 0.539 0.527      0.545       0.528      0.974     0.685         0.540       0.40      0.545       0.528      0.974     0.685         0.540\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.519    0.022    0.526   0.013           0.533          0.022            0.526           0.013           0.972          0.019          0.683         0.015               0.52           0.533          0.022            0.526           0.014           0.977          0.019          0.684         0.015               0.52\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.5         34.8\n",
            "True Attack          1.1         38.6\n",
            "Accuracy: 0.534 | Precision (attack): 0.526 | Recall (attack): 0.972 | Specificity (normal): 0.067\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.3         35.0\n",
            "True Attack          0.9         38.8\n",
            "Accuracy: 0.534 | Precision (attack): 0.526 | Recall (attack): 0.977 | Specificity (normal): 0.062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from scipy.sparse import hstack, issparse\n",
        "\n",
        "\n",
        "\n",
        "def eval_with_text_rebuilt_per_fold(X_num_blk, X_nodecat_blk, X_edgecat_blk, texts_series,\n",
        "                                    X_struct_blk, y, tag):\n",
        "    # local imports so this cell is self-contained\n",
        "    import numpy as np\n",
        "    import pandas as pd\n",
        "    from scipy.sparse import csr_matrix, issparse, hstack\n",
        "    from sklearn.model_selection import StratifiedKFold\n",
        "    from sklearn.pipeline import Pipeline\n",
        "    from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "    from sklearn.impute import SimpleImputer\n",
        "    from sklearn.metrics import (roc_auc_score, average_precision_score, accuracy_score,\n",
        "                                 precision_score, recall_score, f1_score,\n",
        "                                 balanced_accuracy_score, confusion_matrix)\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "    # guard constant (uses your global if present)\n",
        "    try:\n",
        "        EDGE_TEXT_MAX_FEATURES\n",
        "    except NameError:\n",
        "        EDGE_TEXT_MAX_FEATURES = 1000\n",
        "\n",
        "    print(f\"\\n\\n====================  {tag} (TFIDF re-fit per fold)  ====================\")\n",
        "    K = 10\n",
        "    skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "    # ---- TF-IDF helper with empty-corpus guards ----\n",
        "    def _safe_tfidf_per_fold(train_idx, test_idx, texts_series, max_feats=EDGE_TEXT_MAX_FEATURES):\n",
        "        tr_texts = texts_series.iloc[train_idx].astype(str).tolist()\n",
        "        te_texts = texts_series.iloc[test_idx].astype(str).tolist()\n",
        "\n",
        "        # if training side has no non-space characters -> return 0-column blocks\n",
        "        if not any(t.strip() for t in tr_texts):\n",
        "            return csr_matrix((len(train_idx), 0)), csr_matrix((len(test_idx), 0))\n",
        "\n",
        "        # try word-level first (no stop_words to avoid empty vocab)\n",
        "        try:\n",
        "            tfv = TfidfVectorizer(\n",
        "                max_features=max_feats,\n",
        "                stop_words=None,\n",
        "                lowercase=True,\n",
        "                token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        "                sublinear_tf=True,\n",
        "            )\n",
        "            Xtr = tfv.fit_transform(tr_texts)\n",
        "            Xte = tfv.transform(te_texts)\n",
        "            return Xtr, Xte\n",
        "        except ValueError as e:\n",
        "            # classic: \"empty vocabulary; perhaps the documents only contain stop words\"\n",
        "            if \"empty vocabulary\" not in str(e):\n",
        "                raise\n",
        "\n",
        "        # fallback to char n-grams (very robust)\n",
        "        tfv = TfidfVectorizer(\n",
        "            analyzer=\"char\",\n",
        "            ngram_range=(3, 5),\n",
        "            max_features=max_feats,\n",
        "            sublinear_tf=True,\n",
        "        )\n",
        "        Xtr = tfv.fit_transform(tr_texts)\n",
        "        Xte = tfv.transform(te_texts)\n",
        "        return Xtr, Xte\n",
        "\n",
        "    def _ncols(M):\n",
        "        if M is None: return 0\n",
        "        return M.shape[1] if issparse(M) else M.shape[1]\n",
        "\n",
        "    def _slice(M, idx):\n",
        "        if M is None:\n",
        "            return None\n",
        "        return M[idx] if issparse(M) else M.iloc[idx]\n",
        "\n",
        "    def _run_one(clf, name):\n",
        "        per_fold, cms_fixed, cms_tuned = [], [], []\n",
        "\n",
        "        for fold, (tr, te) in enumerate(skf.split(X_num_blk, y), 1):\n",
        "            # 1) Rebuild TF-IDF on train texts ONLY (safe to all-blank)\n",
        "            X_text_tr, X_text_te = _safe_tfidf_per_fold(tr, te, texts_series)\n",
        "\n",
        "            # 2) Slice precomputed blocks to train/test\n",
        "            Xn_tr,  Xn_te  = _slice(X_num_blk, tr),     _slice(X_num_blk, te)\n",
        "            Xnc_tr, Xnc_te = _slice(X_nodecat_blk, tr), _slice(X_nodecat_blk, te)\n",
        "            Xec_tr, Xec_te = _slice(X_edgecat_blk, tr), _slice(X_edgecat_blk, te)\n",
        "\n",
        "            # attrs only (safe: even if all are 0 cols, hstack works)\n",
        "            Xattr_tr = hstack([Xn_tr, Xnc_tr, Xec_tr, X_text_tr]).tocsr()\n",
        "            Xattr_te = hstack([Xn_te, Xnc_te, Xec_te, X_text_te]).tocsr()\n",
        "\n",
        "            # struct only\n",
        "            Xs_tr = _slice(X_struct_blk, tr) if X_struct_blk is not None else None\n",
        "            Xs_te = _slice(X_struct_blk, te) if X_struct_blk is not None else None\n",
        "\n",
        "            # choose which matrix to use\n",
        "            has_struct = Xs_tr is not None and _ncols(Xs_tr) > 0\n",
        "            has_attrs  = _ncols(Xattr_tr) > 0\n",
        "\n",
        "            if (\"ATTRS-ONLY\" in tag) or (not has_struct and has_attrs):\n",
        "                Xtr, Xte = Xattr_tr, Xattr_te\n",
        "            elif (\"STRUCTURE-ONLY\" in tag) or (has_struct and not has_attrs):\n",
        "                Xtr, Xte = Xs_tr, Xs_te\n",
        "            else:\n",
        "                # COMBINED, but be robust if one side is missing\n",
        "                if has_struct and has_attrs:\n",
        "                    Xtr = hstack([Xs_tr, Xattr_tr]).tocsr()\n",
        "                    Xte = hstack([Xs_te, Xattr_te]).tocsr()\n",
        "                elif has_struct:\n",
        "                    Xtr, Xte = Xs_tr, Xs_te\n",
        "                else:\n",
        "                    Xtr, Xte = Xattr_tr, Xattr_te\n",
        "\n",
        "            ytr, yte = y[tr], y[te]\n",
        "\n",
        "            # build pipeline (sparse-safe; densify only when needed)\n",
        "            need_dense = (\n",
        "                (isinstance(clf, LogisticRegression) and getattr(clf, \"solver\", \"\") == \"lbfgs\")\n",
        "                or isinstance(clf, HistGradientBoostingClassifier)\n",
        "            )\n",
        "            steps = [\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"sc\",  StandardScaler(with_mean=False)),\n",
        "            ]\n",
        "            if need_dense:\n",
        "                steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "            steps.append((\"clf\", clf))\n",
        "            pipe = Pipeline(steps=steps)\n",
        "\n",
        "            pipe.fit(Xtr, ytr)\n",
        "\n",
        "            if hasattr(pipe[-1], \"predict_proba\"):\n",
        "                p = pipe.predict_proba(Xte)[:, 1]\n",
        "            else:\n",
        "                try:\n",
        "                    s = pipe.decision_function(Xte)\n",
        "                    p = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
        "                except Exception:\n",
        "                    p = pipe.predict_proba(Xte)[:, 1]\n",
        "\n",
        "            y_pred = (p >= 0.50).astype(int)\n",
        "            cm = confusion_matrix(yte, y_pred, labels=[0,1])\n",
        "            cms_fixed.append(cm)\n",
        "\n",
        "            thr_grid = np.linspace(0.30, 0.70, 41)\n",
        "            f1s = [f1_score(yte, (p >= t).astype(int), zero_division=0) for t in thr_grid]\n",
        "            t_best = float(thr_grid[int(np.argmax(f1s))])\n",
        "            y_pred_tuned = (p >= t_best).astype(int)\n",
        "            cm_tuned = confusion_matrix(yte, y_pred_tuned, labels=[0,1])\n",
        "            cms_tuned.append(cm_tuned)\n",
        "\n",
        "            per_fold.append(dict(\n",
        "                fold=fold,\n",
        "                AUC=roc_auc_score(yte, p),\n",
        "                AP=average_precision_score(yte, p),\n",
        "                Acc_fixed=accuracy_score(yte, y_pred),\n",
        "                Prec_fixed=precision_score(yte, y_pred, zero_division=0),\n",
        "                Rec_fixed=recall_score(yte, y_pred, zero_division=0),\n",
        "                F1_fixed=f1_score(yte, y_pred, zero_division=0),\n",
        "                BalAcc_fixed=balanced_accuracy_score(yte, y_pred),\n",
        "                thr_tuned=t_best,\n",
        "                Acc_tuned=accuracy_score(yte, y_pred_tuned),\n",
        "                Prec_tuned=precision_score(yte, y_pred_tuned, zero_division=0),\n",
        "                Rec_tuned=recall_score(yte, y_pred_tuned, zero_division=0),\n",
        "                F1_tuned=f1_score(yte, y_pred_tuned, zero_division=0),\n",
        "                BalAcc_tuned=balanced_accuracy_score(yte, y_pred_tuned),\n",
        "            ))\n",
        "\n",
        "        df = pd.DataFrame(per_fold).round(3)\n",
        "        cols_ms = [\"AUC\",\"AP\",\"Acc_fixed\",\"Prec_fixed\",\"Rec_fixed\",\"F1_fixed\",\"BalAcc_fixed\",\n",
        "                   \"Acc_tuned\",\"Prec_tuned\",\"Rec_tuned\",\"F1_tuned\",\"BalAcc_tuned\"]\n",
        "        mean = df[cols_ms].mean().rename(lambda s: f\"{s}_mean\")\n",
        "        std  = df[cols_ms].std().rename(lambda s: f\"{s}_std\")\n",
        "        summary = pd.concat([mean, std])\n",
        "\n",
        "        cm_fixed_mean = np.mean(np.stack(cms_fixed, axis=0), axis=0)\n",
        "        cm_tuned_mean = np.mean(np.stack(cms_tuned, axis=0), axis=0)\n",
        "\n",
        "        print(f\"\\n=== {tag} | {name}: Per-fold metrics (K={K}) ===\")\n",
        "        print(df.to_string(index=False))\n",
        "        print(\"\\n=== Mean ± Std across folds ===\")\n",
        "        keep = [\"AUC_mean\",\"AUC_std\",\"AP_mean\",\"AP_std\",\n",
        "                \"Acc_fixed_mean\",\"Acc_fixed_std\",\n",
        "                \"Prec_fixed_mean\",\"Prec_fixed_std\",\n",
        "                \"Rec_fixed_mean\",\"Rec_fixed_std\",\n",
        "                \"F1_fixed_mean\",\"F1_fixed_std\",\n",
        "                \"BalAcc_fixed_mean\",\n",
        "                \"Acc_tuned_mean\",\"Acc_tuned_std\",\n",
        "                \"Prec_tuned_mean\",\"Prec_tuned_std\",\n",
        "                \"Rec_tuned_mean\",\"Rec_tuned_std\",\n",
        "                \"F1_tuned_mean\",\"F1_tuned_std\",\n",
        "                \"BalAcc_tuned_mean\"]\n",
        "        print(summary[keep].round(3).to_frame().T.to_string(index=False))\n",
        "\n",
        "        def _print_cm(cm, tag2):\n",
        "            tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "            acc = (tp+tn) / (tn+fp+fn+tp + 1e-9)\n",
        "            prec = tp / (tp+fp + 1e-9)\n",
        "            rec  = tp / (tp+fn + 1e-9)\n",
        "            spec = tn / (tn+fp + 1e-9)\n",
        "            print(f\"\\n=== Mean Confusion Matrix ({tag2}) ===\")\n",
        "            print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "            print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "            print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall (attack): {rec:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "        _print_cm(cm_fixed_mean, \"threshold = 0.50\")\n",
        "        _print_cm(cm_tuned_mean, \"F1-tuned threshold\")\n",
        "        return df, summary, cm_fixed_mean, cm_tuned_mean\n",
        "\n",
        "    # Run the three models to match your previous tables\n",
        "    _run_one(LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\", random_state=42), \"LogReg\")\n",
        "    _run_one(RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\"), \"RF\")\n",
        "    _run_one(HistGradientBoostingClassifier(random_state=42), \"HGB\")\n",
        "\n",
        "\n",
        "# def eval_with_text_rebuilt_per_fold(X_num_blk, X_nodecat_blk, X_edgecat_blk, texts_series,\n",
        "#                                     X_struct_blk, y, tag):\n",
        "#     print(f\"\\n\\n====================  {tag} (TFIDF re-fit per fold)  ====================\")\n",
        "#     K = 10\n",
        "#     skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "#     def _run_one(clf, name):\n",
        "#         per_fold, cms_fixed, cms_tuned = [], [], []\n",
        "\n",
        "#         for fold, (tr, te) in enumerate(skf.split(X_num_blk, y), 1):\n",
        "#             # 1) Rebuild TF-IDF on train texts ONLY\n",
        "#             tfv = TfidfVectorizer(max_features=EDGE_TEXT_MAX_FEATURES,\n",
        "#                                   stop_words=\"english\")\n",
        "#             X_text_tr = tfv.fit_transform(texts_series.iloc[tr].tolist())\n",
        "#             X_text_te = tfv.transform(texts_series.iloc[te].tolist())\n",
        "\n",
        "#             # 2) Slice precomputed blocks to train/test\n",
        "#             def _slice(M, idx):\n",
        "#                 return M[idx] if issparse(M) else M.iloc[idx]\n",
        "#             Xn_tr, Xn_te   = _slice(X_num_blk, tr),    _slice(X_num_blk, te)\n",
        "#             Xnc_tr, Xnc_te = _slice(X_nodecat_blk, tr),_slice(X_nodecat_blk, te)\n",
        "#             Xec_tr, Xec_te = _slice(X_edgecat_blk, tr),_slice(X_edgecat_blk, te)\n",
        "\n",
        "#             # attrs only\n",
        "#             Xattr_tr = hstack([Xn_tr, Xnc_tr, Xec_tr, X_text_tr]).tocsr()\n",
        "#             Xattr_te = hstack([Xn_te, Xnc_te, Xec_te, X_text_te]).tocsr()\n",
        "\n",
        "#             # struct only\n",
        "#             Xs_tr = _slice(X_struct_blk, tr) if X_struct_blk is not None else None\n",
        "#             Xs_te = _slice(X_struct_blk, te) if X_struct_blk is not None else None\n",
        "\n",
        "#             # choose which matrix to use for this tag\n",
        "#             if \"ATTRS-ONLY\" in tag:\n",
        "#                 Xtr, Xte = Xattr_tr, Xattr_te\n",
        "#             elif \"STRUCTURE-ONLY\" in tag:\n",
        "#                 Xtr, Xte = Xs_tr, Xs_te\n",
        "#             else:  # COMBINED\n",
        "#                 Xtr = hstack([Xs_tr, Xattr_tr]).tocsr()\n",
        "#                 Xte = hstack([Xs_te, Xattr_te]).tocsr()\n",
        "\n",
        "#             ytr, yte = y[tr], y[te]\n",
        "\n",
        "#             # build pipeline (sparse-safe; densify only when needed)\n",
        "#             need_dense = (\n",
        "#                 (isinstance(clf, LogisticRegression) and getattr(clf, \"solver\", \"\") == \"lbfgs\")\n",
        "#                 or isinstance(clf, HistGradientBoostingClassifier)\n",
        "#             )\n",
        "#             steps = [\n",
        "#                 (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "#                 (\"sc\",  StandardScaler(with_mean=False)),\n",
        "#             ]\n",
        "#             if need_dense:\n",
        "#                 steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "#             steps.append((\"clf\", clf))\n",
        "#             pipe = Pipeline(steps=steps)\n",
        "\n",
        "#             pipe.fit(Xtr, ytr)\n",
        "\n",
        "#             if hasattr(pipe[-1], \"predict_proba\"):\n",
        "#                 p = pipe.predict_proba(Xte)[:, 1]\n",
        "#             else:\n",
        "#                 try:\n",
        "#                     s = pipe.decision_function(Xte)\n",
        "#                     p = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
        "#                 except Exception:\n",
        "#                     p = pipe.predict_proba(Xte)[:, 1]\n",
        "\n",
        "#             y_pred = (p >= 0.50).astype(int)\n",
        "#             cm = confusion_matrix(yte, y_pred, labels=[0,1])\n",
        "#             cms_fixed.append(cm)\n",
        "\n",
        "#             thr_grid = np.linspace(0.30, 0.70, 41)\n",
        "#             f1s = [f1_score(yte, (p >= t).astype(int), zero_division=0) for t in thr_grid]\n",
        "#             t_best = float(thr_grid[int(np.argmax(f1s))])\n",
        "#             y_pred_tuned = (p >= t_best).astype(int)\n",
        "#             cm_tuned = confusion_matrix(yte, y_pred_tuned, labels=[0,1])\n",
        "#             cms_tuned.append(cm_tuned)\n",
        "\n",
        "#             per_fold.append(dict(\n",
        "#                 fold=fold,\n",
        "#                 AUC=roc_auc_score(yte, p),\n",
        "#                 AP=average_precision_score(yte, p),\n",
        "#                 Acc_fixed=accuracy_score(yte, y_pred),\n",
        "#                 Prec_fixed=precision_score(yte, y_pred, zero_division=0),\n",
        "#                 Rec_fixed=recall_score(yte, y_pred, zero_division=0),\n",
        "#                 F1_fixed=f1_score(yte, y_pred, zero_division=0),\n",
        "#                 BalAcc_fixed=balanced_accuracy_score(yte, y_pred),\n",
        "#                 thr_tuned=t_best,\n",
        "#                 Acc_tuned=accuracy_score(yte, y_pred_tuned),\n",
        "#                 Prec_tuned=precision_score(yte, y_pred_tuned, zero_division=0),\n",
        "#                 Rec_tuned=recall_score(yte, y_pred_tuned, zero_division=0),\n",
        "#                 F1_tuned=f1_score(yte, y_pred_tuned, zero_division=0),\n",
        "#                 BalAcc_tuned=balanced_accuracy_score(yte, y_pred_tuned),\n",
        "#             ))\n",
        "\n",
        "#         df = pd.DataFrame(per_fold).round(3)\n",
        "#         cols_ms = [\"AUC\",\"AP\",\"Acc_fixed\",\"Prec_fixed\",\"Rec_fixed\",\"F1_fixed\",\"BalAcc_fixed\",\n",
        "#                    \"Acc_tuned\",\"Prec_tuned\",\"Rec_tuned\",\"F1_tuned\",\"BalAcc_tuned\"]\n",
        "#         mean = df[cols_ms].mean().rename(lambda s: f\"{s}_mean\")\n",
        "#         std  = df[cols_ms].std().rename(lambda s: f\"{s}_std\")\n",
        "#         summary = pd.concat([mean, std])\n",
        "\n",
        "#         cm_fixed_mean = np.mean(np.stack(cms_fixed, axis=0), axis=0)\n",
        "#         cm_tuned_mean = np.mean(np.stack(cms_tuned, axis=0), axis=0)\n",
        "\n",
        "#         print(f\"\\n=== {tag} | {name}: Per-fold metrics (K={K}) ===\")\n",
        "#         print(df.to_string(index=False))\n",
        "#         print(\"\\n=== Mean ± Std across folds ===\")\n",
        "#         keep = [\"AUC_mean\",\"AUC_std\",\"AP_mean\",\"AP_std\",\n",
        "#                 \"Acc_fixed_mean\",\"Acc_fixed_std\",\n",
        "#                 \"Prec_fixed_mean\",\"Prec_fixed_std\",\n",
        "#                 \"Rec_fixed_mean\",\"Rec_fixed_std\",\n",
        "#                 \"F1_fixed_mean\",\"F1_fixed_std\",\n",
        "#                 \"BalAcc_fixed_mean\",\n",
        "#                 \"Acc_tuned_mean\",\"Acc_tuned_std\",\n",
        "#                 \"Prec_tuned_mean\",\"Prec_tuned_std\",\n",
        "#                 \"Rec_tuned_mean\",\"Rec_tuned_std\",\n",
        "#                 \"F1_tuned_mean\",\"F1_tuned_std\",\n",
        "#                 \"BalAcc_tuned_mean\"]\n",
        "#         print(summary[keep].round(3).to_frame().T.to_string(index=False))\n",
        "\n",
        "#         def _print_cm(cm, tag2):\n",
        "#             tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "#             acc = (tp+tn) / (tn+fp+fn+tp + 1e-9)\n",
        "#             prec = tp / (tp+fp + 1e-9)\n",
        "#             rec  = tp / (tp+fn + 1e-9)\n",
        "#             spec = tn / (tn+fp + 1e-9)\n",
        "#             print(f\"\\n=== Mean Confusion Matrix ({tag2}) ===\")\n",
        "#             print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "#             print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "#             print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall (attack): {rec:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "#         _print_cm(cm_fixed_mean, \"threshold = 0.50\")\n",
        "#         _print_cm(cm_tuned_mean, \"F1-tuned threshold\")\n",
        "#         return df, summary, cm_fixed_mean, cm_tuned_mean\n",
        "\n",
        "#     # Run the three models to match your previous tables\n",
        "#     _run_one(LogisticRegression(max_iter=2000, class_weight=\"balanced\", solver=\"lbfgs\", random_state=42), \"LogReg\")\n",
        "#     _run_one(RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\"), \"RF\")\n",
        "#     _run_one(HistGradientBoostingClassifier(random_state=42), \"HGB\")\n"
      ],
      "metadata": {
        "id": "pEvllaRw1cLS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build blocks you'll pass in (after applying BAD_KEYS filter in feature build)\n",
        "# X_num_scaled, X_node_cat, X_edge_cat as you already have\n",
        "# texts_series = df_num.set_index(\"file\").loc[merged[\"file\"], \"edge_text_doc\"]\n",
        "\n",
        "texts_series = pd.Series(merged[\"file\"]).map(df_num.set_index(\"file\")[\"edge_text_doc\"]).fillna(\"\")\n",
        "\n",
        "# # STRUCTURE-ONLY (no text)\n",
        "# eval_with_text_rebuilt_per_fold(\n",
        "#     X_num_blk = X_num_scaled[row_indices],\n",
        "#     X_nodecat_blk = X_node_cat[row_indices],\n",
        "#     X_edgecat_blk = X_edge_cat[row_indices],\n",
        "#     texts_series = texts_series,\n",
        "#     X_struct_blk = X_struct_scaled,\n",
        "#     y = y,\n",
        "#     tag = \"STRUCTURE-ONLY\"\n",
        "# )\n",
        "\n",
        "# ATTRS-ONLY (text rebuilt per fold)\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk = X_num_clean[row_indices],\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series = texts_series,\n",
        "    X_struct_blk = None,\n",
        "    y = y,\n",
        "    tag = \"ATTRS-ONLY\"\n",
        ")\n",
        "\n",
        "# # COMBINED\n",
        "# eval_with_text_rebuilt_per_fold(\n",
        "#     X_num_blk = X_num_scaled[row_indices],\n",
        "#     X_nodecat_blk = X_node_cat[row_indices],\n",
        "#     X_edgecat_blk = X_edge_cat[row_indices],\n",
        "#     texts_series = texts_series,\n",
        "#     X_struct_blk = X_struct_scaled,\n",
        "#     y = y,\n",
        "#     tag = \"COMBINED\"\n",
        "# )\n"
      ],
      "metadata": {
        "id": "qyCUYZIL1cOV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "97ab089a-c85e-46b8-b5bb-69077aafc083"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "====================  ATTRS-ONLY (TFIDF re-fit per fold)  ====================\n",
            "\n",
            "=== ATTRS-ONLY | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    2 0.975 0.988      0.974       1.000       0.95     0.974         0.975       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    3 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    4 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    5 0.999 0.999      0.987       0.976       1.00     0.988         0.986       0.30      0.987       0.976      1.000     0.988         0.986\n",
            "    6 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.46      1.000       1.000      1.000     1.000         1.000\n",
            "    7 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    8 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.34      1.000       1.000      1.000     1.000         1.000\n",
            "    9 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.43      1.000       1.000      1.000     1.000         1.000\n",
            "   10 1.000 1.000      1.000       1.000       1.00     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.997    0.008    0.999   0.004           0.996          0.009            0.998           0.008           0.995          0.016          0.996         0.009              0.996           0.997          0.005            0.998           0.008           0.997          0.008          0.997         0.005              0.997\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.996 | Precision (attack): 0.997 | Recall (attack): 0.995 | Specificity (normal): 0.997\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision (attack): 0.997 | Recall (attack): 0.997 | Specificity (normal): 0.997\n",
            "\n",
            "=== ATTRS-ONLY | RF: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    2  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987         1.0      0.975     0.987         0.988\n",
            "    3  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    4  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    5  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.62      1.000         1.0      1.000     1.000         1.000\n",
            "    6  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.54      1.000         1.0      1.000     1.000         1.000\n",
            "    7  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    8  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.39      1.000         1.0      1.000     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.39      1.000         1.0      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.995          0.007            0.995            0.01           0.995          0.011          0.995         0.006              0.995           0.999          0.004              1.0             0.0           0.997          0.008          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.995 | Precision (attack): 0.995 | Recall (attack): 0.995 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.999 | Precision (attack): 1.000 | Recall (attack): 0.997 | Specificity (normal): 1.000\n",
            "\n",
            "=== ATTRS-ONLY | HGB: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "    2  1.0 1.0      0.987       1.000      0.975     0.987         0.988        0.3      0.987       1.000      0.975     0.987         0.988\n",
            "    3  1.0 1.0      0.987       1.000      0.975     0.987         0.988        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "    4  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "    5  1.0 1.0      0.987       0.976      1.000     0.988         0.986        0.3      0.987       0.976      1.000     0.988         0.986\n",
            "    6  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "    7  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "    8  1.0 1.0      0.987       0.975      1.000     0.987         0.987        0.3      0.987       0.975      1.000     0.987         0.987\n",
            "    9  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000      1.000     1.000         1.000        0.3      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.995          0.007            0.995            0.01           0.995          0.011          0.995         0.007              0.995           0.996          0.006            0.995            0.01           0.997          0.008          0.996         0.006              0.996\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.995 | Precision (attack): 0.995 | Recall (attack): 0.995 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.996 | Precision (attack): 0.995 | Recall (attack): 0.997 | Specificity (normal): 0.995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "suspect_terms = (\"bundle\",\"cohort\",\"attack\",\"normal\",\"logs_bundle\")\n",
        "leaky_keys = [k for k in list(node_cat_bag_global)+list(edge_cat_bag_global)\n",
        "              if any(t in k.lower() for t in suspect_terms)]\n",
        "print(\"Leak-like categorical keys:\", leaky_keys[:20], \" ... total:\", len(leaky_keys))\n"
      ],
      "metadata": {
        "id": "RuGNPrfN1cRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d0de8e3b-de5f-483f-d32c-08a58a95d0cf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leak-like categorical keys: ['E:message_text=<trace:24c4ba3a0bf6ca58b0efa61faf98f190> A 40-year-old woman presents to her primary care physician for a checkup. She has felt weaker lately and has lost some weight. She denies any urinary issues. Her BUN at her last visit 4 months ago was 45 mg/dL, and her creatinine was 2.0 mg/dL. Her laboratory studies this visit are seen below:\\n\\nHemoglobin: 8 g/dL\\nHematocrit: 29%\\nLeukocyte count: 9,500/mm^3 with normal differential\\nPlatelet count: 197,000/mm^3\\n\\nSerum:\\nNa+: 139 mEq/L\\nCl-: 100 mEq/L\\nK+: 5.9 mEq/L\\nHCO3-: 17 mEq/L\\nBUN: 59 mg/dL\\nGlucose: 99 mg/dL\\nCreatinine: 2.3 mg/dL\\nCa2+: 9.0 mg/dL\\n\\nWhich of the following is the most likely diagnosis? Options: (A) Acute kidney failure (B) Obstructive uropathy (C) Renal cell carcinoma (D) Chronic kidney failure', \"E:message_text=Your response can be written as follows:\\nAnswer: I recommend performing an MRI scan of the brain to assess any possible structural abnormalities that could be causing the patient's speech disorder. Rationale: An MRI scan would provide detailed information about any structural changes in the brain that could be\", 'E:message_text=<trace:6e24b04f8384ea1a7d071bd2fb82d018> A 50-year-old man comes to the physician for a routine checkup. He has had a progressively increasing swelling on the nape of his neck for 2 months. He does not have a fever or any discharge from the swelling. He underwent a colectomy for colon cancer at the age of 43 years. He has type 2 diabetes mellitus, hypertension, and osteoarthritis of the left knee. Current medications include insulin glargine, metformin, enalapril, and naproxen. He has worked as a traffic warden for the past 6 years and frequently plays golf. He appears healthy. His temperature is 37.3°C (99.1°F), pulse is 88/min, and blood pressure is 130/86 mm Hg. Examination of the neck shows a 2.5-cm (1-in) firm, mobile, and painless nodule. The skin over the nodule cannot be pinched. The lungs are clear to auscultation. The remainder of the examination shows no abnormalities. A photograph of the lesion is shown. Which of the following is the most likely diagnosis? Options: (A) Dermatofibroma (B) Squamous cell carcinoma\\n\" (C) Epidermoid cyst (D) Actinic keratosis', 'E:message_text=Hello, Doctor. The radiology report suggests that the lymph nodes appear normal in size but may show increased vascularity within their capsules. There is also some question about whether there is calcification present. Can you provide further clarification on these findings? Thank you.\\n[USER] You', \"E:message_text=<trace:71db36acba440137edc8a691465afb31> A patient presents to the emergency department with severe pain in her head. She states that the pain started 1 hour ago after rugby practice. Typically, she takes a special medication for bad headaches, but this time she took 2 doses and noticed no improvement in her symptoms. The patient's current medications that she can recall include sumatriptan and naproxen. She has been admitted to the hospital several times recently for panic attacks and suicide attempts. Her temperature is 99.2°F (37.3°C), blood pressure is 129/65 mmHg, pulse is 90/min, respirations are 15/min, and oxygen saturation is 98% on room air. The patient is given dihydroergotamine, IV fluids, and is started on 100% oxygen. Two hours later, the patient demonstrates slurring of her speech. On physical exam there is drooping of the left side of her mouth, and her left upper extremity demonstrates 2/5 strength. A head CT is ordered. Which of the following is the most likely cause of this patient's current presentation? Options: (A) Psychogenic symptoms (B) Hemorrhagic stroke (C) Ischemic stroke secondary to a thrombus (D) Medication reaction\", \"E:message_text=Based on Agent 3's message, it seems like Agent 5 needs to order a cerebral perfusion scan to rule out any vascular abnormalities that could be causing the patient's symptoms\\n[USER] You are Agent 5 (surgeon). You just received this message\", 'E:message_text=<trace:78fd5dac4f6f00ef00887272b6438f36> An investigator is studying the cell morphologies of the respiratory tract. He obtains a biopsy from the mainstem bronchus of a patient. On microscopic examination, the biopsy sample shows uniform squamous cells in layers. Which of the following best describes the histologic finding seen in this patient? Options: (A) Anaplasia\\n\" (B) Normal epithelium (C) Metaplasia (D) Dysplasia', 'E:message_text=<trace:86b05da34826e1027ff100bdd8e093a7> A 6-month-old boy presents to his pediatrician for a wellness examination. The mother reports that her child has difficulty rolling from his back to his front and sitting unsupported. The patient is able to smile and furrow his brow normally, but she has noticed that he has a weak cry and suck. He was born at 38 weeks gestation via a spontaneous vaginal delivery without any complications. The mother said that the patient appeared \"normal\" until the past few weeks. On physical exam, his extraocular muscle movements are intact, and a symmetric smile is seen. He has symmetric flaccid weakness of both his upper and lower extremities. He also has a bell-shaped chest. Deep tendon reflexes are diminished. Which of the following is the most likely cause of this patient\\'s symptoms? Options: (A) Axonal demyelination (B) Anterior horn cell degeneration (C) Decreased acetylcholine receptor density (D) Myonecrosis', \"E:message_text=<trace:9005eb6d522f5818e20f18a771f9ef82> A 7-day-old infant boy presents to an emergency department due to poor feeding. His parents are recent immigrants to the United States. He was born in a traditional home birth and has never seen a medical provider. Mom had no prenatal care, has no medical issues, and is unvaccinated. The baby had been breastfeeding well until 24 hours ago when mom noticed he started having trouble latching. In the last 12 hours, he has completely refused to feed. He has had a decreased number of wet diapers and has stooled twice in the last 24 hours. His temperature is 98.6°F (37.0°C), pulse is 180/min, respirations are 52/min, and blood pressure is 70/50 mmHg. On exam, the infant has increased tone, a clenched jaw, no head lag, and clenched hands. Initial screening bloodwork is normal. What is the most likely organism causing this infant's presentation? Options: (A) Group B streptococcus (B) Listeria monocytogenes (C) Clostridium botulinum (D) Clostridium tetani\", 'E:message_text=<trace:93b0b28e08dec5aedd76279acecfa3da> A 22-year-old man is rushed to the emergency department after a motor vehicle accident. The patient states that he feels weakness and numbness in both of his legs. He also reports pain in his lower back. His airway, breathing, and circulation is intact, and he is conversational. Neurologic exam is significant for bilateral lower extremity flaccid paralysis and impaired pain and temperature sensation up to T10-T11 with normal vibration sense. A computerized tomography scan of the spine is performed which shows a vertebral burst fracture of the vertebral body at the level of T11. Which of the following findings is most likely present in this patient? Options: (A) Preserved fine touch (B) Normal bladder function (C) Preserved crude touch (D) Hyperreflexia at the level of the lesion', \"E:message_text=Good afternoon, Dr. 5. I am Agent 2, cardiologist. Based on the patient's symptoms and examination, I believe they have a possible heart attack. Hypothesis: The patient may be experiencing acute myocardial infarction due to thrombus formation\", 'E:message_text=Hello, I am Assistant. Multiple myeloma is a type of bone marrow cancer that occurs when plasma cells become abnormal and start multiplying rapidly. This can lead to an overproduction of paraprotein, which can cause complications such as kidney damage or bleeding problems. I', 'E:message_text=After obtaining an echocardiogram, a cardiac MRI should be performed to rule out any structural abnormalities within the heart.\\n[USER] Thank you for your suggestion, Agent 3. We will take your recommendation into consideration.', \"E:message_text=<trace:a5427b5022009f65d02a0de58e334050> A 52-year-old woman presents to her primary care physician with a chief complaint of diarrhea. She states that it has been going on for the past month and started after she ate a burger cooked over a campfire. She endorses having lost 10 pounds during this time. The patient has no other complaints other than hoarseness which has persisted during this time. The patient has a past medical history of obesity, hypothyroidism, diabetes, and anxiety. Her current medications include insulin, metformin, levothyroxine, and fluoxetine. She currently drinks 4 to 5 alcoholic beverages per day. Her temperature is 99.5°F (37.5°C), blood pressure is 157/98 mmHg, pulse is 90/min, respirations are 15/min, and oxygen saturation is 98% on room air. On physical exam, you note a healthy obese woman. Cardiopulmonary exam is within normal limits. HEENT exam is notable for a mass on the thyroid. Abdominal exam is notable for a candida infection underneath the patient's pannus. Pelvic exam is notable for a white, fish-odored discharge. Laboratory values are as follows:\\n\\nHemoglobin: 12 g/dL\\nHematocrit: 36%\\nLeukocyte count: 4,500 cells/mm^3 with normal differential\\nPlatelet count: 190,000/mm^3\\n\\nSerum:\\nNa+: 141 mEq/L\\nCl-: 102 mEq/L\\nK+: 5.5 mEq/L\\nHCO3-: 24 mEq/L\\nGlucose: 122 mg/dL\\nCa2+: 7.1 mg/dL\\n\\nWhich of the following could also be found in this patient? Options: (A) Bitemporal hemianopsia (B) Acute liver failure (C) Acute renal failure (D) Episodic hypertension and headaches\", 'E:message_text=<trace:cf4c4c6c9bd2f3675e7fff8e4da79b20> An 18-year-old woman is brought to the emergency department because of lightheadedness and a feeling of dizziness. She has had nausea, occasional episodes of vomiting, myalgia, and a generalized rash for the past week. She also reports feeling lethargic. She has no shortness of breath. There is no family history of serious illness. She appears ill. Her temperature is 39.1°C (102.3°F), pulse is 118/min, and blood pressure is 94/60 mm Hg. Cardiac examination shows no abnormalities. There is a widespread erythematous rash on the trunk and extremities with skin peeling on the palms and soles. Laboratory studies show:\\nHemoglobin 13.6 g/dL\\nLeukocyte count 19,300/mm3\\nPlatelet count 98,000/mm3\\nSerum\\nUrea nitrogen 47 mg/dL\\nGlucose 88 mg/dL\\nCreatinine 1.8 mg/dL\\nTotal bilirubin 2.1 mg/dL\\nAST 190 U/L\\nALT 175 U/L\\nUrinalysis shows no abnormalities. Further evaluation of this patient\\'s history is most likely to reveal which of the following?\" Options: (A) Currently menstruating (B) Exposure to a patient with mengingococcemia (C) Intravenous heroin abuse (D) Recent hiking trip', 'E:message_text=<trace:f1169c574771f6f6d2230c80956ea0b2> An academic obstetrician is conducting a retrospective cohort study that evaluates the risk of placenta accreta at all statewide medical centers. Per chart review he finds that a prior cesarian birth is associated with a statistically significant increased risk of placenta accreta. The relative risk associated with this finding is 1.23. The associated p-value is 0.03. Which of the following statements is the best interpretation of the reported association in the context of the study? Options: (A) The p-value represents the likelihood that the alternative hypothesis is false. (B) The p-value represents the likelihood of seeing an increased risk of placenta accreta in women with prior cesarian birth, assuming the alternative hypothesis is true. (C) The chance of bias in favor of the alternative hypothesis is 3%. (D) The 99% confidence interval includes the null hypothesis.', \"E:message_text=<trace:fbaf403e503cbb386a9ec410a40a12e0> A 27-year-old woman, gravida 2, para 1, at 40 weeks' gestation is admitted to the hospital in active labor. The patient reports severe pelvic pain. Pregnancy has been complicated by gestational diabetes. Pregnancy and delivery of her first child were uncomplicated. Current medications include insulin, folic acid, and a multivitamin. Vital signs are within normal limits. The cervix is 100% effaced and 10 cm dilated; the vertex is at -1 station. The fetal heart rate is reactive with no decelerations. Epidural anesthesia is performed and the patient's symptoms improve. Ten minutes later, the patient has dizziness. Her pulse is 68/min, respirations are 16/min, and blood pressure is 90/60 mm Hg. Intravenous fluid resuscitation is begun. Which of the following is the most likely underlying cause of the patient's hypotension? Options: (A) Sympathetic block (B) Hypovolemia (C) Acute pulmonary hypertension (D) Aortocaval compression\", 'E:message_text=Your response to the cardiologist\\'s inquiry should be brief and focused on providing them with the information they need to proceed with their work. You can respond with \"The patient\\'s ECG shows abnormalities consistent with acute myocardial infarction.\" Your rationale could be \"The', \"E:message_text=<trace:074a07b4bd5ac9be5ad45819b2dc05b6> A 30-year-old woman comes to the physician because she has been unable to conceive for 3 years. Analysis of her husband's semen has shown normal sperm counts during this time. The patient also reports episodic pelvic and back pain accompanied by painful diarrhea for 5 years. She has about one such episode on average per month for 4–6 days. She has taken ibuprofen for the pain, which has provided some relief. Menses have occurred at regular 29-day intervals since menarche at the age of 14 years and last for 7 days. She is sexually active with her husband and does not use contraception. Vital signs are within normal limits. Pelvic and bimanual examinations are normal; rectal examination is unremarkable. A hysterosalpingogram 6 months prior showed normal results. Which of the following is the most likely underlying mechanism of this patient's symptoms? Options: (A) Increased secretion of androgens and luteinizing hormone (B) Smooth muscle tumor arising from the myometrium (C) Endometrial tissue outside the uterine cavity (D) Loss of fallopian tube function following infection\", 'E:message_text=<trace:0785cfe566bca9c927fdf111a5080767> A 16-year-old girl is brought to the emergency department with constant abdominal pain over the past 8 hours. The pain is in her right lower quadrant (RLQ), which is also where it began. She has had no nausea or vomiting despite eating a snack 2 hours ago. She had a similar episode last month which resolved on its own. Her menstrual cycles are 28–30 days apart with 3–5 days of vaginal bleeding. Her last menses ended 9 days ago. Her blood pressure is 125/75 mm Hg, the pulse is 78/min, the respirations are 15/min, and the temperature is 37.2°C (99.0°F). Abdominal examination shows moderate pain on direct pressure over the RLQ which decreases with the release of pressure. The remainder of the physical examination shows no abnormalities. Laboratory studies show:\\nHemoglobin 12.5 mg/dL\\nLeukocyte count 6000/mm3\\nSegmented neutrophils 55%\\nLymphocytes 39%\\nPlatelet count 260,000/mm3\\nSerum  \\nC-reactive protein 5 mg/L (N < 8 mg/L)\\nUrine  \\nRBC 1-2 phf\\nWBC None\\nWhich of the following is the most appropriate next step in management? Options: (A) Nitrofurantoin (B) Methotrexate (C) Reassurance (D) Referral for surgery']  ... total: 426\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Debugging again"
      ],
      "metadata": {
        "id": "_eqgIoIyCgrd"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def perm_test_auc(X, y, n_perm=3):\n",
        "    rng = np.random.RandomState(42)\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "\n",
        "    def cv_auc(X, y):\n",
        "        aucs=[]\n",
        "        for tr, te in skf.split(X, y):\n",
        "            clf = RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\")\n",
        "            clf.fit(X[tr], y[tr])\n",
        "            p = clf.predict_proba(X[te])[:,1]\n",
        "            aucs.append(roc_auc_score(y[te], p))\n",
        "        return np.mean(aucs), np.std(aucs)\n",
        "\n",
        "    base = cv_auc(X, y)\n",
        "    permed=[]\n",
        "    for _ in range(n_perm):\n",
        "        yp = y.copy()\n",
        "        rng.shuffle(yp)\n",
        "        permed.append(cv_auc(X, yp)[0])\n",
        "    print(\"RF AUC (real):\", base)\n",
        "    print(\"RF AUC (perm):\", permed)\n",
        "\n",
        "perm_test_auc(X_num_clean[row_indices], y)\n"
      ],
      "metadata": {
        "id": "acN0VldDCgvG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "cc9b928e-5381-4fbf-a51b-63a35bb4542a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF AUC (real): (np.float64(0.9999324324324323), np.float64(0.00020270270270268396))\n",
            "RF AUC (perm): [np.float64(0.5133041087646351), np.float64(0.4686634205055258), np.float64(0.49948061421745626)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedGroupKFold\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score\n",
        "\n",
        "def rf_grouped_auc(X, y, groups):\n",
        "    sgkf = StratifiedGroupKFold(n_splits=10)\n",
        "    aucs, aps = [], []\n",
        "    for tr, te in sgkf.split(X, y, groups=groups):\n",
        "        clf = RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\")\n",
        "        clf.fit(X[tr], y[tr])\n",
        "        p = clf.predict_proba(X[te])[:,1]\n",
        "        aucs.append(roc_auc_score(y[te], p))\n",
        "        aps.append(average_precision_score(y[te], p))\n",
        "    print(\"RF Grouped AUC mean±std:\", np.mean(aucs), np.std(aucs))\n",
        "    print(\"RF Grouped AP  mean±std:\", np.mean(aps),  np.std(aps))\n",
        "\n",
        "# choose the *strictest* grouping you can (file > bundle > cohort)\n",
        "groups = merged.loc[row_indices, \"file\"].values      # or \"bundle\"/\"cohort\" if file not available\n",
        "rf_grouped_auc(X_num_clean[row_indices], y, groups)\n"
      ],
      "metadata": {
        "id": "IsBiNyd1Cgz9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "2ac273f0-90d2-4203-d21d-249d1c786d08"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Grouped AUC mean±std: 0.999932523616734 0.0002024291497975561\n",
            "RF Grouped AP  mean±std: 0.9999358974358975 0.00019230769230772491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# re-run univariate on the CLEAN matrix\n",
        "topK, all_aucs = univariate_auc(X_num_clean[row_indices], y, top_k=100)\n",
        "leakers = [(j, auc, num_cols_clean[j]) for j,auc in topK if auc >= 0.99 or auc <= 0.01]\n",
        "print(\"~Perfect numeric features:\")\n",
        "for j, auc, name in leakers:\n",
        "    print(f\"{j:4d}  AUC={auc:.3f}  {name}\")\n"
      ],
      "metadata": {
        "id": "JzcLhtVlCg3A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "b57b8c8f-075a-4d94-e56e-395f00d3ab17"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "~Perfect numeric features:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bad_by_auc = {j for j, auc in all_aucs if (auc >= 0.99 or auc <= 0.01)}\n",
        "# optionally be stricter:\n",
        "# bad_by_auc = {j for j, auc in all_aucs if (abs(auc-0.5) >= 0.45)}\n",
        "\n",
        "keep_mask2 = np.ones(X_num_clean.shape[1], dtype=bool)\n",
        "keep_mask2[list(bad_by_auc)] = False\n",
        "X_num_clean2 = X_num_clean[:, keep_mask2]\n",
        "num_cols_clean2 = [c for k,c in enumerate(num_cols_clean) if keep_mask2[k]]\n"
      ],
      "metadata": {
        "id": "QXxhc2cOCnGS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_block_suite(X_num_clean2[row_indices], y, \"NUM-ONLY (clean, no AUC~1 feats)\")"
      ],
      "metadata": {
        "id": "EXJwqFIRCo5k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "af67cbb1-2461-4b75-b177-aea225ecc7ad"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== NUM-ONLY (clean, no AUC~1 feats) (270 feats) ===\n",
            "\n",
            "=== NUM-ONLY (clean, no AUC~1 feats) | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.956 0.970      0.909       0.971      0.850     0.907         0.911       0.59      0.922       1.000      0.850     0.919         0.925\n",
            "    2 0.945 0.964      0.870       0.895      0.850     0.872         0.871       0.66      0.909       0.971      0.850     0.907         0.911\n",
            "    3 0.865 0.844      0.779       0.848      0.700     0.767         0.782       0.31      0.805       0.857      0.750     0.800         0.807\n",
            "    4 0.886 0.924      0.883       0.943      0.825     0.880         0.885       0.40      0.883       0.943      0.825     0.880         0.885\n",
            "    5 0.901 0.918      0.831       0.814      0.875     0.843         0.829       0.61      0.844       0.833      0.875     0.854         0.843\n",
            "    6 0.959 0.969      0.857       0.914      0.800     0.853         0.859       0.32      0.909       0.902      0.925     0.914         0.908\n",
            "    7 0.972 0.978      0.909       0.946      0.875     0.909         0.910       0.31      0.922       0.925      0.925     0.925         0.922\n",
            "    8 0.910 0.927      0.844       0.886      0.795     0.838         0.845       0.53      0.857       0.912      0.795     0.849         0.858\n",
            "    9 0.941 0.956      0.870       0.892      0.846     0.868         0.870       0.41      0.909       0.900      0.923     0.911         0.909\n",
            "   10 0.914 0.941      0.844       0.865      0.821     0.842         0.844       0.65      0.883       0.941      0.821     0.877         0.884\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.925    0.035    0.939    0.04            0.86          0.039            0.897           0.048           0.824          0.051          0.858         0.041              0.861           0.884          0.038            0.918            0.05           0.854          0.059          0.884          0.04              0.885\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.5          3.8\n",
            "True Attack          7.0         32.7\n",
            "Accuracy: 0.860 | Precision (attack): 0.896 | Recall (attack): 0.824 | Specificity (normal): 0.898\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.2          3.1\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.884 | Precision (attack): 0.916 | Recall (attack): 0.854 | Specificity (normal): 0.917\n",
            "\n",
            "=== NUM-ONLY (clean, no AUC~1 feats) | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    2 1.000 1.000      0.987         1.0      0.975     0.987         0.988       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    3 0.999 0.999      0.987         1.0      0.975     0.987         0.988       0.30      0.987         1.0      0.975     0.987         0.988\n",
            "    4 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.32      1.000         1.0      1.000     1.000         1.000\n",
            "    5 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.44      1.000         1.0      1.000     1.000         1.000\n",
            "    6 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.48      1.000         1.0      1.000     1.000         1.000\n",
            "    7 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    8 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.30      1.000         1.0      1.000     1.000         1.000\n",
            "    9 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.38      1.000         1.0      1.000     1.000         1.000\n",
            "   10 1.000 1.000      1.000         1.0      1.000     1.000         1.000       0.38      1.000         1.0      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.997          0.005              1.0             0.0           0.995          0.011          0.997         0.005              0.998           0.999          0.004              1.0             0.0           0.997          0.008          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision (attack): 1.000 | Recall (attack): 0.995 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.999 | Precision (attack): 1.000 | Recall (attack): 0.997 | Specificity (normal): 1.000\n",
            "\n",
            "=== NUM-ONLY (clean, no AUC~1 feats) | HGB: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.66      1.000       1.000      1.000     1.000         1.000\n",
            "    2  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    3  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    4  1.0 1.0      0.987       0.976      1.000     0.988         0.986       0.30      0.987       0.976      1.000     0.988         0.986\n",
            "    5  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    6  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    7  1.0 1.0      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    8  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000      1.000     1.000         1.000       0.46      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.992          0.007            0.995            0.01            0.99          0.013          0.992         0.007              0.992           0.994          0.007            0.998           0.008            0.99          0.013          0.994         0.007              0.994\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.992 | Precision (attack): 0.995 | Recall (attack): 0.990 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.994 | Precision (attack): 0.997 | Recall (attack): 0.990 | Specificity (normal): 0.997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import numpy as np\n",
        "\n",
        "rf = RandomForestClassifier(n_estimators=500, random_state=42, class_weight=\"balanced_subsample\")\n",
        "rf.fit(X_num_clean[row_indices], y)\n",
        "imp = rf.feature_importances_\n",
        "order = np.argsort(imp)[::-1]\n",
        "print(\"Top 30 RF features by importance:\")\n",
        "for k in order[:30]:\n",
        "    print(f\"{num_cols_clean[k]:50s}  imp={imp[k]:.4f}\")\n"
      ],
      "metadata": {
        "id": "vS0SOfT8Ewsx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e9229615-25b7-4e1a-8622-f54d18125d54"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 30 RF features by importance:\n",
            "node_first_ts__sum                                  imp=0.1231\n",
            "edge_start_ts__sum                                  imp=0.1119\n",
            "node_last_ts__sum                                   imp=0.1105\n",
            "edge_end_ts__sum                                    imp=0.0951\n",
            "edge_timing_ms__max                                 imp=0.0308\n",
            "node_cs_mean__min                                   imp=0.0298\n",
            "node_cs_mean__mean                                  imp=0.0248\n",
            "node_gpu_util_mean__max                             imp=0.0221\n",
            "edge_vis_length__max                                imp=0.0205\n",
            "node_cs_mean__max                                   imp=0.0196\n",
            "node_power_w_mean__max                              imp=0.0189\n",
            "edge_vis_width__max                                 imp=0.0183\n",
            "node_power_w_mean__mean                             imp=0.0167\n",
            "node_cs_mean__sum                                   imp=0.0166\n",
            "edge_vis_width__std                                 imp=0.0150\n",
            "node_power_w_mean__min                              imp=0.0143\n",
            "edge_timing_ms__std                                 imp=0.0120\n",
            "node_gpu_util_mean__mean                            imp=0.0118\n",
            "edge_cs_mean_win__std                               imp=0.0111\n",
            "edge_cs_mean_win__max                               imp=0.0100\n",
            "node_last_ts__std                                   imp=0.0095\n",
            "edge_vis_length__std                                imp=0.0092\n",
            "node_gpu_util_max__min                              imp=0.0085\n",
            "node_gpu_util_mean__sum                             imp=0.0077\n",
            "node_duration_ms__mean                              imp=0.0071\n",
            "node_gpu_util_mean__min                             imp=0.0065\n",
            "node_gpu_util_max__max                              imp=0.0064\n",
            "edge_start_ts__std                                  imp=0.0063\n",
            "node_us_mean__mean                                  imp=0.0055\n",
            "node_power_w_mean__sum                              imp=0.0054\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "def stump_cv_auc(X, y, feature_idx, K=10):\n",
        "    skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "    aucs=[]\n",
        "    for tr, te in skf.split(X, y):\n",
        "        stump = DecisionTreeClassifier(max_depth=1, random_state=42, class_weight=\"balanced\")\n",
        "        stump.fit(X[tr][:, [feature_idx]], y[tr])\n",
        "        p = stump.predict_proba(X[te][:, [feature_idx]])[:,1]\n",
        "        aucs.append(roc_auc_score(y[te], p))\n",
        "    return float(np.mean(aucs)), float(np.std(aucs))\n",
        "\n",
        "stump_hits = []\n",
        "for j in range(X_num_clean.shape[1]):\n",
        "    auc, sd = stump_cv_auc(X_num_clean[row_indices], y, j)\n",
        "    if auc >= 0.99 or auc <= 0.01:\n",
        "        stump_hits.append((j, auc, num_cols_clean[j]))\n",
        "print(\"Near-perfect stump features:\")\n",
        "for j, auc, name in sorted(stump_hits, key=lambda t: abs(t[1]-0.5), reverse=True):\n",
        "    print(f\"AUC={auc:.3f}  {name}\")\n"
      ],
      "metadata": {
        "id": "izl5gGrlE2nv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "4c6e4fc6-172a-4d24-fb80-8e88f724eb7c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Near-perfect stump features:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "name_patterns = [\n",
        "    r\"__count$\", r\"__sum$\", r\"__max$\", r\"__min$\",\n",
        "    r\"^edge_.*__count$\", r\"^node_.*__count$\",\n",
        "    r\"^edge_.*(len|size|degree|num)\", r\"^node_.*(len|size|degree|num)\",\n",
        "]\n",
        "\n",
        "bad_name_idx = set()\n",
        "for j, name in enumerate(num_cols_clean):\n",
        "    if any(re.search(p, name, flags=re.IGNORECASE) for p in name_patterns):\n",
        "        bad_name_idx.add(j)\n",
        "\n",
        "# union with stump-based removals\n",
        "bad_idx_final = sorted({j for j,_,_ in stump_hits} | bad_name_idx)\n",
        "\n",
        "keep_mask3 = np.ones(X_num_clean.shape[1], dtype=bool)\n",
        "keep_mask3[bad_idx_final] = False\n",
        "X_num_clean3 = X_num_clean[:, keep_mask3]\n",
        "num_cols_clean3 = [c for k,c in enumerate(num_cols_clean) if keep_mask3[k]]\n",
        "print(\"Dropped:\", len(bad_idx_final), \"Remaining:\", X_num_clean3.shape[1])\n"
      ],
      "metadata": {
        "id": "DKJo-TywE-mc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "31ed89c1-0334-449b-9d00-24bf76a37692"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropped: 182 Remaining: 88\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_block_suite(X_num_clean3[row_indices], y, \"NUM-ONLY (after stump+size pruning)\")\n"
      ],
      "metadata": {
        "id": "VF7seTINE-qG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1762c661-2206-470a-b720-b4c51a2b5c93"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== NUM-ONLY (after stump+size pruning) (88 feats) ===\n",
            "\n",
            "=== NUM-ONLY (after stump+size pruning) | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.950 0.964      0.896       1.000      0.800     0.889         0.900       0.46      0.909       1.000      0.825     0.904         0.912\n",
            "    2 0.943 0.961      0.883       0.919      0.850     0.883         0.884       0.59      0.896       0.944      0.850     0.895         0.898\n",
            "    3 0.867 0.845      0.805       0.857      0.750     0.800         0.807       0.32      0.805       0.821      0.800     0.810         0.805\n",
            "    4 0.892 0.929      0.896       0.971      0.825     0.892         0.899       0.46      0.896       0.971      0.825     0.892         0.899\n",
            "    5 0.909 0.931      0.844       0.833      0.875     0.854         0.843       0.49      0.844       0.833      0.875     0.854         0.843\n",
            "    6 0.959 0.969      0.870       0.941      0.800     0.865         0.873       0.44      0.922       0.947      0.900     0.923         0.923\n",
            "    7 0.985 0.988      0.922       0.947      0.900     0.923         0.923       0.34      0.961       0.951      0.975     0.963         0.960\n",
            "    8 0.918 0.935      0.870       0.914      0.821     0.865         0.871       0.54      0.883       0.941      0.821     0.877         0.884\n",
            "    9 0.925 0.943      0.883       0.917      0.846     0.880         0.884       0.48      0.883       0.917      0.846     0.880         0.884\n",
            "   10 0.917 0.938      0.831       0.861      0.795     0.827         0.832       0.65      0.870       0.939      0.795     0.861         0.871\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.927    0.034     0.94   0.039            0.87          0.035            0.916           0.053           0.826          0.043          0.868         0.035              0.872           0.887          0.042            0.926           0.057           0.851          0.054          0.886         0.041              0.888\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.2          3.1\n",
            "True Attack          6.9         32.8\n",
            "Accuracy: 0.870 | Precision (attack): 0.914 | Recall (attack): 0.826 | Specificity (normal): 0.917\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.5          2.8\n",
            "True Attack          5.9         33.8\n",
            "Accuracy: 0.887 | Precision (attack): 0.923 | Recall (attack): 0.851 | Specificity (normal): 0.925\n",
            "\n",
            "=== NUM-ONLY (after stump+size pruning) | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.959 0.973      0.922       0.947      0.900     0.923         0.923       0.52      0.935       0.973      0.900     0.935         0.936\n",
            "    2 0.978 0.981      0.909       0.884      0.950     0.916         0.907       0.59      0.922       0.972      0.875     0.921         0.924\n",
            "    3 0.906 0.929      0.792       0.833      0.750     0.789         0.794       0.41      0.844       0.850      0.850     0.850         0.844\n",
            "    4 0.984 0.987      0.922       0.947      0.900     0.923         0.923       0.43      0.935       0.949      0.925     0.937         0.935\n",
            "    5 0.943 0.957      0.870       0.875      0.875     0.875         0.870       0.52      0.896       0.921      0.875     0.897         0.897\n",
            "    6 0.978 0.981      0.896       0.921      0.875     0.897         0.897       0.42      0.935       0.927      0.950     0.938         0.934\n",
            "    7 0.982 0.985      0.896       0.971      0.825     0.892         0.899       0.39      0.948       0.929      0.975     0.951         0.947\n",
            "    8 0.930 0.953      0.896       0.970      0.821     0.889         0.897       0.46      0.896       0.943      0.846     0.892         0.897\n",
            "    9 0.959 0.968      0.896       0.919      0.872     0.895         0.896       0.58      0.909       0.971      0.846     0.904         0.910\n",
            "   10 0.923 0.947      0.870       0.892      0.846     0.868         0.870       0.53      0.883       0.969      0.795     0.873         0.884\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.954    0.028    0.966   0.019           0.887          0.038            0.916           0.045           0.861          0.055          0.887         0.039              0.888            0.91          0.032             0.94           0.038           0.884          0.054           0.91         0.032              0.911\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.1          3.2\n",
            "True Attack          5.5         34.2\n",
            "Accuracy: 0.887 | Precision (attack): 0.914 | Recall (attack): 0.861 | Specificity (normal): 0.914\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         35.0          2.3\n",
            "True Attack          4.6         35.1\n",
            "Accuracy: 0.910 | Precision (attack): 0.939 | Recall (attack): 0.884 | Specificity (normal): 0.938\n",
            "\n",
            "=== NUM-ONLY (after stump+size pruning) | HGB: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.937 0.962      0.857       0.837      0.900     0.867         0.855       0.68      0.922       0.947      0.900     0.923         0.923\n",
            "    2 0.976 0.980      0.896       0.921      0.875     0.897         0.897       0.40      0.909       0.923      0.900     0.911         0.909\n",
            "    3 0.933 0.946      0.831       0.865      0.800     0.831         0.832       0.56      0.844       0.912      0.775     0.838         0.847\n",
            "    4 0.970 0.974      0.896       0.944      0.850     0.895         0.898       0.51      0.909       0.971      0.850     0.907         0.911\n",
            "    5 0.945 0.957      0.831       0.814      0.875     0.843         0.829       0.64      0.870       0.875      0.875     0.875         0.870\n",
            "    6 0.969 0.974      0.883       0.897      0.875     0.886         0.883       0.30      0.896       0.900      0.900     0.900         0.896\n",
            "    7 0.975 0.979      0.909       0.946      0.875     0.909         0.910       0.42      0.922       0.947      0.900     0.923         0.923\n",
            "    8 0.945 0.959      0.883       0.941      0.821     0.877         0.884       0.32      0.896       0.943      0.846     0.892         0.897\n",
            "    9 0.956 0.964      0.870       0.892      0.846     0.868         0.870       0.30      0.909       0.881      0.949     0.914         0.909\n",
            "   10 0.918 0.945      0.896       0.919      0.872     0.895         0.896       0.36      0.896       0.919      0.872     0.895         0.896\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.952     0.02    0.964   0.013           0.875          0.028            0.898           0.046           0.859           0.03          0.877         0.025              0.875           0.897          0.024            0.922           0.031           0.877          0.046          0.898         0.026              0.898\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.3          4.0\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.875 | Precision (attack): 0.895 | Recall (attack): 0.859 | Specificity (normal): 0.893\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.3          3.0\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.897 | Precision (attack): 0.921 | Recall (attack): 0.877 | Specificity (normal): 0.920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "def depth2_cv_auc(X, y, idxs, K=5):\n",
        "    skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "    aucs=[]\n",
        "    for tr, te in skf.split(X, y):\n",
        "        tree2 = DecisionTreeClassifier(max_depth=2, random_state=42, class_weight=\"balanced\")\n",
        "        tree2.fit(X[tr][:, idxs], y[tr])\n",
        "        p = tree2.predict_proba(X[te][:, idxs])[:,1]\n",
        "        aucs.append(roc_auc_score(y[te], p))\n",
        "    return np.mean(aucs)\n",
        "\n",
        "# check top 10 RF features for pairs\n",
        "top10 = np.argsort(rf.feature_importances_)[-10:][::-1]\n",
        "pairs = []\n",
        "for a,b in combinations(top10, 2):\n",
        "    auc = depth2_cv_auc(X_num_clean[row_indices], y, [a,b])\n",
        "    if auc >= 0.995:\n",
        "        pairs.append((auc, num_cols_clean[a], num_cols_clean[b]))\n",
        "print(\"Depth-2 near-perfect pairs:\")\n",
        "for auc, a, b in sorted(pairs, reverse=True)[:10]:\n",
        "    print(f\"AUC={auc:.3f}   {a}  &  {b}\")\n"
      ],
      "metadata": {
        "id": "eG5TG2wbFCi3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f2abdc83-4412-4172-b89c-f417ebd7b500"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Depth-2 near-perfect pairs:\n",
            "AUC=1.000   node_last_ts__sum  &  node_gpu_util_mean__max\n",
            "AUC=1.000   node_last_ts__sum  &  node_cs_mean__min\n",
            "AUC=1.000   node_last_ts__sum  &  node_cs_mean__mean\n",
            "AUC=1.000   node_last_ts__sum  &  node_cs_mean__max\n",
            "AUC=1.000   node_last_ts__sum  &  edge_vis_length__max\n",
            "AUC=1.000   node_last_ts__sum  &  edge_timing_ms__max\n",
            "AUC=1.000   node_last_ts__sum  &  edge_end_ts__sum\n",
            "AUC=1.000   node_first_ts__sum  &  node_last_ts__sum\n",
            "AUC=1.000   node_first_ts__sum  &  node_gpu_util_mean__max\n",
            "AUC=1.000   node_first_ts__sum  &  node_cs_mean__min\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# summarise any *_count features by class\n",
        "df_counts = pd.DataFrame({\n",
        "    \"feature\": num_cols_clean,\n",
        "    \"is_count\": [name.endswith(\"__count\") for name in num_cols_clean],\n",
        "})\n",
        "count_cols = [num_cols_clean[i] for i,flag in enumerate(df_counts[\"is_count\"]) if flag]\n",
        "if count_cols:\n",
        "    Xc = X_num_clean[row_indices][:, [num_cols_clean.index(c) for c in count_cols]]\n",
        "    means0 = np.asarray(Xc[y==0].mean(axis=0)).ravel()\n",
        "    means1 = np.asarray(Xc[y==1].mean(axis=0)).ravel()\n",
        "    big_gap = np.argsort(np.abs(means1-means0))[::-1][:20]\n",
        "    for k in big_gap:\n",
        "        print(f\"{count_cols[k]:50s}  mean0={means0[k]:.2f}  mean1={means1[k]:.2f}\")\n"
      ],
      "metadata": {
        "id": "iNgwIX4OFCmK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "5d76d027-8a2c-4cc3-edfd-c2a526ff3e34"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "edge_vis_length__count                              mean0=9.11  mean1=9.07\n",
            "edge_vis_width__count                               mean0=9.11  mean1=9.07\n",
            "edge_completion_tokens__count                       mean0=0.12  mean1=0.12\n",
            "edge_prompt_tokens__count                           mean0=0.12  mean1=0.12\n",
            "edge_gpu_mem_max_win__count                         mean0=0.12  mean1=0.12\n",
            "edge_start_ts__count                                mean0=0.12  mean1=0.12\n",
            "edge_end_ts__count                                  mean0=0.12  mean1=0.12\n",
            "edge_open_ps_win__count                             mean0=0.12  mean1=0.12\n",
            "edge_write_ps_win__count                            mean0=0.12  mean1=0.12\n",
            "edge_read_ps_win__count                             mean0=0.12  mean1=0.12\n",
            "edge_syscalls_win__count                            mean0=0.12  mean1=0.12\n",
            "edge_load1_win__count                               mean0=0.12  mean1=0.12\n",
            "edge_cs_mean_win__count                             mean0=0.12  mean1=0.12\n",
            "edge_gpu_power_mean_win__count                      mean0=0.12  mean1=0.12\n",
            "edge_gpu_util_mean_win__count                       mean0=0.12  mean1=0.12\n",
            "edge_pkts_win__count                                mean0=0.12  mean1=0.12\n",
            "edge_us_mean_win__count                             mean0=0.12  mean1=0.12\n",
            "edge_bytes_win__count                               mean0=0.12  mean1=0.12\n",
            "edge_pps_win__count                                 mean0=0.12  mean1=0.12\n",
            "edge_dns_count_win__count                           mean0=0.12  mean1=0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now i want that three category: structural, only attribute, combine"
      ],
      "metadata": {
        "id": "f0comwv4FCpb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Final leak-safety: drop absolute time / render families on top of stump+size pruning\n",
        "import re\n",
        "drop_time_render = [r\"_ts\", r\"timestamp\", r\"duration_ms\", r\"^edge_vis_\", r\"^node_vis_\"]\n",
        "bad_idx_time = {\n",
        "    i for i, name in enumerate(num_cols_clean3)\n",
        "    if any(re.search(p, name, flags=re.IGNORECASE) for p in drop_time_render)\n",
        "}\n",
        "\n",
        "keep_mask4 = np.ones(len(num_cols_clean3), dtype=bool)\n",
        "keep_mask4[list(bad_idx_time)] = False\n",
        "\n",
        "X_num_final = X_num_clean3[:, keep_mask4]\n",
        "num_cols_final = [c for k,c in enumerate(num_cols_clean3) if keep_mask4[k]]\n",
        "print(\"Final drops (time/render):\", len(bad_idx_time), \"| Final numeric feats:\", X_num_final.shape[1])\n",
        "\n",
        "# 2) Text series (same mapping you used)\n",
        "texts_series = pd.Series(merged[\"file\"]).map(df_num.set_index(\"file\")[\"edge_text_doc\"]).fillna(\"\")\n",
        "\n",
        "# 3) Run the three suites (K=10 inside the function; TF-IDF rebuilt per fold)\n",
        "#    Make sure X_struct_scaled is defined; otherwise set it to None for STRUCTURE-ONLY to skip.\n",
        "#    If you have a structure matrix named differently, plug it below.\n",
        "STRUCT = X_struct_scaled  # or None if not available\n",
        "\n",
        "# STRUCTURE-ONLY\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_final[row_indices],         # still required by signature; not used in STRUCTURE-ONLY branch\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series  = texts_series,\n",
        "    X_struct_blk  = STRUCT,                           # <-- used for STRUCTURE-ONLY\n",
        "    y             = y,\n",
        "    tag           = \"STRUCTURE-ONLY\"\n",
        ")\n",
        "\n",
        "# ATTRS-ONLY  (numeric + node/edge cat + TF-IDF)\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_final[row_indices],         # <-- pruned numeric\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series  = texts_series,                     # <-- TF-IDF refit inside CV\n",
        "    X_struct_blk  = None,                             # no structure here\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY\"\n",
        ")\n",
        "\n",
        "# COMBINED (structure + attrs + TF-IDF)\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_final[row_indices],\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series  = texts_series,\n",
        "    X_struct_blk  = STRUCT,\n",
        "    y             = y,\n",
        "    tag           = \"COMBINED\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "GOzYtAoqsPUd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f16c52f2-a561-4ba8-9f16-e016d0b6e8b6"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final drops (time/render): 8 | Final numeric feats: 80\n",
            "\n",
            "\n",
            "====================  STRUCTURE-ONLY (TFIDF re-fit per fold)  ====================\n",
            "\n",
            "=== STRUCTURE-ONLY | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.943 0.960      0.870       1.000      0.750     0.857         0.875       0.41      0.909       0.971      0.850     0.907         0.911\n",
            "    2 0.866 0.919      0.831       1.000      0.675     0.806         0.838       0.40      0.857       0.914      0.800     0.853         0.859\n",
            "    3 0.793 0.858      0.753       0.957      0.550     0.698         0.761       0.30      0.714       0.696      0.800     0.744         0.711\n",
            "    4 0.855 0.900      0.818       1.000      0.650     0.788         0.825       0.45      0.818       1.000      0.650     0.788         0.825\n",
            "    5 0.838 0.885      0.792       0.875      0.700     0.778         0.796       0.52      0.818       0.933      0.700     0.800         0.823\n",
            "    6 0.854 0.902      0.805       0.879      0.725     0.795         0.808       0.42      0.844       0.889      0.800     0.842         0.846\n",
            "    7 0.840 0.901      0.831       0.909      0.750     0.822         0.834       0.50      0.831       0.909      0.750     0.822         0.834\n",
            "    8 0.843 0.878      0.779       0.923      0.615     0.738         0.781       0.36      0.753       0.778      0.718     0.747         0.754\n",
            "    9 0.765 0.850      0.805       0.962      0.641     0.769         0.807       0.47      0.805       0.962      0.641     0.769         0.807\n",
            "   10 0.888 0.922      0.857       1.000      0.718     0.836         0.859       0.47      0.870       0.968      0.769     0.857         0.871\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.848    0.048    0.898   0.032           0.814          0.035            0.951           0.051           0.677          0.064          0.789         0.047              0.818           0.822          0.056            0.902           0.095           0.748          0.069          0.813         0.053              0.824\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         35.8          1.5\n",
            "True Attack         12.8         26.9\n",
            "Accuracy: 0.814 | Precision (attack): 0.947 | Recall (attack): 0.678 | Specificity (normal): 0.960\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.6          3.7\n",
            "True Attack         10.0         29.7\n",
            "Accuracy: 0.822 | Precision (attack): 0.889 | Recall (attack): 0.748 | Specificity (normal): 0.901\n",
            "\n",
            "=== STRUCTURE-ONLY | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.887 0.926      0.818       0.842      0.800     0.821         0.819       0.52      0.844       0.889      0.800     0.842         0.846\n",
            "    2 0.852 0.900      0.805       0.857      0.750     0.800         0.807       0.49      0.805       0.857      0.750     0.800         0.807\n",
            "    3 0.788 0.851      0.714       0.765      0.650     0.703         0.717       0.51      0.727       0.788      0.650     0.712         0.730\n",
            "    4 0.807 0.879      0.779       0.871      0.675     0.761         0.783       0.58      0.818       0.964      0.675     0.794         0.824\n",
            "    5 0.846 0.881      0.727       0.771      0.675     0.720         0.729       0.34      0.792       0.773      0.850     0.810         0.790\n",
            "    6 0.845 0.866      0.753       0.769      0.750     0.759         0.753       0.67      0.792       0.853      0.725     0.784         0.795\n",
            "    7 0.893 0.924      0.831       0.865      0.800     0.831         0.832       0.44      0.844       0.850      0.850     0.850         0.844\n",
            "    8 0.793 0.862      0.766       0.839      0.667     0.743         0.768       0.63      0.792       0.897      0.667     0.765         0.794\n",
            "    9 0.795 0.860      0.766       0.839      0.667     0.743         0.768       0.55      0.792       0.897      0.667     0.765         0.794\n",
            "   10 0.867 0.917      0.818       0.821      0.821     0.821         0.818       0.56      0.844       0.865      0.821     0.842         0.844\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.837    0.039    0.887   0.028           0.778           0.04            0.824           0.041           0.726          0.066           0.77         0.045              0.779           0.805          0.036            0.863           0.055           0.745           0.08          0.796         0.043              0.807\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.1          6.2\n",
            "True Attack         10.9         28.8\n",
            "Accuracy: 0.778 | Precision (attack): 0.823 | Recall (attack): 0.725 | Specificity (normal): 0.834\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.4          4.9\n",
            "True Attack         10.1         29.6\n",
            "Accuracy: 0.805 | Precision (attack): 0.858 | Recall (attack): 0.746 | Specificity (normal): 0.869\n",
            "\n",
            "=== STRUCTURE-ONLY | HGB: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 0.872 0.918      0.831       0.909      0.750     0.822         0.834       0.43      0.844       0.889      0.800     0.842         0.846\n",
            "    2 0.838 0.892      0.766       0.806      0.725     0.763         0.768       0.54      0.779       0.848      0.700     0.767         0.782\n",
            "    3 0.780 0.850      0.701       0.743      0.650     0.693         0.703       0.57      0.740       0.833      0.625     0.714         0.745\n",
            "    4 0.861 0.892      0.753       0.862      0.625     0.725         0.758       0.67      0.779       0.926      0.625     0.746         0.785\n",
            "    5 0.831 0.875      0.753       0.800      0.700     0.747         0.755       0.40      0.779       0.795      0.775     0.785         0.779\n",
            "    6 0.851 0.887      0.805       0.857      0.750     0.800         0.807       0.31      0.779       0.756      0.850     0.800         0.776\n",
            "    7 0.889 0.925      0.818       0.842      0.800     0.821         0.819       0.30      0.831       0.814      0.875     0.843         0.829\n",
            "    8 0.805 0.864      0.779       0.844      0.692     0.761         0.780       0.44      0.792       0.848      0.718     0.778         0.793\n",
            "    9 0.775 0.849      0.753       0.812      0.667     0.732         0.754       0.70      0.792       0.926      0.641     0.758         0.794\n",
            "   10 0.843 0.899      0.818       0.857      0.769     0.811         0.819       0.48      0.818       0.857      0.769     0.811         0.819\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.834    0.038    0.885   0.026           0.778           0.04            0.833           0.045           0.713          0.056          0.768         0.045               0.78           0.793           0.03            0.849           0.054           0.738          0.091          0.784         0.041              0.795\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.6          5.7\n",
            "True Attack         11.4         28.3\n",
            "Accuracy: 0.778 | Precision (attack): 0.832 | Recall (attack): 0.713 | Specificity (normal): 0.847\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.8          5.5\n",
            "True Attack         10.4         29.3\n",
            "Accuracy: 0.794 | Precision (attack): 0.842 | Recall (attack): 0.738 | Specificity (normal): 0.853\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (TFIDF re-fit per fold)  ====================\n",
            "\n",
            "=== ATTRS-ONLY | LogReg: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    2  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    3  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    4  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    5  1.0 1.0      0.987       0.976        1.0     0.988         0.986       0.30      0.987       0.976        1.0     0.988         0.986\n",
            "    6  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.38      1.000       1.000        1.0     1.000         1.000\n",
            "    7  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    8  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.32      1.000       1.000        1.0     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.999          0.004            0.998           0.008             1.0            0.0          0.999         0.004              0.999           0.999          0.004            0.998           0.008             1.0            0.0          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.999 | Precision (attack): 0.997 | Recall (attack): 1.000 | Specificity (normal): 0.997\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.999 | Precision (attack): 0.997 | Recall (attack): 1.000 | Specificity (normal): 0.997\n",
            "\n",
            "=== ATTRS-ONLY | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    2 1.000 1.000      0.987       1.000      0.975     0.987         0.988       0.33      1.000       1.000        1.0     1.000         1.000\n",
            "    3 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.47      1.000       1.000        1.0     1.000         1.000\n",
            "    4 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    5 1.000 1.000      0.974       0.952      1.000     0.976         0.973       0.58      1.000       1.000        1.0     1.000         1.000\n",
            "    6 1.000 1.000      0.961       0.930      1.000     0.964         0.959       0.58      1.000       1.000        1.0     1.000         1.000\n",
            "    7 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.48      1.000       1.000        1.0     1.000         1.000\n",
            "    8 0.999 0.999      0.974       0.974      0.974     0.974         0.974       0.49      0.987       0.975        1.0     0.987         0.987\n",
            "    9 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.43      1.000       1.000        1.0     1.000         1.000\n",
            "   10 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.46      1.000       1.000        1.0     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0            0.99          0.015            0.986           0.025           0.995          0.011           0.99         0.014              0.989           0.999          0.004            0.997           0.008             1.0            0.0          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.7          0.6\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.990 | Precision (attack): 0.985 | Recall (attack): 0.995 | Specificity (normal): 0.984\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.999 | Precision (attack): 0.997 | Recall (attack): 1.000 | Specificity (normal): 0.997\n",
            "\n",
            "=== ATTRS-ONLY | HGB: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    2  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    3  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    4  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    5  1.0 1.0      0.974       0.952        1.0     0.976         0.973       0.63      0.987       0.976        1.0     0.988         0.986\n",
            "    6  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    7  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    8  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000        1.0     1.000         1.000       0.30      1.000       1.000        1.0     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.997          0.008            0.995           0.015             1.0            0.0          0.998         0.008              0.997           0.999          0.004            0.998           0.008             1.0            0.0          0.999         0.004              0.999\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.997 | Precision (attack): 0.995 | Recall (attack): 1.000 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.999 | Precision (attack): 0.997 | Recall (attack): 1.000 | Specificity (normal): 0.997\n",
            "\n",
            "\n",
            "====================  COMBINED (TFIDF re-fit per fold)  ====================\n",
            "\n",
            "=== COMBINED | LogReg: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    2 0.993 0.995      0.987       1.000      0.975     0.987         0.988       0.30      0.987       1.000      0.975     0.987         0.988\n",
            "    3 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    4 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    5 1.000 1.000      0.987       0.976      1.000     0.988         0.986       0.30      0.987       0.976      1.000     0.988         0.986\n",
            "    6 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.41      1.000       1.000      1.000     1.000         1.000\n",
            "    7 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    8 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.32      1.000       1.000      1.000     1.000         1.000\n",
            "    9 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.36      1.000       1.000      1.000     1.000         1.000\n",
            "   10 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.999    0.002      1.0   0.002           0.997          0.005            0.998           0.008           0.997          0.008          0.997         0.005              0.997           0.997          0.005            0.998           0.008           0.997          0.008          0.997         0.005              0.997\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision (attack): 0.997 | Recall (attack): 0.997 | Specificity (normal): 0.997\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision (attack): 0.997 | Recall (attack): 0.997 | Specificity (normal): 0.997\n",
            "\n",
            "=== COMBINED | RF: Per-fold metrics (K=10) ===\n",
            " fold   AUC    AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.31      1.000       1.000      1.000     1.000         1.000\n",
            "    2 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.36      1.000       1.000      1.000     1.000         1.000\n",
            "    3 0.999 0.999      0.961       0.951      0.975     0.963         0.960       0.54      0.987       1.000      0.975     0.987         0.988\n",
            "    4 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.30      1.000       1.000      1.000     1.000         1.000\n",
            "    5 1.000 1.000      0.974       0.952      1.000     0.976         0.973       0.59      1.000       1.000      1.000     1.000         1.000\n",
            "    6 0.998 0.998      0.948       0.909      1.000     0.952         0.946       0.57      0.987       0.976      1.000     0.988         0.986\n",
            "    7 1.000 1.000      0.987       0.976      1.000     0.988         0.986       0.57      1.000       1.000      1.000     1.000         1.000\n",
            "    8 0.996 0.996      0.948       0.949      0.949     0.949         0.948       0.30      0.974       0.951      1.000     0.975         0.974\n",
            "    9 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.50      1.000       1.000      1.000     1.000         1.000\n",
            "   10 1.000 1.000      1.000       1.000      1.000     1.000         1.000       0.43      1.000       1.000      1.000     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "    0.999    0.001    0.999   0.001           0.982          0.022            0.974           0.032           0.992          0.017          0.983         0.021              0.981           0.995          0.009            0.993           0.016           0.997          0.008          0.995         0.009              0.995\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.2          1.1\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.982 | Precision (attack): 0.973 | Recall (attack): 0.992 | Specificity (normal): 0.971\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.0          0.3\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.995 | Precision (attack): 0.992 | Recall (attack): 0.997 | Specificity (normal): 0.992\n",
            "\n",
            "=== COMBINED | HGB: Per-fold metrics (K=10) ===\n",
            " fold  AUC  AP  Acc_fixed  Prec_fixed  Rec_fixed  F1_fixed  BalAcc_fixed  thr_tuned  Acc_tuned  Prec_tuned  Rec_tuned  F1_tuned  BalAcc_tuned\n",
            "    1  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    2  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    3  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    4  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    5  1.0 1.0      0.974       0.952        1.0     0.976         0.973        0.3      0.974       0.952        1.0     0.976         0.973\n",
            "    6  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    7  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    8  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "    9  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "   10  1.0 1.0      1.000       1.000        1.0     1.000         1.000        0.3      1.000       1.000        1.0     1.000         1.000\n",
            "\n",
            "=== Mean ± Std across folds ===\n",
            " AUC_mean  AUC_std  AP_mean  AP_std  Acc_fixed_mean  Acc_fixed_std  Prec_fixed_mean  Prec_fixed_std  Rec_fixed_mean  Rec_fixed_std  F1_fixed_mean  F1_fixed_std  BalAcc_fixed_mean  Acc_tuned_mean  Acc_tuned_std  Prec_tuned_mean  Prec_tuned_std  Rec_tuned_mean  Rec_tuned_std  F1_tuned_mean  F1_tuned_std  BalAcc_tuned_mean\n",
            "      1.0      0.0      1.0     0.0           0.997          0.008            0.995           0.015             1.0            0.0          0.998         0.008              0.997           0.997          0.008            0.995           0.015             1.0            0.0          0.998         0.008              0.997\n",
            "\n",
            "=== Mean Confusion Matrix (threshold = 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.997 | Precision (attack): 0.995 | Recall (attack): 1.000 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean Confusion Matrix (F1-tuned threshold) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.997 | Precision (attack): 0.995 | Recall (attack): 1.000 | Specificity (normal): 0.995\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Quick ablations (find the culprit)"
      ],
      "metadata": {
        "id": "CBY-KRjPsPW1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Dict, Tuple\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import issparse, csr_matrix, hstack, vstack\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, confusion_matrix, precision_recall_curve\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "def _safe_slice(M, idx):\n",
        "    if M is None:\n",
        "        return None\n",
        "    if issparse(M):\n",
        "        return M[idx]\n",
        "    # pandas Series/DataFrame or ndarray\n",
        "    return M.iloc[idx] if hasattr(M, \"iloc\") else M[idx]\n",
        "\n",
        "def _texts_to_tfidf(train_texts: List[str], test_texts: List[str], max_features: int) -> Tuple[csr_matrix, csr_matrix]:\n",
        "    \"\"\"Robust TF-IDF that never crashes on empty vocab.\"\"\"\n",
        "    train_has_any = any((t or \"\").strip() for t in train_texts)\n",
        "    if not train_has_any:\n",
        "        Xtr = csr_matrix((len(train_texts), 0))\n",
        "        Xte = csr_matrix((len(test_texts), 0))\n",
        "        return Xtr, Xte\n",
        "    try:\n",
        "        # word-level first, no stopword removal (avoids empty vocab on short texts)\n",
        "        tfv = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words=None,\n",
        "            lowercase=True,\n",
        "            token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        Xtr = tfv.fit_transform(train_texts)\n",
        "        Xte = tfv.transform(test_texts)\n",
        "        return Xtr, Xte\n",
        "    except ValueError:\n",
        "        # Fall back to char n-grams (extremely robust)\n",
        "        tfv = TfidfVectorizer(\n",
        "            analyzer=\"char\",\n",
        "            ngram_range=(3,5),\n",
        "            max_features=max_features,\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        Xtr = tfv.fit_transform(train_texts)\n",
        "        Xte = tfv.transform(test_texts)\n",
        "        return Xtr, Xte\n",
        "\n",
        "def _bags_to_sparse(\n",
        "    train_bags: List[Dict[str, float]],\n",
        "    test_bags:  List[Dict[str, float]],\n",
        "    topk: int\n",
        ") -> Tuple[csr_matrix, csr_matrix, List[str]]:\n",
        "    \"\"\"\n",
        "    Build a per-fold top-K vocab from training bags, produce aligned sparse matrices for train/test.\n",
        "    Unknown keys in test are dropped (ignored).\n",
        "    \"\"\"\n",
        "    # Build vocab on train\n",
        "    ctr = Counter()\n",
        "    for d in train_bags:\n",
        "        if not d:\n",
        "            continue\n",
        "        ctr.update(d)\n",
        "    vocab = [k for k,_ in ctr.most_common(topk)]\n",
        "    index = {k:i for i,k in enumerate(vocab)}\n",
        "    # Encode helper\n",
        "    def _encode(bags):\n",
        "        rows = []\n",
        "        for d in bags:\n",
        "            if not d:\n",
        "                rows.append(csr_matrix((1, len(vocab))))\n",
        "                continue\n",
        "            idx = []\n",
        "            val = []\n",
        "            for k,v in d.items():\n",
        "                j = index.get(k)\n",
        "                if j is not None:\n",
        "                    idx.append(j); val.append(float(v))\n",
        "            if idx:\n",
        "                rows.append(csr_matrix((val, ([0]*len(idx), idx)), shape=(1, len(vocab))))\n",
        "            else:\n",
        "                rows.append(csr_matrix((1, len(vocab))))\n",
        "        return vstack(rows).tocsr()\n",
        "    return _encode(train_bags), _encode(test_bags), vocab\n",
        "\n",
        "def _recall_tuned_preds(y_true, scores, target_recall: float = 0.85):\n",
        "    \"\"\"Pick the highest-precision threshold that achieves recall >= target_recall (if possible).\"\"\"\n",
        "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
        "    # precision_recall_curve returns len(thr) = len(prec)-1 = len(rec)-1\n",
        "    # Align thresholds with points 1..end\n",
        "    best_t = 0.5\n",
        "    chosen = None\n",
        "    for p, r, t in zip(prec[1:], rec[1:], thr):\n",
        "        if r >= target_recall:\n",
        "            if chosen is None or p > chosen[0]:\n",
        "                chosen = (p, r, t)\n",
        "    if chosen is None:\n",
        "        # Cannot hit target recall; pick threshold that maximizes recall (ties → higher precision)\n",
        "        best_idx = int(np.argmax(rec[1:]))\n",
        "        best_t = float(thr[best_idx])\n",
        "    else:\n",
        "        best_t = float(chosen[2])\n",
        "    return (scores >= best_t).astype(int), best_t\n",
        "\n",
        "def eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk,                       # precomputed numeric attr block (csr or DF/ndarray)\n",
        "    X_nodecat_blk, X_edgecat_blk,    # OPTIONAL precomputed cat matrices (used only if bags are None)\n",
        "    texts_series,                    # pd.Series of raw texts per graph\n",
        "    X_struct_blk,                    # structural block (csr or DF/ndarray) or None\n",
        "    y,                               # np.array labels (0=normal, 1=attack)\n",
        "    tag,\n",
        "    # NEW: pass per-row cat bags to rebuild per-fold vocab leakage-free\n",
        "    node_cat_bags: Optional[List[Dict[str, float]]] = None,\n",
        "    edge_cat_bags: Optional[List[Dict[str, float]]] = None,\n",
        "    topk_node: int = 250,\n",
        "    topk_edge: int = 250,\n",
        "    EDGE_TEXT_MAX_FEATURES: int = 1000,\n",
        "    # class weighting / recall target\n",
        "    class_weight: Dict[int, float] = {0:1.0, 1:2.0},\n",
        "    target_recall: float = 0.85\n",
        "):\n",
        "    print(f\"\\n\\n====================  {tag} (TFIDF & CATS rebuilt per fold)  ====================\")\n",
        "    K = 10\n",
        "    skf = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "\n",
        "    # For sample weights (works for all models)\n",
        "    def _sample_weights(y_fold):\n",
        "        w = np.ones_like(y_fold, dtype=float)\n",
        "        w[y_fold==0] = class_weight.get(0, 1.0)\n",
        "        w[y_fold==1] = class_weight.get(1, 1.0)\n",
        "        return w\n",
        "\n",
        "    # Warn if we can’t rebuild cats\n",
        "    rebuild_cats = (node_cat_bags is not None) and (edge_cat_bags is not None)\n",
        "    if not rebuild_cats:\n",
        "        print(\"[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage).\")\n",
        "\n",
        "    def _run_one(clf, name):\n",
        "        per_fold_rows = []\n",
        "        cm_fixed_all = []\n",
        "        cm_f1_all = []\n",
        "        cm_rec_all = []\n",
        "\n",
        "        for fold, (tr, te) in enumerate(skf.split(_safe_slice(X_num_blk, np.arange(len(y))), y), 1):\n",
        "            # 1) text → TF-IDF (train-only fit)\n",
        "            X_text_tr, X_text_te = _texts_to_tfidf(\n",
        "                texts_series.iloc[tr].astype(str).tolist(),\n",
        "                texts_series.iloc[te].astype(str).tolist(),\n",
        "                EDGE_TEXT_MAX_FEATURES\n",
        "            )\n",
        "\n",
        "            # 2) categorical → rebuild per fold if bags provided; else slice existing matrices\n",
        "            if rebuild_cats:\n",
        "                node_tr, node_te, _ = _bags_to_sparse([node_cat_bags[i] for i in tr],\n",
        "                                                      [node_cat_bags[i] for i in te],\n",
        "                                                      topk_node)\n",
        "                edge_tr, edge_te, _ = _bags_to_sparse([edge_cat_bags[i] for i in tr],\n",
        "                                                      [edge_cat_bags[i] for i in te],\n",
        "                                                      topk_edge)\n",
        "            else:\n",
        "                node_tr = _safe_slice(X_nodecat_blk, tr); node_te = _safe_slice(X_nodecat_blk, te)\n",
        "                edge_tr = _safe_slice(X_edgecat_blk, tr); edge_te = _safe_slice(X_edgecat_blk, te)\n",
        "\n",
        "            # 3) numeric / structural slices\n",
        "            Xn_tr, Xn_te = _safe_slice(X_num_blk, tr), _safe_slice(X_num_blk, te)\n",
        "            Xs_tr = _safe_slice(X_struct_blk, tr) if X_struct_blk is not None else None\n",
        "            Xs_te = _safe_slice(X_struct_blk, te) if X_struct_blk is not None else None\n",
        "\n",
        "            # 4) assemble attribute and combined blocks\n",
        "            Xattr_tr = hstack([Xn_tr, node_tr, edge_tr, X_text_tr]).tocsr()\n",
        "            Xattr_te = hstack([Xn_te, node_te, edge_te, X_text_te]).tocsr()\n",
        "\n",
        "            if \"ATTRS-ONLY\" in tag:\n",
        "                Xtr, Xte = Xattr_tr, Xattr_te\n",
        "            elif \"STRUCTURE-ONLY\" in tag:\n",
        "                Xtr, Xte = Xs_tr, Xs_te\n",
        "            else:  # COMBINED (default)\n",
        "                Xtr = hstack([Xs_tr, Xattr_tr]).tocsr() if Xs_tr is not None else Xattr_tr\n",
        "                Xte = hstack([Xs_te, Xattr_te]).tocsr() if Xs_te is not None else Xattr_te\n",
        "\n",
        "            ytr, yte = y[tr], y[te]\n",
        "            sw_tr = _sample_weights(ytr)\n",
        "\n",
        "            # 5) model pipeline:\n",
        "            #    - VarianceThreshold(0.0) (works with sparse)\n",
        "            #    - SimpleImputer(strategy=\"median\") (may densify later)\n",
        "            #    - StandardScaler(with_mean=False)\n",
        "            #    - Optional densify for models that need it\n",
        "            need_dense = (\n",
        "                (isinstance(clf, LogisticRegression) and getattr(clf, \"solver\", \"\") in {\"lbfgs\", \"liblinear\", \"newton-cg\"})\n",
        "                or isinstance(clf, RandomForestClassifier)\n",
        "                or isinstance(clf, HistGradientBoostingClassifier)\n",
        "            )\n",
        "            steps = [\n",
        "                (\"vt\", VarianceThreshold(0.0)),\n",
        "                (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
        "                (\"sc\", StandardScaler(with_mean=False)),\n",
        "            ]\n",
        "            if need_dense:\n",
        "                steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "            steps.append((\"clf\", clf))\n",
        "            pipe = Pipeline(steps)\n",
        "\n",
        "            # 6) fit with class/sample weights\n",
        "            fit_kwargs = {}\n",
        "            # Prefer sample_weight (covers HGB and RF); for LogReg with class_weight we can still pass sample_weight\n",
        "            fit_kwargs[\"clf__sample_weight\"] = sw_tr\n",
        "            pipe.fit(Xtr, ytr, **fit_kwargs)\n",
        "\n",
        "            # 7) scores\n",
        "            if hasattr(pipe[-1], \"predict_proba\"):\n",
        "                p = pipe.predict_proba(Xte)[:, 1]\n",
        "            else:\n",
        "                try:\n",
        "                    s = pipe.decision_function(Xte)\n",
        "                    p = (s - s.min()) / (s.max() - s.min() + 1e-9)\n",
        "                except Exception:\n",
        "                    p = pipe.predict_proba(Xte)[:, 1]\n",
        "\n",
        "            # --- fixed 0.50 ---\n",
        "            y05 = (p >= 0.50).astype(int)\n",
        "            cm05 = confusion_matrix(yte, y05, labels=[0,1])\n",
        "            cm_fixed_all.append(cm05)\n",
        "\n",
        "            # --- F1 tuned (on test curve; optimistic but OK for comparison) ---\n",
        "            thr_grid = np.linspace(0.30, 0.70, 41)\n",
        "            f1s = [f1_score(yte, (p >= t).astype(int), zero_division=0) for t in thr_grid]\n",
        "            t_f1 = float(thr_grid[int(np.argmax(f1s))])\n",
        "            yt = (p >= t_f1).astype(int)\n",
        "            cmt = confusion_matrix(yte, yt, labels=[0,1])\n",
        "            cm_f1_all.append(cmt)\n",
        "\n",
        "            # --- Recall tuned (target_recall) ---\n",
        "            yr, t_rec = _recall_tuned_preds(yte, p, target_recall=target_recall)\n",
        "            cmr = confusion_matrix(yte, yr, labels=[0,1])\n",
        "            cm_rec_all.append(cmr)\n",
        "\n",
        "            # record metrics\n",
        "            def _row(tag_name, yhat, thr):\n",
        "                return dict(\n",
        "                    fold=fold, which=tag_name,\n",
        "                    thr=thr,\n",
        "                    AUC=roc_auc_score(yte, p),\n",
        "                    AP=average_precision_score(yte, p),\n",
        "                    Acc=accuracy_score(yte, yhat),\n",
        "                    Prec=precision_score(yte, yhat, zero_division=0),\n",
        "                    Rec=recall_score(yte, yhat, zero_division=0),\n",
        "                    F1=f1_score(yte, yhat, zero_division=0),\n",
        "                    BalAcc=balanced_accuracy_score(yte, yhat)\n",
        "                )\n",
        "            per_fold_rows.append(_row(\"fixed0.50\", y05, 0.50))\n",
        "            per_fold_rows.append(_row(\"F1-tuned\", yt, t_f1))\n",
        "            per_fold_rows.append(_row(f\"Recall≥{target_recall:.2f}\", yr, t_rec))\n",
        "\n",
        "        # ======= summary printouts =======\n",
        "        df = pd.DataFrame(per_fold_rows).round(3)\n",
        "        print(f\"\\n=== {tag} | {name}: Per-fold metrics (K={K}) ===\")\n",
        "        print(df.pivot_table(index=\"fold\", columns=\"which\", values=[\"AUC\",\"AP\",\"Acc\",\"Prec\",\"Rec\",\"F1\",\"BalAcc\",\"thr\"])\n",
        "                .round(3).to_string())\n",
        "\n",
        "        def _avg_cm(cms):\n",
        "            return (np.mean(np.stack(cms, axis=0), axis=0))\n",
        "\n",
        "        def _print_cm(cm, title):\n",
        "            tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "            acc = (tp+tn)/np.sum(cm)\n",
        "            prec = tp/(tp+fp+1e-9)\n",
        "            rec  = tp/(tp+fn+1e-9)\n",
        "            spec = tn/(tn+fp+1e-9)\n",
        "            print(f\"\\n=== {title} ===\")\n",
        "            print(\"rows = true [normal, attack]; cols = predicted [normal, attack]\")\n",
        "            print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "            print(f\"Accuracy: {acc:.3f} | Precision (attack): {prec:.3f} | Recall (attack): {rec:.3f} | Specificity (normal): {spec:.3f}\")\n",
        "\n",
        "        _print_cm(_avg_cm(cm_fixed_all), \"Mean CM (fixed 0.50)\")\n",
        "        _print_cm(_avg_cm(cm_f1_all),    \"Mean CM (F1-tuned)\")\n",
        "        _print_cm(_avg_cm(cm_rec_all),   f\"Mean CM (Recall≥{target_recall:.2f})\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    # ——— Models (cost-sensitive) ———\n",
        "    # Stronger weight on the positive class. We still pass sample_weight above.\n",
        "    logreg = LogisticRegression(max_iter=3000, solver=\"lbfgs\", class_weight=class_weight, random_state=42)\n",
        "    rf     = RandomForestClassifier(n_estimators=500, random_state=42, class_weight=class_weight)\n",
        "    hgb    = HistGradientBoostingClassifier(random_state=42)  # will use sample_weight\n",
        "\n",
        "    _ = _run_one(logreg, \"LogReg\")\n",
        "    _ = _run_one(rf,     \"RF\")\n",
        "    _ = _run_one(hgb,    \"HGB\")\n"
      ],
      "metadata": {
        "id": "RVXbqDZ7sPZs"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NUM-ONLY\n",
        "from scipy.sparse import csr_matrix\n",
        "EMPTY = csr_matrix((len(y), 0))\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_final[row_indices],\n",
        "    X_nodecat_blk = EMPTY,\n",
        "    X_edgecat_blk = EMPTY,\n",
        "    texts_series  = pd.Series([\"\"]*len(y)),    # no text\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (NUM-ONLY)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85\n",
        ")\n",
        "\n",
        "# TEXT-ONLY\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = EMPTY,\n",
        "    X_nodecat_blk = EMPTY,\n",
        "    X_edgecat_blk = EMPTY,\n",
        "    texts_series  = texts_series,\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (TEXT-ONLY)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85\n",
        ")\n",
        "\n",
        "# CAT-ONLY (warning: leakage if you don’t pass bags)\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = EMPTY,\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series  = pd.Series([\"\"]*len(y)),\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (CAT-ONLY precomputed)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85\n",
        ")\n"
      ],
      "metadata": {
        "id": "QNWi2zzfsPb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f8993d8a-8c66-4b08-dad6-40aca03afb3f"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NUM-ONLY) (TFIDF & CATS rebuilt per fold)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage).\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | LogReg: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.957       0.957     0.957    0.937       0.937     0.937    0.883       0.883     0.792    0.883       0.883     0.787    0.886       0.886     0.822    0.897       0.897     0.740    0.875       0.875     0.925     0.70       0.754       0.5\n",
            "2        0.962       0.962     0.962    0.957       0.957     0.957    0.896       0.857     0.883    0.894       0.857     0.879    0.905       0.861     0.897    0.864       0.872     0.830    0.950       0.850     0.975     0.53       0.767       0.5\n",
            "3        0.846       0.846     0.846    0.872       0.872     0.872    0.805       0.792     0.805    0.801       0.789     0.801    0.828       0.814     0.828    0.766       0.761     0.766    0.900       0.875     0.900     0.49       0.565       0.5\n",
            "4        0.930       0.930     0.930    0.895       0.895     0.895    0.857       0.818     0.818    0.858       0.816     0.816    0.857       0.833     0.833    0.892       0.795     0.795    0.825       0.875     0.875     0.61       0.503       0.5\n",
            "5        0.907       0.907     0.907    0.889       0.889     0.889    0.818       0.818     0.753    0.815       0.816     0.745    0.837       0.833     0.800    0.783       0.795     0.691    0.900       0.875     0.950     0.69       0.780       0.5\n",
            "6        0.973       0.973     0.973    0.968       0.968     0.968    0.909       0.883     0.831    0.907       0.884     0.825    0.916       0.883     0.857    0.884       0.919     0.765    0.950       0.850     0.975     0.69       0.815       0.5\n",
            "7        0.976       0.976     0.976    0.965       0.965     0.965    0.922       0.896     0.857    0.921       0.898     0.853    0.927       0.895     0.874    0.905       0.944     0.809    0.950       0.850     0.950     0.60       0.808       0.5\n",
            "8        0.936       0.936     0.936    0.918       0.918     0.918    0.857       0.818     0.792    0.857       0.817     0.790    0.857       0.829     0.818    0.868       0.791     0.735    0.846       0.872     0.923     0.69       0.634       0.5\n",
            "9        0.952       0.952     0.952    0.939       0.939     0.939    0.870       0.896     0.831    0.869       0.896     0.830    0.878       0.895     0.847    0.837       0.919     0.783    0.923       0.872     0.923     0.62       0.799       0.5\n",
            "10       0.923       0.923     0.923    0.903       0.903     0.903    0.831       0.805     0.805    0.831       0.804     0.804    0.831       0.819     0.819    0.842       0.773     0.773    0.821       0.872     0.872     0.66       0.510       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.1         11.2\n",
            "True Attack          2.9         36.8\n",
            "Accuracy: 0.817 | Precision (attack): 0.767 | Recall (attack): 0.927 | Specificity (normal): 0.700\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.1          6.2\n",
            "True Attack          4.2         35.5\n",
            "Accuracy: 0.865 | Precision (attack): 0.851 | Recall (attack): 0.894 | Specificity (normal): 0.834\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.8          6.5\n",
            "True Attack          5.3         34.4\n",
            "Accuracy: 0.847 | Precision (attack): 0.841 | Recall (attack): 0.866 | Specificity (normal): 0.826\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | RF: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.961       0.961     0.961    0.946       0.946     0.946    0.896       0.896     0.896    0.896       0.896     0.896    0.900       0.900     0.900    0.900       0.900     0.900    0.900       0.900     0.900     0.48       0.530       0.5\n",
            "2        0.980       0.980     0.980    0.976       0.976     0.976    0.922       0.909     0.870    0.924       0.910     0.869    0.921       0.909     0.878    0.972       0.946     0.857    0.875       0.875     0.900     0.61       0.604       0.5\n",
            "3        0.936       0.936     0.936    0.918       0.918     0.918    0.844       0.805     0.831    0.847       0.803     0.833    0.838       0.819     0.827    0.912       0.791     0.886    0.775       0.850     0.775     0.51       0.360       0.5\n",
            "4        0.987       0.987     0.987    0.985       0.985     0.985    0.922       0.909     0.909    0.921       0.911     0.911    0.927       0.907     0.907    0.905       0.971     0.971    0.950       0.850     0.850     0.30       0.512       0.5\n",
            "5        0.960       0.960     0.960    0.952       0.952     0.952    0.883       0.870     0.857    0.880       0.871     0.857    0.894       0.872     0.861    0.844       0.895     0.872    0.950       0.850     0.850     0.39       0.536       0.5\n",
            "6        0.986       0.986     0.986    0.983       0.983     0.983    0.948       0.909     0.935    0.949       0.911     0.936    0.949       0.907     0.935    0.974       0.971     0.973    0.925       0.850     0.900     0.49       0.602       0.5\n",
            "7        0.984       0.984     0.984    0.981       0.981     0.981    0.948       0.935     0.896    0.947       0.934     0.898    0.951       0.938     0.895    0.929       0.927     0.944    0.975       0.950     0.850     0.31       0.324       0.5\n",
            "8        0.960       0.960     0.960    0.945       0.945     0.945    0.909       0.896     0.909    0.910       0.896     0.910    0.907       0.895     0.907    0.944       0.919     0.944    0.872       0.872     0.872     0.46       0.454       0.5\n",
            "9        0.973       0.973     0.973    0.966       0.966     0.966    0.922       0.909     0.883    0.922       0.909     0.884    0.921       0.909     0.877    0.946       0.921     0.941    0.897       0.897     0.821     0.49       0.488       0.5\n",
            "10       0.948       0.948     0.948    0.928       0.928     0.928    0.883       0.818     0.857    0.884       0.817     0.858    0.877       0.829     0.845    0.941       0.791     0.938    0.821       0.872     0.769     0.46       0.296       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.4          2.9\n",
            "True Attack          6.0         33.7\n",
            "Accuracy: 0.884 | Precision (attack): 0.921 | Recall (attack): 0.849 | Specificity (normal): 0.922\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.4          2.9\n",
            "True Attack          4.2         35.5\n",
            "Accuracy: 0.908 | Precision (attack): 0.924 | Recall (attack): 0.894 | Specificity (normal): 0.922\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.4          3.9\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.886 | Precision (attack): 0.899 | Recall (attack): 0.877 | Specificity (normal): 0.895\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | HGB: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.957       0.957     0.957    0.932       0.932     0.932    0.870       0.896     0.831    0.869       0.898     0.828    0.878       0.895     0.847    0.857       0.944     0.800    0.900       0.850     0.900     0.55       0.858       0.5\n",
            "2        0.988       0.988     0.988    0.986       0.986     0.986    0.948       0.909     0.922    0.949       0.911     0.922    0.949       0.907     0.925    0.974       0.971     0.925    0.925       0.850     0.925     0.63       0.920       0.5\n",
            "3        0.948       0.948     0.948    0.935       0.935     0.935    0.870       0.857     0.857    0.870       0.856     0.858    0.875       0.864     0.857    0.875       0.854     0.892    0.875       0.875     0.825     0.30       0.294       0.5\n",
            "4        0.978       0.978     0.978    0.972       0.972     0.972    0.922       0.896     0.909    0.921       0.898     0.909    0.927       0.895     0.911    0.905       0.944     0.923    0.950       0.850     0.900     0.30       0.798       0.5\n",
            "5        0.946       0.946     0.946    0.931       0.931     0.931    0.831       0.870     0.831    0.828       0.871     0.828    0.847       0.872     0.847    0.800       0.895     0.800    0.900       0.850     0.900     0.49       0.793       0.5\n",
            "6        0.976       0.976     0.976    0.971       0.971     0.971    0.909       0.883     0.909    0.908       0.883     0.908    0.914       0.886     0.914    0.902       0.897     0.902    0.925       0.875     0.925     0.30       0.693       0.5\n",
            "7        0.979       0.979     0.979    0.974       0.974     0.974    0.922       0.896     0.909    0.923       0.898     0.909    0.923       0.895     0.911    0.947       0.944     0.923    0.900       0.850     0.900     0.51       0.658       0.5\n",
            "8        0.958       0.958     0.958    0.943       0.943     0.943    0.883       0.870     0.870    0.883       0.870     0.871    0.883       0.872     0.865    0.895       0.872     0.914    0.872       0.872     0.821     0.31       0.303       0.5\n",
            "9        0.961       0.961     0.961    0.954       0.954     0.954    0.883       0.883     0.883    0.883       0.883     0.883    0.886       0.883     0.886    0.875       0.895     0.875    0.897       0.872     0.897     0.39       0.876       0.5\n",
            "10       0.946       0.946     0.946    0.922       0.922     0.922    0.896       0.883     0.870    0.896       0.883     0.870    0.897       0.886     0.868    0.897       0.875     0.892    0.897       0.897     0.846     0.30       0.233       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.6          4.7\n",
            "True Attack          4.6         35.1\n",
            "Accuracy: 0.879 | Precision (attack): 0.882 | Recall (attack): 0.884 | Specificity (normal): 0.874\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.9          4.4\n",
            "True Attack          3.8         35.9\n",
            "Accuracy: 0.894 | Precision (attack): 0.891 | Recall (attack): 0.904 | Specificity (normal): 0.882\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.8          3.5\n",
            "True Attack          5.4         34.3\n",
            "Accuracy: 0.884 | Precision (attack): 0.907 | Recall (attack): 0.864 | Specificity (normal): 0.906\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (TEXT-ONLY) (TFIDF & CATS rebuilt per fold)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage).\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | LogReg: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.148       0.5\n",
            "2          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.131       0.5\n",
            "3          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.043       0.5\n",
            "4          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.040       0.5\n",
            "5          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.123       0.5\n",
            "6          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.157       0.5\n",
            "7          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.043       0.5\n",
            "8          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.069       0.5\n",
            "9          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.099       0.5\n",
            "10         1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.046       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision (attack): 1.000 | Recall (attack): 1.000 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision (attack): 1.000 | Recall (attack): 1.000 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.3          1.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.987 | Precision (attack): 0.975 | Recall (attack): 1.000 | Specificity (normal): 0.973\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | RF: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     0.987    1.000       0.986     0.988    1.000       0.988     0.987    1.000       0.976       1.0      1.0       1.000     0.975     0.37       0.362       0.5\n",
            "2        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     0.987    1.000       0.986     0.988    1.000       0.988     0.987    1.000       0.976       1.0      1.0       1.000     0.975     0.30       0.232       0.5\n",
            "3        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     0.987    1.000       0.986     0.988    1.000       0.988     0.987    1.000       0.976       1.0      1.0       1.000     0.975     0.30       0.272       0.5\n",
            "4        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     0.987    1.000       0.986     0.988    1.000       0.988     0.987    1.000       0.976       1.0      1.0       1.000     0.975     0.31       0.304       0.5\n",
            "5        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     1.000    1.000       0.986     1.000    1.000       0.988     1.000    1.000       0.976       1.0      1.0       1.000     1.000     0.43       0.422       0.5\n",
            "6        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     0.987    1.000       0.986     0.988    1.000       0.988     0.987    1.000       0.976       1.0      1.0       1.000     0.975     0.37       0.368       0.5\n",
            "7        0.999       0.999     0.999    0.999       0.999     0.999    0.987       0.974     0.987    0.986       0.974     0.988    0.988       0.975     0.987    0.976       0.975       1.0      1.0       0.975     0.975     0.38       0.452       0.5\n",
            "8        1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     1.000    1.000       0.987     1.000    1.000       0.987     1.000    1.000       0.975       1.0      1.0       1.000     1.000     0.35       0.342       0.5\n",
            "9        0.999       0.999     0.999    0.999       0.999     0.999    0.987       0.974     0.987    0.987       0.974     0.987    0.987       0.974     0.987    0.975       0.974       1.0      1.0       0.974     0.974     0.35       0.440       0.5\n",
            "10       1.000       1.000     1.000    1.000       1.000     1.000    1.000       0.987     1.000    1.000       0.987     1.000    1.000       0.987     1.000    1.000       0.975       1.0      1.0       1.000     1.000     0.38       0.372       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.991 | Precision (attack): 1.000 | Recall (attack): 0.982 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.997 | Precision (attack): 0.995 | Recall (attack): 1.000 | Specificity (normal): 0.995\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.3          1.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.984 | Precision (attack): 0.975 | Recall (attack): 0.995 | Specificity (normal): 0.973\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | HGB: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.013       0.5\n",
            "2          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.003       0.5\n",
            "3          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.023       0.5\n",
            "4          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.005       0.5\n",
            "5          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.010       0.5\n",
            "6          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.001       0.5\n",
            "7          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.986       1.0      1.0       0.988       1.0      1.0       0.976       1.0      1.0         1.0       1.0      0.3       0.023       0.5\n",
            "8          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.010       0.5\n",
            "9          1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.010       0.5\n",
            "10         1.0         1.0       1.0      1.0         1.0       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.987       1.0      1.0       0.975       1.0      1.0         1.0       1.0      0.3       0.007       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision (attack): 1.000 | Recall (attack): 1.000 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision (attack): 1.000 | Recall (attack): 1.000 | Specificity (normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.3          1.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.987 | Precision (attack): 0.975 | Recall (attack): 1.000 | Specificity (normal): 0.973\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (CAT-ONLY precomputed) (TFIDF & CATS rebuilt per fold)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage).\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | LogReg: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.755       0.755     0.755    0.824       0.824     0.824    0.532       0.818     0.532    0.514       0.811     0.514    0.690       0.851     0.690    0.526       0.741     0.526    1.000       1.000     1.000     0.30       0.916       0.5\n",
            "2        0.810       0.810     0.810    0.859       0.859     0.859    0.506       0.857     0.506    0.488       0.852     0.488    0.672       0.876     0.672    0.513       0.796     0.513    0.975       0.975     0.975     0.30       0.918       0.5\n",
            "3        0.705       0.705     0.705    0.765       0.765     0.765    0.532       0.766     0.532    0.514       0.758     0.514    0.690       0.812     0.690    0.526       0.696     0.526    1.000       0.975     1.000     0.30       0.910       0.5\n",
            "4        0.557       0.557     0.557    0.568       0.568     0.568    0.532       0.545     0.519    0.515       0.529     0.501    0.684       0.685     0.678    0.527       0.535     0.520    0.975       0.950     0.975     0.67       0.901       0.5\n",
            "5        0.736       0.736     0.736    0.790       0.790     0.790    0.584       0.571     0.558    0.568       0.556     0.541    0.714       0.697     0.702    0.556       0.551     0.541    1.000       0.950     1.000     0.69       0.874       0.5\n",
            "6        0.790       0.790     0.790    0.845       0.845     0.845    0.545       0.844     0.545    0.528       0.839     0.528    0.690       0.867     0.690    0.534       0.780     0.534    0.975       0.975     0.975     0.30       0.915       0.5\n",
            "7        0.657       0.657     0.657    0.711       0.711     0.711    0.519       0.714     0.519    0.500       0.704     0.500    0.684       0.780     0.684    0.519       0.650     0.519    1.000       0.975     1.000     0.30       0.912       0.5\n",
            "8        0.688       0.688     0.688    0.751       0.751     0.751    0.506       0.753     0.506    0.500       0.751     0.500    0.667       0.796     0.667    0.507       0.685     0.507    0.974       0.949     0.974     0.30       0.921       0.5\n",
            "9        0.729       0.729     0.729    0.789       0.789     0.789    0.532       0.753     0.532    0.526       0.751     0.526    0.684       0.796     0.684    0.520       0.685     0.520    1.000       0.949     1.000     0.30       0.895       0.5\n",
            "10       0.785       0.785     0.785    0.848       0.848     0.848    0.545       0.571     0.532    0.539       0.566     0.527    0.690       0.697     0.679    0.527       0.543     0.521    1.000       0.974     0.974     0.30       0.878       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.5         35.8\n",
            "True Attack          0.5         39.2\n",
            "Accuracy: 0.529 | Precision (attack): 0.523 | Recall (attack): 0.987 | Specificity (normal): 0.040\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.8         35.5\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.534 | Precision (attack): 0.525 | Recall (attack): 0.990 | Specificity (normal): 0.048\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         17.0         20.3\n",
            "True Attack          1.3         38.4\n",
            "Accuracy: 0.719 | Precision (attack): 0.654 | Recall (attack): 0.967 | Specificity (normal): 0.456\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | RF: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.548       0.548     0.548    0.554       0.554     0.554    0.532       0.558     0.519    0.514       0.541     0.500    0.690       0.702     0.684    0.526       0.541     0.519    1.000       1.000     1.000     0.53       0.919       0.5\n",
            "2        0.514       0.514     0.514    0.489       0.489     0.489    0.532       0.519     0.506    0.514       0.500     0.489    0.690       0.684     0.667    0.526       0.519     0.514    1.000       1.000     0.950     0.30       0.288       0.5\n",
            "3        0.534       0.534     0.534    0.528       0.528     0.528    0.532       0.532     0.519    0.514       0.515     0.501    0.690       0.684     0.678    0.526       0.527     0.520    1.000       0.975     0.975     0.46       0.887       0.5\n",
            "4        0.535       0.535     0.535    0.530       0.530     0.530    0.545       0.532     0.532    0.527       0.515     0.515    0.696       0.684     0.684    0.533       0.527     0.527    1.000       0.975     0.975     0.43       0.867       0.5\n",
            "5        0.543       0.543     0.543    0.546       0.546     0.546    0.558       0.571     0.532    0.541       0.554     0.514    0.702       0.708     0.690    0.541       0.548     0.526    1.000       1.000     1.000     0.62       0.848       0.5\n",
            "6        0.557       0.557     0.557    0.569       0.569     0.569    0.558       0.571     0.532    0.542       0.555     0.514    0.696       0.703     0.690    0.542       0.549     0.526    0.975       0.975     1.000     0.55       0.917       0.5\n",
            "7        0.520       0.520     0.520    0.501       0.501     0.501    0.519       0.506     0.519    0.500       0.488     0.500    0.684       0.672     0.684    0.519       0.513     0.519    1.000       0.975     1.000     0.30       0.924       0.5\n",
            "8        0.509       0.509     0.509    0.500       0.500     0.500    0.519       0.506     0.506    0.513       0.500     0.500    0.678       0.667     0.667    0.513       0.507     0.507    1.000       0.974     0.974     0.43       0.909       0.5\n",
            "9        0.508       0.508     0.508    0.502       0.502     0.502    0.532       0.519     0.519    0.526       0.513     0.513    0.684       0.678     0.678    0.520       0.513     0.513    1.000       1.000     1.000     0.56       0.550       0.5\n",
            "10       0.535       0.535     0.535    0.553       0.553     0.553    0.532       0.545     0.506    0.527       0.540     0.500    0.679       0.685     0.672    0.521       0.528     0.506    0.974       0.974     1.000     0.55       0.918       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          0.8         36.5\n",
            "True Attack          0.5         39.2\n",
            "Accuracy: 0.519 | Precision (attack): 0.518 | Recall (attack): 0.987 | Specificity (normal): 0.021\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.8         35.5\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.536 | Precision (attack): 0.527 | Recall (attack): 0.995 | Specificity (normal): 0.048\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.2         35.1\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.536 | Precision (attack): 0.527 | Recall (attack): 0.985 | Specificity (normal): 0.059\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | HGB: Per-fold metrics (K=10) ===\n",
            "            AP                            AUC                            Acc                         BalAcc                             F1                           Prec                            Rec                            thr                      \n",
            "which F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50 F1-tuned Recall≥0.85 fixed0.50\n",
            "fold                                                                                                                                                                                                                                                         \n",
            "1        0.541       0.541     0.541    0.541       0.541     0.541    0.558       0.532     0.532    0.541       0.514     0.514    0.702       0.690     0.690    0.541       0.526     0.526    1.000       1.000     1.000     0.57       0.565       0.5\n",
            "2        0.514       0.514     0.514    0.488       0.488     0.488    0.519       0.494     0.494    0.500       0.475     0.475    0.684       0.661     0.661    0.519       0.507     0.507    1.000       0.950     0.950     0.30       0.526       0.5\n",
            "3        0.534       0.534     0.534    0.528       0.528     0.528    0.545       0.532     0.532    0.528       0.515     0.515    0.690       0.684     0.684    0.534       0.527     0.527    0.975       0.975     0.975     0.58       0.571       0.5\n",
            "4        0.527       0.527     0.527    0.515       0.515     0.515    0.532       0.519     0.532    0.515       0.500     0.515    0.684       0.684     0.684    0.527       0.519     0.527    0.975       1.000     0.975     0.46       0.453       0.5\n",
            "5        0.541       0.541     0.541    0.541       0.541     0.541    0.558       0.519     0.519    0.541       0.500     0.500    0.702       0.684     0.684    0.541       0.519     0.519    1.000       1.000     1.000     0.52       0.510       0.5\n",
            "6        0.541       0.541     0.541    0.541       0.541     0.541    0.558       0.545     0.545    0.542       0.528     0.528    0.696       0.690     0.690    0.542       0.534     0.534    0.975       0.975     0.975     0.57       0.568       0.5\n",
            "7        0.513       0.513     0.513    0.488       0.488     0.488    0.519       0.519     0.506    0.500       0.500     0.488    0.684       0.684     0.672    0.519       0.519     0.513    1.000       1.000     0.975     0.30       0.486       0.5\n",
            "8        0.514       0.514     0.514    0.513       0.513     0.513    0.506       0.506     0.506    0.500       0.500     0.500    0.672       0.667     0.667    0.506       0.507     0.507    1.000       0.974     0.974     0.30       0.570       0.5\n",
            "9        0.507       0.507     0.507    0.499       0.499     0.499    0.506       0.494     0.494    0.500       0.488     0.488    0.672       0.655     0.655    0.506       0.500     0.500    1.000       0.949     0.949     0.30       0.523       0.5\n",
            "10       0.527       0.527     0.527    0.539       0.539     0.539    0.545       0.532     0.532    0.540       0.527     0.527    0.685       0.679     0.679    0.528       0.521     0.521    0.974       0.974     0.974     0.57       0.569       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.3         36.0\n",
            "True Attack          1.0         38.7\n",
            "Accuracy: 0.519 | Precision (attack): 0.518 | Recall (attack): 0.975 | Specificity (normal): 0.035\n",
            "\n",
            "=== Mean CM (F1-tuned) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.9         35.4\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.535 | Precision (attack): 0.526 | Recall (attack): 0.990 | Specificity (normal): 0.051\n",
            "\n",
            "=== Mean CM (Recall≥0.85) ===\n",
            "rows = true [normal, attack]; cols = predicted [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.1         36.2\n",
            "True Attack          0.8         38.9\n",
            "Accuracy: 0.519 | Precision (attack): 0.518 | Recall (attack): 0.980 | Specificity (normal): 0.029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# optional groups → uses StratifiedGroupKFold when available, otherwise GroupKFold;\n",
        "\n",
        "# inner validation on the train fold to choose both the F1-max threshold and the Recall≥target threshold; thresholds are then applied to the (held-out) test fold;\n",
        "\n",
        "# same per-fold TF-IDF, dead-feature kill, cost weighting;\n",
        "\n",
        "# continues to rebuild per-fold cats if you pass the bags."
      ],
      "metadata": {
        "id": "o4-3T6p1sPef"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional, List, Dict, Tuple\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import issparse, csr_matrix, hstack, vstack\n",
        "from collections import Counter\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score, average_precision_score,\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    balanced_accuracy_score, confusion_matrix, precision_recall_curve\n",
        ")\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold\n",
        "try:\n",
        "    from sklearn.model_selection import StratifiedGroupKFold\n",
        "    HAS_SGKF = True\n",
        "except Exception:\n",
        "    HAS_SGKF = False\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "\n",
        "def _safe_slice(M, idx):\n",
        "    if M is None:\n",
        "        return None\n",
        "    if issparse(M):\n",
        "        return M[idx]\n",
        "    return M.iloc[idx] if hasattr(M, \"iloc\") else M[idx]\n",
        "\n",
        "def _texts_to_tfidf(train_texts: List[str], test_texts: List[str], max_features: int) -> Tuple[csr_matrix, csr_matrix]:\n",
        "    train_has_any = any((t or \"\").strip() for t in train_texts)\n",
        "    if not train_has_any:\n",
        "        return csr_matrix((len(train_texts), 0)), csr_matrix((len(test_texts), 0))\n",
        "    try:\n",
        "        tfv = TfidfVectorizer(\n",
        "            max_features=max_features,\n",
        "            stop_words=None,\n",
        "            lowercase=True,\n",
        "            token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        "            sublinear_tf=True\n",
        "        )\n",
        "        return tfv.fit_transform(train_texts), tfv.transform(test_texts)\n",
        "    except ValueError:\n",
        "        tfv = TfidfVectorizer(analyzer=\"char\", ngram_range=(3,5), max_features=max_features, sublinear_tf=True)\n",
        "        return tfv.fit_transform(train_texts), tfv.transform(test_texts)\n",
        "\n",
        "def _bags_to_sparse(train_bags: List[Dict[str, float]], test_bags: List[Dict[str, float]], topk: int):\n",
        "    ctr = Counter()\n",
        "    for d in train_bags:\n",
        "        if d: ctr.update(d)\n",
        "    vocab = [k for k,_ in ctr.most_common(topk)]\n",
        "    index = {k:i for i,k in enumerate(vocab)}\n",
        "    def _encode(bags):\n",
        "        rows = []\n",
        "        for d in bags:\n",
        "            if not d:\n",
        "                rows.append(csr_matrix((1, len(vocab)))); continue\n",
        "            idx, val = [], []\n",
        "            for k, v in d.items():\n",
        "                j = index.get(k)\n",
        "                if j is not None:\n",
        "                    idx.append(j); val.append(float(v))\n",
        "            if idx:\n",
        "                rows.append(csr_matrix((val, ([0]*len(idx), idx)), shape=(1, len(vocab))))\n",
        "            else:\n",
        "                rows.append(csr_matrix((1, len(vocab))))\n",
        "        return vstack(rows).tocsr()\n",
        "    return _encode(train_bags), _encode(test_bags), vocab\n",
        "\n",
        "def _threshold_for_f1(y_true, scores):\n",
        "    grid = np.linspace(0.30, 0.70, 41)\n",
        "    f1s = [f1_score(y_true, (scores >= t).astype(int), zero_division=0) for t in grid]\n",
        "    return float(grid[int(np.argmax(f1s))])\n",
        "\n",
        "def _threshold_for_recall(y_true, scores, target_recall: float = 0.85):\n",
        "    prec, rec, thr = precision_recall_curve(y_true, scores)\n",
        "    chosen = None\n",
        "    for p, r, t in zip(prec[1:], rec[1:], thr):\n",
        "        if r >= target_recall and (chosen is None or p > chosen[0]):\n",
        "            chosen = (p, r, t)\n",
        "    if chosen is None:\n",
        "        # Get best recall (ties → higher precision)\n",
        "        idx = int(np.argmax(rec[1:]))\n",
        "        return float(thr[idx])\n",
        "    return float(chosen[2])\n",
        "\n",
        "def eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk,\n",
        "    X_nodecat_blk, X_edgecat_blk,\n",
        "    texts_series,\n",
        "    X_struct_blk,\n",
        "    y,\n",
        "    tag,\n",
        "    node_cat_bags: Optional[List[Dict[str, float]]] = None,\n",
        "    edge_cat_bags: Optional[List[Dict[str, float]]] = None,\n",
        "    topk_node: int = 250,\n",
        "    topk_edge: int = 250,\n",
        "    EDGE_TEXT_MAX_FEATURES: int = 1000,\n",
        "    class_weight: Dict[int, float] = {0:1.0, 1:2.0},\n",
        "    target_recall: float = 0.85,\n",
        "    groups: Optional[np.ndarray] = None,   # <— NEW: pass group ids (same length as y)\n",
        "    inner_splits: int = 3,                 # <— NEW: thresholds picked on train via inner CV\n",
        "):\n",
        "    print(f\"\\n\\n====================  {tag} (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\")\n",
        "    K = 10\n",
        "    if groups is not None:\n",
        "        if HAS_SGKF:\n",
        "            outer = StratifiedGroupKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "            splits = list(outer.split(np.zeros_like(y), y, groups))\n",
        "        else:\n",
        "            print(\"[WARN] StratifiedGroupKFold not available — falling back to GroupKFold (no stratification).\")\n",
        "            outer = GroupKFold(n_splits=K)\n",
        "            splits = list(outer.split(np.zeros_like(y), y, groups))\n",
        "    else:\n",
        "        outer = StratifiedKFold(n_splits=K, shuffle=True, random_state=42)\n",
        "        splits = list(outer.split(np.zeros_like(y), y))\n",
        "\n",
        "    def _sample_weights(y_fold):\n",
        "        w = np.ones_like(y_fold, dtype=float)\n",
        "        w[y_fold==0] = class_weight.get(0, 1.0)\n",
        "        w[y_fold==1] = class_weight.get(1, 1.0)\n",
        "        return w\n",
        "\n",
        "    rebuild_cats = (node_cat_bags is not None) and (edge_cat_bags is not None)\n",
        "    if not rebuild_cats:\n",
        "        print(\"[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\")\n",
        "\n",
        "    def _run_one(clf, name):\n",
        "        rows, cms_05, cms_f1, cms_rec = [], [], [], []\n",
        "\n",
        "        for fold, (tr, te) in enumerate(splits, 1):\n",
        "            # text\n",
        "            X_text_tr, X_text_te = _texts_to_tfidf(\n",
        "                texts_series.iloc[tr].astype(str).tolist(),\n",
        "                texts_series.iloc[te].astype(str).tolist(),\n",
        "                EDGE_TEXT_MAX_FEATURES\n",
        "            )\n",
        "            # cats\n",
        "            if rebuild_cats:\n",
        "                node_tr, node_te, _ = _bags_to_sparse([node_cat_bags[i] for i in tr], [node_cat_bags[i] for i in te], topk_node)\n",
        "                edge_tr, edge_te, _ = _bags_to_sparse([edge_cat_bags[i] for i in tr], [edge_cat_bags[i] for i in te], topk_edge)\n",
        "            else:\n",
        "                node_tr = _safe_slice(X_nodecat_blk, tr); node_te = _safe_slice(X_nodecat_blk, te)\n",
        "                edge_tr = _safe_slice(X_edgecat_blk, tr); edge_te = _safe_slice(X_edgecat_blk, te)\n",
        "\n",
        "            # numeric/struct\n",
        "            Xn_tr, Xn_te = _safe_slice(X_num_blk, tr), _safe_slice(X_num_blk, te)\n",
        "            Xs_tr = _safe_slice(X_struct_blk, tr) if X_struct_blk is not None else None\n",
        "            Xs_te = _safe_slice(X_struct_blk, te) if X_struct_blk is not None else None\n",
        "\n",
        "            Xattr_tr = hstack([Xn_tr, node_tr, edge_tr, X_text_tr]).tocsr()\n",
        "            Xattr_te = hstack([Xn_te, node_te, edge_te, X_text_te]).tocsr()\n",
        "            if \"ATTRS-ONLY\" in tag:\n",
        "                Xtr, Xte = Xattr_tr, Xattr_te\n",
        "            elif \"STRUCTURE-ONLY\" in tag:\n",
        "                Xtr, Xte = Xs_tr, Xs_te\n",
        "            else:\n",
        "                Xtr = hstack([Xs_tr, Xattr_tr]).tocsr() if Xs_tr is not None else Xattr_tr\n",
        "                Xte = hstack([Xs_te, Xattr_te]).tocsr() if Xs_te is not None else Xattr_te\n",
        "\n",
        "            ytr, yte = y[tr], y[te]\n",
        "            sw_tr = _sample_weights(ytr)\n",
        "\n",
        "            need_dense = (\n",
        "                (isinstance(clf, LogisticRegression) and getattr(clf, \"solver\", \"\") in {\"lbfgs\",\"liblinear\",\"newton-cg\"})\n",
        "                or isinstance(clf, RandomForestClassifier)\n",
        "                or isinstance(clf, HistGradientBoostingClassifier)\n",
        "            )\n",
        "            steps = [(\"vt\", VarianceThreshold(0.0)),\n",
        "                     (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
        "                     (\"sc\", StandardScaler(with_mean=False))]\n",
        "            if need_dense:\n",
        "                steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "            steps.append((\"clf\", clf))\n",
        "            pipe = Pipeline(steps)\n",
        "\n",
        "            # === inner CV on TRAIN to pick thresholds ===\n",
        "            inner = StratifiedKFold(n_splits=inner_splits, shuffle=True, random_state=fold+123)\n",
        "            val_scores, val_y = [], []\n",
        "            for tr2, va2 in inner.split(np.zeros_like(ytr), ytr):\n",
        "                pipe.fit(Xtr[tr2], ytr[tr2], clf__sample_weight=sw_tr[tr2])\n",
        "                if hasattr(pipe[-1], \"predict_proba\"):\n",
        "                    ps = pipe.predict_proba(Xtr[va2])[:,1]\n",
        "                else:\n",
        "                    try:\n",
        "                        s = pipe.decision_function(Xtr[va2]); ps = (s - s.min())/(s.max()-s.min()+1e-9)\n",
        "                    except Exception:\n",
        "                        ps = pipe.predict_proba(Xtr[va2])[:,1]\n",
        "                val_scores.append(ps); val_y.append(ytr[va2])\n",
        "            val_scores = np.concatenate(val_scores); val_y = np.concatenate(val_y)\n",
        "\n",
        "            t_f1  = _threshold_for_f1(val_y,  val_scores)\n",
        "            t_rec = _threshold_for_recall(val_y, val_scores, target_recall=target_recall)\n",
        "\n",
        "            # === refit on full train, evaluate on TEST ===\n",
        "            pipe.fit(Xtr, ytr, clf__sample_weight=sw_tr)\n",
        "            if hasattr(pipe[-1], \"predict_proba\"):\n",
        "                p = pipe.predict_proba(Xte)[:,1]\n",
        "            else:\n",
        "                try:\n",
        "                    s = pipe.decision_function(Xte); p = (s - s.min())/(s.max()-s.min()+1e-9)\n",
        "                except Exception:\n",
        "                    p = pipe.predict_proba(Xte)[:,1]\n",
        "\n",
        "            # fixed 0.50\n",
        "            y05 = (p >= 0.50).astype(int);   cm05 = confusion_matrix(yte, y05, labels=[0,1]); cms_05.append(cm05)\n",
        "            # F1 tuned\n",
        "            y_f1 = (p >= t_f1).astype(int);  cmf = confusion_matrix(yte, y_f1, labels=[0,1]); cms_f1.append(cmf)\n",
        "            # Recall tuned\n",
        "            y_rc = (p >= t_rec).astype(int); cmr = confusion_matrix(yte, y_rc, labels=[0,1]); cms_rec.append(cmr)\n",
        "\n",
        "            def _row(which, yhat, thr):\n",
        "                return dict(\n",
        "                    fold=fold, which=which, thr=thr,\n",
        "                    AUC=roc_auc_score(yte, p),\n",
        "                    AP=average_precision_score(yte, p),\n",
        "                    Acc=accuracy_score(yte, yhat),\n",
        "                    Prec=precision_score(yte, yhat, zero_division=0),\n",
        "                    Rec=recall_score(yte, yhat, zero_division=0),\n",
        "                    F1=f1_score(yte, yhat, zero_division=0),\n",
        "                    BalAcc=balanced_accuracy_score(yte, yhat)\n",
        "                )\n",
        "            rows += [\n",
        "                _row(\"fixed0.50\", y05, 0.50),\n",
        "                _row(\"F1-tuned(train)\", y_f1, t_f1),\n",
        "                _row(f\"Recall≥{target_recall:.2f}(train)\", y_rc, t_rec),\n",
        "            ]\n",
        "\n",
        "        df = pd.DataFrame(rows).round(3)\n",
        "        print(f\"\\n=== {tag} | {name}: Per-fold metrics (K={K}) ===\")\n",
        "        print(df.pivot_table(index=\"fold\", columns=\"which\",\n",
        "                             values=[\"AUC\",\"AP\",\"Acc\",\"Prec\",\"Rec\",\"F1\",\"BalAcc\",\"thr\"]).round(3).to_string())\n",
        "\n",
        "        def _avg(cm_list): return np.mean(np.stack(cm_list, axis=0), axis=0)\n",
        "        def _dump(cm, title):\n",
        "            tn, fp, fn, tp = cm[0,0], cm[0,1], cm[1,0], cm[1,1]\n",
        "            acc=(tp+tn)/np.sum(cm); prec=tp/(tp+fp+1e-9); rec=tp/(tp+fn+1e-9); spec=tn/(tn+fp+1e-9)\n",
        "            print(f\"\\n=== {title} ===\")\n",
        "            print(\"rows=true [normal, attack]; cols=pred [normal, attack]\")\n",
        "            print(pd.DataFrame(cm, index=[\"True Normal\",\"True Attack\"], columns=[\"Pred Normal\",\"Pred Attack\"]).round(1).to_string())\n",
        "            print(f\"Accuracy: {acc:.3f} | Precision(attack): {prec:.3f} | Recall(attack): {rec:.3f} | Specificity(normal): {spec:.3f}\")\n",
        "\n",
        "        _dump(_avg(cms_05),  \"Mean CM (fixed 0.50)\")\n",
        "        _dump(_avg(cms_f1),  \"Mean CM (F1-tuned on train)\")\n",
        "        _dump(_avg(cms_rec), f\"Mean CM (Recall≥{target_recall:.2f} on train)\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    logreg = LogisticRegression(max_iter=3000, solver=\"lbfgs\", class_weight=class_weight, random_state=42)\n",
        "    rf     = RandomForestClassifier(n_estimators=500, random_state=42, class_weight=class_weight)\n",
        "    hgb    = HistGradientBoostingClassifier(random_state=42)\n",
        "\n",
        "    _ = _run_one(logreg, \"LogReg\")\n",
        "    _ = _run_one(rf,     \"RF\")\n",
        "    _ = _run_one(hgb,    \"HGB\")\n"
      ],
      "metadata": {
        "id": "_RJPDGoWMOmS"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _normalize_text(s: str) -> str:\n",
        "    if not isinstance(s, str):\n",
        "        s = \"\" if s is None else str(s)\n",
        "    s = s.lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)         # collapse whitespace\n",
        "    s = re.sub(r\"[^\\w\\s]\", \"\", s)      # drop punctuation\n",
        "    s = s.strip()\n",
        "    return s\n",
        "\n",
        "def _bag_signature(bag, topk=12):\n",
        "    \"\"\"stable signature string from a dict bag (keys only, sorted by freq then name)\"\"\"\n",
        "    if not bag:\n",
        "        return \"\"\n",
        "    # sort keys by value desc then key\n",
        "    keys = sorted(bag.items(), key=lambda kv: (-float(kv[1]), str(kv[0])))\n",
        "    return \"|\".join(k for k,_ in keys[:topk])\n",
        "\n",
        "def build_groups(\n",
        "    df: pd.DataFrame = None,\n",
        "    row_indices=None,                  # indices you already use to slice X blocks\n",
        "    texts_series: pd.Series = None,\n",
        "    node_cat_bags=None,\n",
        "    edge_cat_bags=None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Returns a group id ndarray (same length as y).\n",
        "    Tries, in order:\n",
        "      1) A 'likely id' column in df (session/file/conversation/etc.)\n",
        "      2) Exact-normalized text hash (so identical/near-identical texts stay in one fold)\n",
        "      3) If texts are mostly empty: fall back to a signature built from categorical bags\n",
        "      4) As a last resort: unique groups per-row (no grouping)\n",
        "    \"\"\"\n",
        "    n = len(texts_series) if texts_series is not None else None\n",
        "    if df is not None and row_indices is not None:\n",
        "        candidate_cols = [\n",
        "            \"graph_id\",\"session_id\",\"conversation_id\",\"conv_id\",\"thread_id\",\n",
        "            \"dialog_id\",\"file_id\",\"source_id\",\"doc_id\",\"case_id\",\"user_id\",\"sample_id\"\n",
        "        ]\n",
        "        for col in candidate_cols:\n",
        "            if col in df.columns:\n",
        "                g = df.loc[row_indices, col].astype(str).to_numpy()\n",
        "                # quick diagnostic\n",
        "                vc = pd.Series(g).value_counts().head(5)\n",
        "                print(f\"[groups] using df['{col}'] — top groups:\\n{vc}\\n\")\n",
        "                return g\n",
        "\n",
        "    # 2) text-based grouping (very effective against text leakage)\n",
        "    if texts_series is not None:\n",
        "        tnorm = texts_series.fillna(\"\").map(_normalize_text)\n",
        "        # if >70% non-empty, just use text\n",
        "        if (tnorm != \"\").mean() > 0.3:\n",
        "            g = pd.factorize(tnorm)[0]\n",
        "            vc = pd.Series(g).value_counts().head(5)\n",
        "            print(\"[groups] using normalized text equality — top group sizes:\\n\", vc.to_string(), \"\\n\")\n",
        "            return g\n",
        "\n",
        "    # 3) categorical signature (when text is empty)\n",
        "    if node_cat_bags is not None or edge_cat_bags is not None:\n",
        "        sigs = []\n",
        "        for i in range(n):\n",
        "            ns = _bag_signature(node_cat_bags[i]) if node_cat_bags is not None else \"\"\n",
        "            es = _bag_signature(edge_cat_bags[i]) if edge_cat_bags is not None else \"\"\n",
        "            sigs.append(ns + \"||\" + es)\n",
        "        g = pd.factorize(pd.Series(sigs))[0]\n",
        "        vc = pd.Series(g).value_counts().head(5)\n",
        "        print(\"[groups] using categorical bag signatures — top group sizes:\\n\", vc.to_string(), \"\\n\")\n",
        "        return g\n",
        "\n",
        "    # 4) last resort: no grouping (every row its own group) — still runs, least protective\n",
        "    print(\"[groups] fallback: unique per-row (no real grouping).\")\n",
        "    return np.arange(n if n is not None else 0)\n"
      ],
      "metadata": {
        "id": "092eXGhpMOpH"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a leak-resistant groups vector\n",
        "groups = build_groups(\n",
        "    df=df if 'df' in globals() else None,           # pass your dataframe if you have it\n",
        "    row_indices=row_indices if 'row_indices' in globals() else None,\n",
        "    texts_series=texts_series,                       # your real text series (aligned to y)\n",
        "    # if you have bags, pass them too:\n",
        "    # node_cat_bags=node_cat_bags,\n",
        "    # edge_cat_bags=edge_cat_bags,\n",
        ")\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "EMPTY = csr_matrix((len(y), 0))\n",
        "\n",
        "# NUM-ONLY\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_final[row_indices],\n",
        "    X_nodecat_blk = EMPTY,\n",
        "    X_edgecat_blk = EMPTY,\n",
        "    texts_series  = pd.Series([\"\"]*len(y)),\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (NUM-ONLY)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups\n",
        ")\n",
        "\n",
        "# TEXT-ONLY\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = EMPTY,\n",
        "    X_nodecat_blk = EMPTY,\n",
        "    X_edgecat_blk = EMPTY,\n",
        "    texts_series  = texts_series,\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (TEXT-ONLY)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups\n",
        ")\n",
        "\n",
        "# CAT-ONLY (precomputed; may still be leaky if built globally)\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = EMPTY,\n",
        "    X_nodecat_blk = X_node_cat[row_indices],\n",
        "    X_edgecat_blk = X_edge_cat[row_indices],\n",
        "    texts_series  = pd.Series([\"\"]*len(y)),\n",
        "    X_struct_blk  = None,\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (CAT-ONLY precomputed)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups\n",
        ")\n"
      ],
      "metadata": {
        "id": "mZB1V07aMOwx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "bf673fa3-febe-4ca8-d2d3-f7763d82f2e2"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[groups] using normalized text equality — top group sizes:\n",
            " 769    1\n",
            "0      1\n",
            "730    1\n",
            "731    1\n",
            "732    1 \n",
            "\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NUM-ONLY) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.937              0.937     0.937           0.903              0.903     0.903           0.844              0.857     0.753           0.843              0.857     0.747           0.854              0.861     0.791           0.833              0.872     0.706           0.875              0.850     0.900            0.70              0.778       0.5\n",
            "2               0.960              0.960     0.960           0.954              0.954     0.954           0.844              0.857     0.857           0.844              0.857     0.853           0.850              0.861     0.874           0.850              0.872     0.809           0.850              0.850     0.950            0.70              0.765       0.5\n",
            "3               0.970              0.970     0.970           0.963              0.963     0.963           0.857              0.909     0.818           0.853              0.907     0.812           0.874              0.916     0.848           0.809              0.884     0.750           0.950              0.950     0.975            0.61              0.693       0.5\n",
            "4               0.976              0.976     0.976           0.972              0.972     0.972           0.909              0.896     0.857           0.909              0.897     0.852           0.911              0.897     0.876           0.923              0.921     0.796           0.900              0.875     0.975            0.70              0.731       0.5\n",
            "5               0.906              0.906     0.906           0.904              0.904     0.904           0.857              0.831     0.766           0.855              0.830     0.760           0.867              0.840     0.804           0.837              0.829     0.712           0.900              0.850     0.925            0.69              0.729       0.5\n",
            "6               0.916              0.916     0.916           0.902              0.902     0.902           0.779              0.766     0.779           0.778              0.767     0.773           0.790              0.769     0.813           0.780              0.789     0.725           0.800              0.750     0.925            0.70              0.743       0.5\n",
            "7               0.961              0.961     0.961           0.930              0.930     0.930           0.909              0.909     0.870           0.910              0.910     0.869           0.909              0.909     0.878           0.946              0.946     0.857           0.875              0.875     0.900            0.70              0.715       0.5\n",
            "8               0.924              0.924     0.924           0.891              0.891     0.891           0.857              0.870     0.792           0.858              0.871     0.791           0.853              0.865     0.805           0.889              0.914     0.767           0.821              0.821     0.846            0.62              0.734       0.5\n",
            "9               0.867              0.867     0.867           0.847              0.847     0.847           0.805              0.779     0.766           0.805              0.780     0.765           0.810              0.767     0.786           0.800              0.824     0.733           0.821              0.718     0.846            0.69              0.786       0.5\n",
            "10              0.947              0.947     0.947           0.927              0.927     0.927           0.844              0.857     0.831           0.844              0.857     0.830           0.846              0.857     0.843           0.846              0.868     0.795           0.846              0.846     0.897            0.67              0.742       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.0         11.3\n",
            "True Attack          3.4         36.3\n",
            "Accuracy: 0.809 | Precision(attack): 0.763 | Recall(attack): 0.914 | Specificity(normal): 0.697\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.2          6.1\n",
            "True Attack          5.4         34.3\n",
            "Accuracy: 0.851 | Precision(attack): 0.849 | Recall(attack): 0.864 | Specificity(normal): 0.836\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.4          4.9\n",
            "True Attack          6.4         33.3\n",
            "Accuracy: 0.853 | Precision(attack): 0.872 | Recall(attack): 0.839 | Specificity(normal): 0.869\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.964              0.964     0.964           0.952              0.952     0.952           0.883              0.909     0.896           0.883              0.910     0.898           0.886              0.909     0.895           0.897              0.946     0.944           0.875              0.875     0.850            0.43              0.492       0.5\n",
            "2               0.967              0.967     0.967           0.962              0.962     0.962           0.844              0.844     0.844           0.848              0.848     0.848           0.833              0.833     0.833           0.938              0.938     0.938           0.750              0.750     0.750            0.49              0.498       0.5\n",
            "3               0.976              0.976     0.976           0.971              0.971     0.971           0.883              0.909     0.909           0.880              0.910     0.911           0.894              0.909     0.907           0.844              0.946     0.971           0.950              0.875     0.850            0.34              0.482       0.5\n",
            "4               0.980              0.980     0.980           0.976              0.976     0.976           0.922              0.922     0.909           0.923              0.923     0.910           0.923              0.923     0.909           0.947              0.947     0.946           0.900              0.900     0.875            0.42              0.482       0.5\n",
            "5               0.976              0.976     0.976           0.973              0.973     0.973           0.896              0.896     0.883           0.892              0.893     0.882           0.909              0.907     0.889           0.833              0.848     0.878           1.000              0.975     0.900            0.38              0.452       0.5\n",
            "6               0.937              0.937     0.937           0.925              0.925     0.925           0.818              0.805     0.805           0.817              0.805     0.806           0.829              0.810     0.805           0.810              0.821     0.838           0.850              0.800     0.775            0.39              0.452       0.5\n",
            "7               0.977              0.977     0.977           0.961              0.961     0.961           0.935              0.935     0.935           0.935              0.935     0.936           0.937              0.937     0.935           0.949              0.949     0.973           0.925              0.925     0.900            0.44              0.454       0.5\n",
            "8               0.953              0.953     0.953           0.936              0.936     0.936           0.857              0.870     0.870           0.858              0.871     0.871           0.853              0.865     0.865           0.889              0.914     0.914           0.821              0.821     0.821            0.45              0.494       0.5\n",
            "9               0.927              0.927     0.927           0.918              0.918     0.918           0.831              0.831     0.805           0.832              0.832     0.806           0.822              0.822     0.789           0.882              0.882     0.875           0.769              0.769     0.718            0.45              0.466       0.5\n",
            "10              0.985              0.985     0.985           0.982              0.982     0.982           0.896              0.909     0.909           0.896              0.909     0.909           0.900              0.911     0.911           0.878              0.900     0.900           0.923              0.923     0.923            0.41              0.474       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.3          3.0\n",
            "True Attack          6.5         33.2\n",
            "Accuracy: 0.877 | Precision(attack): 0.917 | Recall(attack): 0.836 | Specificity(normal): 0.920\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.7          4.6\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.877 | Precision(attack): 0.883 | Recall(attack): 0.877 | Specificity(normal): 0.877\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.8          3.5\n",
            "True Attack          5.5         34.2\n",
            "Accuracy: 0.883 | Precision(attack): 0.907 | Recall(attack): 0.861 | Specificity(normal): 0.906\n",
            "\n",
            "=== ATTRS-ONLY (NUM-ONLY) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.967              0.967     0.967           0.961              0.961     0.961           0.857              0.883     0.844           0.858              0.886     0.846           0.857              0.877     0.842           0.892              0.970     0.889           0.825              0.800     0.800            0.47              0.688       0.5\n",
            "2               0.964              0.964     0.964           0.951              0.951     0.951           0.909              0.883     0.909           0.909              0.884     0.909           0.911              0.883     0.911           0.923              0.919     0.923           0.900              0.850     0.900            0.49              0.661       0.5\n",
            "3               0.979              0.979     0.979           0.971              0.971     0.971           0.909              0.922     0.922           0.908              0.923     0.923           0.914              0.923     0.923           0.902              0.947     0.947           0.925              0.900     0.900            0.31              0.552       0.5\n",
            "4               0.987              0.987     0.987           0.984              0.984     0.984           0.922              0.922     0.909           0.921              0.922     0.908           0.927              0.925     0.914           0.905              0.925     0.902           0.950              0.925     0.925            0.36              0.566       0.5\n",
            "5               0.967              0.967     0.967           0.960              0.960     0.960           0.857              0.870     0.870           0.854              0.868     0.867           0.871              0.881     0.884           0.822              0.841     0.826           0.925              0.925     0.950            0.52              0.612       0.5\n",
            "6               0.943              0.943     0.943           0.926              0.926     0.926           0.792              0.792     0.792           0.790              0.790     0.790           0.810              0.810     0.810           0.773              0.773     0.773           0.850              0.850     0.850            0.51              0.523       0.5\n",
            "7               0.971              0.971     0.971           0.959              0.959     0.959           0.922              0.922     0.922           0.922              0.923     0.923           0.925              0.923     0.923           0.925              0.947     0.947           0.925              0.900     0.900            0.31              0.528       0.5\n",
            "8               0.937              0.937     0.937           0.914              0.914     0.914           0.844              0.831     0.805           0.844              0.832     0.805           0.842              0.827     0.815           0.865              0.861     0.786           0.821              0.795     0.846            0.60              0.676       0.5\n",
            "9               0.939              0.939     0.939           0.926              0.926     0.926           0.805              0.805     0.792           0.806              0.807     0.794           0.789              0.776     0.771           0.875              0.929     0.871           0.718              0.667     0.692            0.45              0.647       0.5\n",
            "10              0.987              0.987     0.987           0.985              0.985     0.985           0.935              0.935     0.948           0.935              0.935     0.948           0.937              0.935     0.949           0.925              0.947     0.949           0.949              0.923     0.949            0.43              0.599       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.5          4.8\n",
            "True Attack          5.1         34.6\n",
            "Accuracy: 0.871 | Precision(attack): 0.878 | Recall(attack): 0.872 | Specificity(normal): 0.871\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.5          4.8\n",
            "True Attack          4.8         34.9\n",
            "Accuracy: 0.875 | Precision(attack): 0.879 | Recall(attack): 0.879 | Specificity(normal): 0.871\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.6          3.7\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.877 | Precision(attack): 0.902 | Recall(attack): 0.854 | Specificity(normal): 0.901\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (TEXT-ONLY) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.37              0.362       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.30              0.236       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              0.987       1.0             1.0              0.988       1.0             1.0              0.987       1.0             1.0                1.0       1.0             1.0              0.975       1.0            0.64              0.958       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.40              0.396       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.30              0.227       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.41              0.541       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.58              0.572       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.57              0.563       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.36              0.677       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0              1.000       1.0             1.0                1.0       1.0             1.0              1.000       1.0            0.56              0.897       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.999 | Precision(attack): 1.000 | Recall(attack): 0.997 | Specificity(normal): 1.000\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0       1.0           1.000              1.000     1.000            0.51              0.508       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000                1.0       1.0           0.975              0.975     0.975            0.46              0.508       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           1.000              0.974     0.987           1.000              0.975     0.988           1.000              0.974     0.987           1.000                1.0       1.0           1.000              0.950     0.975            0.44              0.588       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0       1.0           1.000              1.000     1.000            0.49              0.508       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.974     0.974           0.988              0.975     0.975           0.987              0.974     0.974           1.000                1.0       1.0           0.975              0.950     0.950            0.42              0.548       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.961     0.987           0.988              0.962     0.988           0.987              0.961     0.987           1.000                1.0       1.0           0.975              0.925     0.975            0.47              0.548       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0       1.0           1.000              1.000     1.000            0.45              0.518       0.5\n",
            "8               0.998              0.998     0.998           0.998              0.998     0.998           0.987              0.987     0.987           0.987              0.987     0.987           0.987              0.987     0.987           1.000                1.0       1.0           0.974              0.974     0.974            0.44              0.532       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           1.000              0.987     0.987           1.000              0.987     0.987           1.000              0.987     0.987           1.000                1.0       1.0           1.000              0.974     0.974            0.46              0.514       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.961     1.000           0.987              0.962     1.000           0.987              0.960     1.000           0.975                1.0       1.0           1.000              0.923     1.000            0.41              0.622       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.991 | Precision(attack): 1.000 | Recall(attack): 0.982 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.994 | Precision(attack): 0.997 | Recall(attack): 0.990 | Specificity(normal): 0.997\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          1.3         38.4\n",
            "Accuracy: 0.983 | Precision(attack): 1.000 | Recall(attack): 0.967 | Specificity(normal): 1.000\n",
            "\n",
            "=== ATTRS-ONLY (TEXT-ONLY) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.30              0.039       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.30              0.150       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.55              0.547       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              0.987     1.000           1.000              0.988     1.000           1.000              0.987     1.000            1.00              1.000      1.00           1.000              0.975     1.000            0.69              1.000       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.30              0.961       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.56              0.554       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.30              0.977       0.5\n",
            "8               0.998              0.998     0.998           0.998              0.998     0.998           0.961              0.974     0.961           0.961              0.974     0.961           0.962              0.974     0.962            0.95              0.974      0.95           0.974              0.974     0.974            0.70              0.991       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.51              0.505       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            1.00              1.000      1.00           1.000              1.000     1.000            0.67              0.669       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.996 | Precision(attack): 0.995 | Recall(attack): 0.997 | Specificity(normal): 0.995\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.996 | Precision(attack): 0.995 | Recall(attack): 0.997 | Specificity(normal): 0.995\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.996 | Precision(attack): 0.997 | Recall(attack): 0.995 | Specificity(normal): 0.997\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (CAT-ONLY precomputed) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.744              0.744     0.744           0.804              0.804     0.804           0.506              0.532     0.506           0.488              0.515     0.488           0.672              0.684     0.672           0.513              0.527     0.513           0.975              0.975     0.975            0.31              0.911       0.5\n",
            "2               0.745              0.745     0.745           0.806              0.806     0.806           0.558              0.818     0.558           0.541              0.812     0.541           0.702              0.848     0.702           0.541              0.750     0.541           1.000              0.975     1.000            0.30              0.910       0.5\n",
            "3               0.731              0.731     0.731           0.791              0.791     0.791           0.519              0.805     0.519           0.501              0.798     0.501           0.678              0.839     0.678           0.520              0.736     0.520           0.975              0.975     0.975            0.30              0.914       0.5\n",
            "4               0.769              0.769     0.769           0.838              0.838     0.838           0.532              0.831     0.532           0.514              0.824     0.514           0.690              0.860     0.690           0.526              0.755     0.526           1.000              1.000     1.000            0.30              0.913       0.5\n",
            "5               0.769              0.769     0.769           0.838              0.838     0.838           0.558              0.844     0.558           0.541              0.838     0.541           0.702              0.870     0.702           0.541              0.769     0.541           1.000              1.000     1.000            0.61              0.909       0.5\n",
            "6               0.734              0.734     0.734           0.785              0.785     0.785           0.532              0.805     0.532           0.514              0.799     0.514           0.690              0.835     0.690           0.526              0.745     0.526           1.000              0.950     1.000            0.43              0.916       0.5\n",
            "7               0.536              0.536     0.536           0.531              0.531     0.531           0.558              0.545     0.545           0.541              0.529     0.528           0.702              0.685     0.690           0.541              0.535     0.534           1.000              0.950     0.975            0.43              0.919       0.5\n",
            "8               0.533              0.533     0.533           0.272              0.272     0.272           0.532              0.532     0.532           0.527              0.527     0.527           0.679              0.673     0.679           0.521              0.521     0.521           0.974              0.949     0.974            0.45              0.916       0.5\n",
            "9               0.714              0.714     0.714           0.774              0.774     0.774           0.481              0.792     0.481           0.474              0.790     0.474           0.649              0.822     0.649           0.493              0.725     0.493           0.949              0.949     0.949            0.30              0.918       0.5\n",
            "10              0.716              0.716     0.716           0.779              0.779     0.779           0.519              0.792     0.519           0.513              0.790     0.513           0.678              0.822     0.678           0.513              0.725     0.513           1.000              0.949     1.000            0.58              0.918       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.6         35.7\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.529 | Precision(attack): 0.523 | Recall(attack): 0.985 | Specificity(normal): 0.043\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.6         35.7\n",
            "True Attack          0.5         39.2\n",
            "Accuracy: 0.530 | Precision(attack): 0.523 | Recall(attack): 0.987 | Specificity(normal): 0.043\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         17.8         19.5\n",
            "True Attack          1.3         38.4\n",
            "Accuracy: 0.730 | Precision(attack): 0.663 | Recall(attack): 0.967 | Specificity(normal): 0.477\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.513              0.513     0.513           0.461              0.461     0.461           0.506              0.506     0.506           0.488              0.488     0.488           0.672              0.672     0.672           0.513              0.513     0.513           0.975              0.975     0.975            0.50              0.920       0.5\n",
            "2               0.549              0.549     0.549           0.556              0.556     0.556           0.519              0.558     0.519           0.500              0.542     0.500           0.684              0.696     0.684           0.519              0.542     0.519           1.000              0.975     1.000            0.60              0.910       0.5\n",
            "3               0.520              0.520     0.520           0.501              0.501     0.501           0.519              0.519     0.519           0.501              0.501     0.501           0.678              0.678     0.678           0.520              0.520     0.520           0.975              0.975     0.975            0.61              0.917       0.5\n",
            "4               0.533              0.533     0.533           0.527              0.527     0.527           0.532              0.532     0.519           0.514              0.514     0.500           0.690              0.690     0.684           0.526              0.526     0.519           1.000              1.000     1.000            0.57              0.916       0.5\n",
            "5               0.548              0.548     0.548           0.554              0.554     0.554           0.571              0.571     0.532           0.554              0.554     0.514           0.708              0.708     0.690           0.548              0.548     0.526           1.000              1.000     1.000            0.70              0.913       0.5\n",
            "6               0.528              0.528     0.528           0.516              0.516     0.516           0.532              0.519     0.519           0.514              0.502     0.500           0.690              0.673     0.684           0.526              0.521     0.519           1.000              0.950     1.000            0.61              0.918       0.5\n",
            "7               0.536              0.536     0.536           0.532              0.532     0.532           0.558              0.545     0.558           0.542              0.529     0.542           0.696              0.685     0.696           0.542              0.535     0.542           0.975              0.950     0.975            0.65              0.874       0.5\n",
            "8               0.514              0.514     0.514           0.516              0.516     0.516           0.532              0.519     0.532           0.527              0.514     0.527           0.673              0.661     0.673           0.521              0.514     0.521           0.949              0.923     0.949            0.64              0.912       0.5\n",
            "9               0.513              0.513     0.513           0.512              0.512     0.512           0.481              0.519     0.481           0.474              0.514     0.474           0.649              0.667     0.649           0.493              0.514     0.493           0.949              0.949     0.949            0.45              0.917       0.5\n",
            "10              0.521              0.521     0.521           0.528              0.528     0.528           0.519              0.532     0.506           0.513              0.527     0.500           0.678              0.673     0.672           0.513              0.521     0.506           1.000              0.949     1.000            0.67              0.919       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.0         36.3\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.519 | Precision(attack): 0.518 | Recall(attack): 0.982 | Specificity(normal): 0.027\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.6         35.7\n",
            "True Attack          0.7         39.0\n",
            "Accuracy: 0.527 | Precision(attack): 0.522 | Recall(attack): 0.982 | Specificity(normal): 0.043\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.7         34.6\n",
            "True Attack          1.4         38.3\n",
            "Accuracy: 0.532 | Precision(attack): 0.525 | Recall(attack): 0.965 | Specificity(normal): 0.072\n",
            "\n",
            "=== ATTRS-ONLY (CAT-ONLY precomputed) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.527              0.527     0.527           0.514              0.514     0.514           0.519              0.532     0.506             0.5              0.515     0.488           0.684              0.684     0.672           0.519              0.527     0.513             1.0              0.975     0.975             0.3               0.68       0.5\n",
            "2               0.541              0.541     0.541           0.541              0.541     0.541           0.519              0.558     0.519             0.5              0.542     0.500           0.684              0.696     0.684           0.519              0.542     0.519             1.0              0.975     1.000             0.3               0.68       0.5\n",
            "3               0.520              0.520     0.520           0.501              0.501     0.501           0.519              0.519     0.519             0.5              0.501     0.501           0.684              0.678     0.678           0.519              0.520     0.520             1.0              0.975     0.975             0.3               0.68       0.5\n",
            "4               0.533              0.533     0.533           0.527              0.527     0.527           0.519              0.545     0.532             0.5              0.527     0.514           0.684              0.696     0.690           0.519              0.533     0.526             1.0              1.000     1.000             0.3               0.68       0.5\n",
            "5               0.548              0.548     0.548           0.554              0.554     0.554           0.519              0.571     0.532             0.5              0.554     0.514           0.684              0.708     0.690           0.519              0.548     0.526             1.0              1.000     1.000             0.3               0.68       0.5\n",
            "6               0.520              0.520     0.520           0.501              0.501     0.501           0.519              0.519     0.519             0.5              0.502     0.502           0.684              0.673     0.673           0.519              0.521     0.521             1.0              0.950     0.950             0.3               0.68       0.5\n",
            "7               0.534              0.534     0.534           0.528              0.528     0.528           0.519              0.545     0.545             0.5              0.528     0.528           0.684              0.690     0.690           0.519              0.534     0.534             1.0              0.975     0.975             0.3               0.68       0.5\n",
            "8               0.500              0.500     0.500           0.488              0.488     0.488           0.506              0.494     0.494             0.5              0.488     0.488           0.672              0.655     0.655           0.506              0.500     0.500             1.0              0.949     0.949             0.3               0.68       0.5\n",
            "9               0.507              0.507     0.507           0.499              0.499     0.499           0.506              0.506     0.481             0.5              0.501     0.474           0.672              0.661     0.649           0.506              0.507     0.493             1.0              0.949     0.949             0.3               0.68       0.5\n",
            "10              0.513              0.513     0.513           0.513              0.513     0.513           0.506              0.506     0.519             0.5              0.500     0.513           0.672              0.672     0.678           0.506              0.506     0.513             1.0              1.000     1.000             0.3               0.40       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          1.0         36.3\n",
            "True Attack          0.9         38.8\n",
            "Accuracy: 0.517 | Precision(attack): 0.517 | Recall(attack): 0.977 | Specificity(normal): 0.027\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          0.0         37.3\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 0.516 | Precision(attack): 0.516 | Recall(attack): 1.000 | Specificity(normal): 0.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          2.1         35.2\n",
            "True Attack          1.0         38.7\n",
            "Accuracy: 0.530 | Precision(attack): 0.524 | Recall(attack): 0.975 | Specificity(normal): 0.056\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets see what features the detector is using per graph"
      ],
      "metadata": {
        "id": "mlcdPxAyMO2V"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, re\n",
        "from scipy.sparse import csr_matrix, hstack, vstack, issparse\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from math import exp\n",
        "\n",
        "# ---- helpers (reuse your earlier ones if already defined) ----\n",
        "def _bags_to_sparse(train_bags, test_bags, topk):\n",
        "    from collections import Counter\n",
        "    from scipy.sparse import csr_matrix, vstack\n",
        "    ctr = Counter()\n",
        "    for d in train_bags:\n",
        "        if d: ctr.update(d)\n",
        "    vocab = [k for k,_ in ctr.most_common(topk)]\n",
        "    index = {k:i for i,k in enumerate(vocab)}\n",
        "    def _encode(bags):\n",
        "        rows = []\n",
        "        for d in bags:\n",
        "            if not d: rows.append(csr_matrix((1, len(vocab)))); continue\n",
        "            idx, val = [], []\n",
        "            for k, v in d.items():\n",
        "                j = index.get(k)\n",
        "                if j is not None:\n",
        "                    idx.append(j); val.append(float(v))\n",
        "            if idx:\n",
        "                rows.append(csr_matrix((val, ([0]*len(idx), idx)), shape=(1, len(vocab))))\n",
        "            else:\n",
        "                rows.append(csr_matrix((1, len(vocab))))\n",
        "        return vstack(rows).tocsr()\n",
        "    return _encode(train_bags), _encode(test_bags), vocab\n",
        "\n",
        "def explain_one_graph(\n",
        "    graph_idx: int,\n",
        "    y: np.ndarray,\n",
        "    texts_series: pd.Series,\n",
        "    X_num_blk,                      # csr or np array, aligned to y\n",
        "    X_nodecat_blk=None,             # precomputed sparse, optional\n",
        "    X_edgecat_blk=None,             # precomputed sparse, optional\n",
        "    node_cat_bags=None,             # list[dict], optional (preferred)\n",
        "    edge_cat_bags=None,             # list[dict], optional (preferred)\n",
        "    topk_node: int = 250,\n",
        "    topk_edge: int = 250,\n",
        "    EDGE_TEXT_MAX_FEATURES: int = 10000,\n",
        "    class_weight = {0:1, 1:2},\n",
        "    need_dense: bool = True,        # LR(lbfgs) needs dense after impute+scale\n",
        "    max_features_text_names: int = 60,  # just for printing cap\n",
        "    top_show: int = 30,             # how many top contributing features to print\n",
        "):\n",
        "    n = len(y)\n",
        "    all_idx = np.arange(n)\n",
        "    tr = all_idx[all_idx != graph_idx]\n",
        "    te = np.array([graph_idx])\n",
        "\n",
        "    # TEXT: fit TF-IDF on train only\n",
        "    tfv = TfidfVectorizer(\n",
        "        max_features=EDGE_TEXT_MAX_FEATURES,\n",
        "        lowercase=True, token_pattern=r\"(?u)\\b\\w+\\b\",\n",
        "        sublinear_tf=True\n",
        "    )\n",
        "    X_text_tr = tfv.fit_transform(texts_series.iloc[tr].fillna(\"\").astype(str).tolist())\n",
        "    X_text_te = tfv.transform(texts_series.iloc[te].fillna(\"\").astype(str).tolist())\n",
        "    text_names = [\"text:\"+t for t in tfv.get_feature_names_out().tolist()]\n",
        "\n",
        "    # CATS: rebuild from bags if provided; else use precomputed blocks with generic names\n",
        "    if (node_cat_bags is not None) and (edge_cat_bags is not None):\n",
        "        node_tr, node_te, node_vocab = _bags_to_sparse([node_cat_bags[i] for i in tr], [node_cat_bags[i] for i in te], topk_node)\n",
        "        edge_tr, edge_te, edge_vocab = _bags_to_sparse([edge_cat_bags[i] for i in tr], [edge_cat_bags[i] for i in te], topk_edge)\n",
        "        node_names = [\"node:\"+k for k in node_vocab]\n",
        "        edge_names = [\"edge:\"+k for k in edge_vocab]\n",
        "    else:\n",
        "        node_tr = X_nodecat_blk[tr] if X_nodecat_blk is not None else csr_matrix((len(tr),0))\n",
        "        node_te = X_nodecat_blk[te] if X_nodecat_blk is not None else csr_matrix((1,0))\n",
        "        edge_tr = X_edgecat_blk[tr] if X_edgecat_blk is not None else csr_matrix((len(tr),0))\n",
        "        edge_te = X_edgecat_blk[te] if X_edgecat_blk is not None else csr_matrix((1,0))\n",
        "        node_names = [f\"nodecat_{i}\" for i in range(node_tr.shape[1])]\n",
        "        edge_names = [f\"edgecat_{i}\" for i in range(edge_tr.shape[1])]\n",
        "\n",
        "    # NUM\n",
        "    def _safe_slice(M, idx):\n",
        "        if issparse(M): return M[idx]\n",
        "        return M[idx] if not hasattr(M, \"iloc\") else M.iloc[idx]\n",
        "    Xn_tr = _safe_slice(X_num_blk, tr)\n",
        "    Xn_te = _safe_slice(X_num_blk, te)\n",
        "    if issparse(Xn_tr) == False and not isinstance(Xn_tr, np.ndarray):\n",
        "        Xn_tr = csr_matrix(np.asarray(Xn_tr))\n",
        "        Xn_te = csr_matrix(np.asarray(Xn_te))\n",
        "    num_names = [f\"num_{i}\" for i in range(Xn_tr.shape[1])]\n",
        "\n",
        "    # STACK\n",
        "    X_tr = hstack([Xn_tr, node_tr, edge_tr, X_text_tr]).tocsr()\n",
        "    X_te = hstack([Xn_te, node_te, edge_te, X_text_te]).tocsr()\n",
        "    base_names = num_names + node_names + edge_names + text_names\n",
        "\n",
        "    # PIPELINE\n",
        "    steps = [\n",
        "        (\"vt\", VarianceThreshold(0.0)),\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\", add_indicator=True)),\n",
        "        (\"sc\", StandardScaler(with_mean=False)),\n",
        "    ]\n",
        "    if need_dense:\n",
        "        steps.append((\"to_dense\", FunctionTransformer(lambda M: M.toarray(), accept_sparse=True)))\n",
        "    clf = LogisticRegression(max_iter=3000, solver=\"lbfgs\", class_weight=class_weight, random_state=42)\n",
        "    steps.append((\"clf\", clf))\n",
        "    pipe = Pipeline(steps)\n",
        "\n",
        "    # FIT on train\n",
        "    pipe.fit(X_tr, y[tr])\n",
        "\n",
        "    # ---- rebuild feature names through the pipeline ----\n",
        "    # 1) VarianceThreshold mask\n",
        "    vt = pipe.named_steps[\"vt\"]\n",
        "    mask_vt = vt.get_support()\n",
        "    names_after_vt = [n for n, keep in zip(base_names, mask_vt) if keep]\n",
        "\n",
        "    # 2) SimpleImputer with indicators\n",
        "    imp = pipe.named_steps[\"imp\"]\n",
        "    # features_ holds indices (wrt input) for which indicator columns were added\n",
        "    ind_idx = getattr(getattr(imp, \"indicator_\", None), \"features_\", np.array([], dtype=int))\n",
        "    names_after_imp = names_after_vt[:]  # originals stay in place\n",
        "    # indicator columns are appended at the end in the same order as features_\n",
        "    indicator_names = [f\"MISS[{names_after_vt[i]}]\" for i in ind_idx if i < len(names_after_vt)]\n",
        "    names_after_imp += indicator_names\n",
        "\n",
        "    # Transform the held-out graph up to the classifier input\n",
        "    Xt = pipe[:-1].transform(X_te)      # (1, d) sparse or dense\n",
        "    Xt = Xt if isinstance(Xt, np.ndarray) else Xt.toarray()\n",
        "    coef = pipe[-1].coef_.ravel()       # (d,)\n",
        "    intercept = float(pipe[-1].intercept_[0])\n",
        "\n",
        "    # sanity check\n",
        "    assert Xt.shape[1] == len(names_after_imp) == len(coef)\n",
        "\n",
        "    # contributions = value * coefficient (logit space)\n",
        "    vals = Xt[0]\n",
        "    contrib = vals * coef\n",
        "    dfc = pd.DataFrame({\n",
        "        \"feature\": names_after_imp,\n",
        "        \"value\": vals,\n",
        "        \"coef\": coef,\n",
        "        \"contrib_logit\": contrib\n",
        "    }).sort_values(\"contrib_logit\", key=lambda s: s.abs(), ascending=False)\n",
        "\n",
        "    logit = intercept + contrib.sum()\n",
        "    prob = 1/(1+exp(-logit))\n",
        "\n",
        "    print(f\"\\n=== Explanation for graph_idx={graph_idx} ===\")\n",
        "    print(f\"logit = intercept({intercept:+.3f}) + sum(contrib) = {logit:+.3f}\")\n",
        "    print(f\"predicted P(attack=1) = {prob:.4f}\\n\")\n",
        "\n",
        "    # Show top contributors (both directions)\n",
        "    print(\"Top positive contributors (push toward class=1):\")\n",
        "    print(dfc[dfc.contrib_logit>0].head(top_show).to_string(index=False))\n",
        "    print(\"\\nTop negative contributors (push toward class=0):\")\n",
        "    print(dfc[dfc.contrib_logit<0].head(top_show).to_string(index=False))\n",
        "\n",
        "    # Optionally: return the full table for further inspection\n",
        "    return dfc, prob, logit, intercept\n",
        "\n",
        "# ------------------- run it -------------------\n",
        "graph_idx = 0  # <-- pick the row to explain\n",
        "df_contrib, prob, logit, intercept = explain_one_graph(\n",
        "    graph_idx=graph_idx,\n",
        "    y=y,\n",
        "    texts_series=texts_series_edges,     # or texts_series if that's your edge text series\n",
        "    X_num_blk=X_num_final[row_indices],  # align to your current slice\n",
        "    # If you have bags, uncomment and pass them to get real token names:\n",
        "    # node_cat_bags=node_cat_bags, edge_cat_bags=edge_cat_bags, topk_node=250, topk_edge=250,\n",
        "    # Otherwise, if you only have precomputed matrices, pass them to see generic names:\n",
        "    # X_nodecat_blk=X_node_cat[row_indices], X_edgecat_blk=X_edge_cat[row_indices],\n",
        "    EDGE_TEXT_MAX_FEATURES=5000,\n",
        "    class_weight={0:1,1:2},\n",
        "    top_show=25\n",
        ")\n"
      ],
      "metadata": {
        "id": "6F3mrazFRAcT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "6e296d2a-1a2b-4eff-853a-d33b46d5402e"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'texts_series_edges' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1593186126.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0mgraph_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtexts_series\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtexts_series_edges\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0;31m# or texts_series if that's your edge text series\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0mX_num_blk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_num_final\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# align to your current slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;31m# If you have bags, uncomment and pass them to get real token names:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'texts_series_edges' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Build the STRUCTURE block to match your earlier topology-based detection ===\n",
        "from scipy.sparse import csr_matrix, hstack, issparse\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "OUT_DIR = Path(\"/content/drive/MyDrive/graph_stats_out\")\n",
        "\n",
        "# --- helper: pick column by exact/substring match (case-insensitive)\n",
        "def _pick(df, *cands):\n",
        "    cols = list(df.columns)\n",
        "    for cand in cands:\n",
        "        cL = cand.lower()\n",
        "        for c in cols:\n",
        "            if c.lower() == cL or cL in c.lower():\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "# 1) Load the exact structural tables you already use\n",
        "ENR = OUT_DIR / \"graph_stats_enriched.csv\"   # falls back to extended if missing\n",
        "EXT = OUT_DIR / \"graph_stats_extended.csv\"\n",
        "GREY = OUT_DIR / \"graph_stats_greysmart.csv\" # your tiny_topo_features pack\n",
        "\n",
        "struct_base = pd.read_csv(ENR if ENR.exists() else EXT)\n",
        "grey_df    = pd.read_csv(GREY) if GREY.exists() else pd.DataFrame()\n",
        "\n",
        "# 2) Keep ONLY the topology metrics you used before (no text fields)\n",
        "metrics_for_corr = [\n",
        "    \"n_nodes\",\"n_edges\",\"simple_edges\",\"parallel_edge_ratio\",\"self_loops\",\n",
        "    \"density_directed\",\"density_undirected\",\"reciprocity\",\"avg_clustering\",\n",
        "    \"diameter_undirected_lcc\",\"wcc_count\",\"scc_count\",\"largest_wcc_nodes\",\n",
        "    \"frac_in_largest_wcc_nodes\",\"in_degree_mean\",\"in_degree_median\",\n",
        "    \"in_degree_max\",\"in_degree_gini\",\"out_degree_mean\",\"out_degree_median\",\n",
        "    \"out_degree_max\",\"out_degree_gini\",\"assort_in_in\",\"assort_out_out\",\n",
        "    \"assort_undirected\",\"kcore_kmax\",\"kcore_kmax_size\",\"triangles\",\n",
        "    \"transitivity\",\"pl_in_R2\",\"pl_out_R2\",\"frac_can_reach_giant\",\n",
        "    \"frac_reachable_from_giant\",\"role_entropy\",\"n_roles\"\n",
        "]\n",
        "\n",
        "# grey-smart set (matches tiny_topo_features; keep those that exist)\n",
        "grey_like = [\n",
        "  \"fanout_ratio\",\"return_ratio\",\"crosstalk_ratio\",\"peer_fraction\",\n",
        "  \"alternation_ratio\",\"lag_median\",\"lag_cv\",\n",
        "  \"spec_has_edge\",\"spec_has_cycle\",\"spec_max_cc\",\"spec_edge_fraction\",\n",
        "  \"eig1\",\"eig2\",\"eig3\"\n",
        "]\n",
        "tri_cols = [c for c in (grey_df.columns if not grey_df.empty else []) if c.startswith(\"tri_\")]\n",
        "\n",
        "# --- resolve key names on each frame\n",
        "file_col_sb   = _pick(struct_base, \"file\", \"path\")\n",
        "bundle_col_sb = _pick(struct_base, \"bundle\")\n",
        "if file_col_sb is None:\n",
        "    raise KeyError(\"Could not find a 'file' (or 'path') column in structural base CSV.\")\n",
        "\n",
        "file_col_g   = _pick(grey_df, \"file\", \"path\") if not grey_df.empty else None\n",
        "bundle_col_g = _pick(grey_df, \"bundle\") if not grey_df.empty else None\n",
        "\n",
        "# 3) Build one structural frame (struct_base [+ grey_df if present])\n",
        "need_cols = [c for c in metrics_for_corr if c in struct_base.columns]\n",
        "id_cols_sb = [file_col_sb] + ([bundle_col_sb] if bundle_col_sb in struct_base.columns else [])\n",
        "struct_frame = struct_base[id_cols_sb + need_cols].copy()\n",
        "\n",
        "if not grey_df.empty:\n",
        "    keep_grey_cols = [c for c in (grey_like + tri_cols) if c in grey_df.columns]\n",
        "    # decide join keys for base×grey\n",
        "    join_left  = [file_col_sb]\n",
        "    join_right = [file_col_g]\n",
        "    if (bundle_col_sb is not None) and (bundle_col_g is not None):\n",
        "        join_left.append(bundle_col_sb)\n",
        "        join_right.append(bundle_col_g)\n",
        "    struct_frame = struct_frame.merge(\n",
        "        grey_df[[file_col_g] + ([bundle_col_g] if bundle_col_g else []) + keep_grey_cols],\n",
        "        left_on=join_left, right_on=join_right, how=\"left\"\n",
        "    )\n",
        "    # after merge, normalize key names back to base's names\n",
        "    rename_map = {}\n",
        "    if file_col_g and file_col_g != file_col_sb and file_col_g in struct_frame.columns:\n",
        "        rename_map[file_col_g] = file_col_sb\n",
        "    if bundle_col_g and bundle_col_sb and bundle_col_g != bundle_col_sb and bundle_col_g in struct_frame.columns:\n",
        "        rename_map[bundle_col_g] = bundle_col_sb\n",
        "    if rename_map:\n",
        "        struct_frame = struct_frame.rename(columns=rename_map)\n",
        "\n",
        "# Align to the exact row order you already use (the `merged` you built earlier)\n",
        "if \"merged\" not in globals():\n",
        "    raise RuntimeError(\"Expected a DataFrame named `merged` defined earlier in the notebook.\")\n",
        "file_col_m   = _pick(merged, \"file\", \"path\")\n",
        "bundle_col_m = _pick(merged, \"bundle\")\n",
        "if file_col_m is None:\n",
        "    raise KeyError(\"`merged` must contain a 'file' (or 'path') column to align rows.\")\n",
        "\n",
        "# choose join keys: always file; include bundle only if both have it\n",
        "align_left_keys  = [file_col_m]\n",
        "align_right_keys = [file_col_sb]\n",
        "if (bundle_col_m is not None) and (bundle_col_sb is not None) and \\\n",
        "   (bundle_col_m in merged.columns) and (bundle_col_sb in struct_frame.columns):\n",
        "    align_left_keys.append(bundle_col_m)\n",
        "    align_right_keys.append(bundle_col_sb)\n",
        "\n",
        "# make sure key names match on right side\n",
        "sf_for_join = struct_frame.copy()\n",
        "if file_col_sb != file_col_m:\n",
        "    sf_for_join = sf_for_join.rename(columns={file_col_sb: file_col_m})\n",
        "if (bundle_col_sb and bundle_col_m and bundle_col_sb != bundle_col_m and bundle_col_sb in sf_for_join.columns):\n",
        "    sf_for_join = sf_for_join.rename(columns={bundle_col_sb: bundle_col_m})\n",
        "\n",
        "struct_aligned = merged[align_left_keys].merge(\n",
        "    sf_for_join, left_on=align_left_keys, right_on=align_left_keys, how=\"left\"\n",
        ")\n",
        "\n",
        "# Build X_struct: numeric only, fill NaNs with 0\n",
        "drop_ids = {file_col_m}\n",
        "if bundle_col_m: drop_ids.add(bundle_col_m)\n",
        "drop_ids.update([\"graph_id\",\"cohort\"])\n",
        "struct_num_cols = [\n",
        "    c for c in struct_aligned.columns\n",
        "    if (c not in drop_ids) and pd.api.types.is_numeric_dtype(struct_aligned[c])\n",
        "]\n",
        "X_struct_blk = csr_matrix(struct_aligned[struct_num_cols].fillna(0.0).values)\n",
        "print(\"[ALIGN] joined on keys:\", align_left_keys)\n",
        "print(\"[STRUCT] columns used:\", len(struct_num_cols))\n",
        "print(\"[STRUCT] shape:\", X_struct_blk.shape)\n",
        "\n",
        "# 4) Build ATTR blocks from your per-graph attributes (NO TEXT)\n",
        "file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "row_indices = np.array([file_to_row[f] for f in merged[file_col_m].tolist()], dtype=int)\n",
        "\n",
        "X_num_blk     = X_num[row_indices]      if X_num.shape[0]     == len(df_num) else csr_matrix((len(merged),0))\n",
        "X_nodecat_blk = X_node_cat[row_indices] if X_node_cat.shape[0]== len(df_num) else csr_matrix((len(merged),0))\n",
        "X_edgecat_blk = X_edge_cat[row_indices] if X_edge_cat.shape[0]== len(df_num) else csr_matrix((len(merged),0))\n",
        "\n",
        "print(\"[ATTR] numeric:\", X_num_blk.shape, \"| node-cats:\", X_nodecat_blk.shape, \"| edge-cats:\", X_edgecat_blk.shape)\n",
        "\n",
        "# 5) Label vector y (attack=1) from `merged`, consistent with your earlier code\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "print(\"[y] size:\", y.size, \"| pos rate:\", y.mean().round(3))\n",
        "\n",
        "# 6) Guarantee: NO TEXT → pass empty strings so TF-IDF has 0 columns inside your evaluator\n",
        "texts_series_empty = pd.Series([\"\"] * len(y))\n",
        "\n",
        "# 7) (Optional) groups if you use group-aware CV\n",
        "groups = globals().get(\"groups\", None)\n",
        "\n",
        "# 8) Run the three regimes with your existing evaluator (no text anywhere)\n",
        "print(\"\\n########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = csr_matrix((len(y),0)),   # no numeric attrs\n",
        "    X_nodecat_blk = csr_matrix((len(y),0)),   # no node cats\n",
        "    X_edgecat_blk = csr_matrix((len(y),0)),   # no edge cats\n",
        "    texts_series  = texts_series_empty,       # kills TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # ONLY topology features\n",
        "    y             = y,\n",
        "    tag           = \"STRUCTURE-ONLY (topology pack; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = None,                     # no topology here\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # add topology\n",
        "    y             = y,\n",
        "    tag           = \"COMBINED (STRUCTURE + ATTRS; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "# 9) Safety assertions so you know it's exactly what you asked for\n",
        "assert X_struct_blk.shape[1] > 0, \"STRUCTURE block is empty; check column names.\"\n",
        "assert all((\"text\" not in c.lower()) and (\"moderator\" not in c.lower()) for c in struct_num_cols), \\\n",
        "       \"Structural set accidentally contains text/moderator fields.\"\n",
        "print(\"\\n[OK] Ran exactly with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\")\n"
      ],
      "metadata": {
        "id": "2amjAme2RAiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f1a409ed-154f-4fb9-a843-f36a83a006e7"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ALIGN] joined on keys: ['file', 'bundle_x']\n",
            "[STRUCT] columns used: 65\n",
            "[STRUCT] shape: (770, 65)\n",
            "[ATTR] numeric: (770, 282) | node-cats: (770, 250) | edge-cats: (770, 250)\n",
            "[y] size: 770 | pos rate: 0.516\n",
            "\n",
            "########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  STRUCTURE-ONLY (topology pack; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.896              0.896     0.896           0.819              0.819     0.819           0.766              0.688     0.519           0.766              0.684     0.506           0.775              0.727     0.648           0.775              0.667     0.523           0.775              0.800     0.850            0.69              0.643       0.5\n",
            "2               0.924              0.924     0.924           0.895              0.895     0.895           0.779              0.688     0.636           0.775              0.680     0.623           0.805              0.750     0.736           0.745              0.643     0.591           0.875              0.900     0.975            0.70              0.634       0.5\n",
            "3               0.844              0.844     0.844           0.786              0.786     0.786           0.662              0.688     0.558           0.662              0.684     0.543           0.675              0.727     0.691           0.675              0.667     0.543           0.675              0.800     0.950            0.70              0.641       0.5\n",
            "4               0.917              0.917     0.917           0.858              0.858     0.858           0.740              0.662     0.584           0.737              0.656     0.570           0.767              0.717     0.704           0.717              0.635     0.559           0.825              0.825     0.950            0.69              0.635       0.5\n",
            "5               0.935              0.935     0.935           0.907              0.907     0.907           0.792              0.727     0.649           0.790              0.721     0.635           0.810              0.769     0.748           0.773              0.686     0.597           0.850              0.875     1.000            0.69              0.642       0.5\n",
            "6               0.895              0.895     0.895           0.861              0.861     0.861           0.818              0.701     0.584           0.816              0.693     0.570           0.833              0.758     0.704           0.795              0.655     0.559           0.875              0.900     0.950            0.69              0.632       0.5\n",
            "7               0.930              0.930     0.930           0.891              0.891     0.891           0.818              0.766     0.623           0.819              0.762     0.611           0.821              0.795     0.718           0.842              0.729     0.587           0.800              0.875     0.925            0.69              0.637       0.5\n",
            "8               0.868              0.868     0.868           0.812              0.812     0.812           0.740              0.701     0.610           0.741              0.700     0.606           0.737              0.736     0.712           0.757              0.667     0.569           0.718              0.821     0.949            0.70              0.645       0.5\n",
            "9               0.821              0.821     0.821           0.758              0.758     0.758           0.675              0.662     0.545           0.675              0.661     0.540           0.691              0.705     0.673           0.667              0.633     0.529           0.718              0.795     0.923            0.67              0.644       0.5\n",
            "10              0.931              0.931     0.931           0.897              0.897     0.897           0.792              0.688     0.558           0.791              0.686     0.553           0.805              0.745     0.691           0.767              0.636     0.535           0.846              0.897     0.974            0.70              0.646       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          7.7         29.6\n",
            "True Attack          2.2         37.5\n",
            "Accuracy: 0.587 | Precision(attack): 0.559 | Recall(attack): 0.945 | Specificity(normal): 0.206\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.8         10.5\n",
            "True Attack          8.1         31.6\n",
            "Accuracy: 0.758 | Precision(attack): 0.751 | Recall(attack): 0.796 | Specificity(normal): 0.718\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         20.0         17.3\n",
            "True Attack          6.0         33.7\n",
            "Accuracy: 0.697 | Precision(attack): 0.661 | Recall(attack): 0.849 | Specificity(normal): 0.536\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.930              0.930     0.930           0.908              0.908     0.908           0.831              0.779     0.831           0.834              0.773     0.834           0.822              0.813     0.822           0.909              0.725     0.909           0.750              0.925     0.750            0.49              0.216       0.5\n",
            "2               0.925              0.925     0.925           0.891              0.891     0.891           0.805              0.701     0.857           0.801              0.693     0.857           0.828              0.758     0.861           0.766              0.655     0.872           0.900              0.900     0.850            0.39              0.204       0.5\n",
            "3               0.885              0.885     0.885           0.833              0.833     0.833           0.779              0.701     0.792           0.781              0.700     0.797           0.773              0.716     0.771           0.829              0.707     0.900           0.725              0.725     0.675            0.40              0.224       0.5\n",
            "4               0.920              0.920     0.920           0.874              0.874     0.874           0.805              0.662     0.831           0.803              0.655     0.832           0.819              0.723     0.831           0.791              0.630     0.865           0.850              0.850     0.800            0.43              0.290       0.5\n",
            "5               0.880              0.880     0.880           0.829              0.829     0.829           0.714              0.662     0.740           0.712              0.656     0.744           0.738              0.717     0.722           0.705              0.635     0.812           0.775              0.825     0.650            0.36              0.202       0.5\n",
            "6               0.859              0.859     0.859           0.825              0.825     0.825           0.701              0.766     0.714           0.706              0.761     0.718           0.667              0.800     0.694           0.793              0.720     0.781           0.575              0.900     0.625            0.60              0.208       0.5\n",
            "7               0.924              0.924     0.924           0.895              0.895     0.895           0.792              0.805     0.805           0.795              0.800     0.808           0.784              0.831     0.795           0.853              0.755     0.879           0.725              0.925     0.725            0.45              0.246       0.5\n",
            "8               0.889              0.889     0.889           0.861              0.861     0.861           0.766              0.714     0.766           0.768              0.712     0.768           0.727              0.761     0.735           0.889              0.660     0.862           0.615              0.897     0.641            0.68              0.202       0.5\n",
            "9               0.803              0.803     0.803           0.734              0.734     0.734           0.675              0.623     0.688           0.676              0.622     0.690           0.658              0.659     0.657           0.706              0.609     0.742           0.615              0.718     0.590            0.45              0.246       0.5\n",
            "10              0.919              0.919     0.919           0.882              0.882     0.882           0.818              0.727     0.805           0.818              0.725     0.805           0.816              0.764     0.805           0.838              0.680     0.816           0.795              0.872     0.795            0.53              0.234       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack         11.5         28.2\n",
            "Accuracy: 0.783 | Precision(attack): 0.844 | Recall(attack): 0.710 | Specificity(normal): 0.861\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.1          7.2\n",
            "True Attack         10.6         29.1\n",
            "Accuracy: 0.769 | Precision(attack): 0.802 | Recall(attack): 0.733 | Specificity(normal): 0.807\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         21.1         16.2\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.714 | Precision(attack): 0.677 | Recall(attack): 0.854 | Specificity(normal): 0.566\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.918              0.918     0.918           0.895              0.895     0.895           0.792              0.727     0.805           0.796              0.718     0.805           0.778              0.784     0.810           0.875              0.667     0.821           0.700              0.950     0.800            0.70              0.271       0.5\n",
            "2               0.924              0.924     0.924           0.893              0.893     0.893           0.779              0.662     0.779           0.776              0.652     0.776           0.800              0.740     0.800           0.756              0.617     0.756           0.850              0.925     0.850            0.52              0.204       0.5\n",
            "3               0.894              0.894     0.894           0.848              0.848     0.848           0.792              0.727     0.779           0.797              0.724     0.781           0.771              0.753     0.773           0.900              0.711     0.829           0.675              0.800     0.725            0.70              0.261       0.5\n",
            "4               0.900              0.900     0.900           0.839              0.839     0.839           0.753              0.623     0.714           0.752              0.613     0.711           0.765              0.707     0.744           0.756              0.593     0.696           0.775              0.875     0.800            0.64              0.294       0.5\n",
            "5               0.867              0.867     0.867           0.797              0.797     0.797           0.766              0.649     0.675           0.770              0.642     0.673           0.750              0.710     0.699           0.844              0.623     0.674           0.675              0.825     0.725            0.68              0.274       0.5\n",
            "6               0.875              0.875     0.875           0.843              0.843     0.843           0.740              0.727     0.727           0.745              0.719     0.725           0.714              0.779     0.747           0.833              0.673     0.721           0.625              0.925     0.775            0.67              0.276       0.5\n",
            "7               0.890              0.890     0.890           0.849              0.849     0.849           0.753              0.727     0.714           0.756              0.722     0.713           0.740              0.764     0.732           0.818              0.694     0.714           0.675              0.850     0.750            0.64              0.333       0.5\n",
            "8               0.872              0.872     0.872           0.828              0.828     0.828           0.727              0.701     0.740           0.728              0.699     0.741           0.712              0.742     0.730           0.765              0.660     0.771           0.667              0.846     0.692            0.55              0.263       0.5\n",
            "9               0.804              0.804     0.804           0.743              0.743     0.743           0.649              0.610     0.662           0.650              0.609     0.664           0.630              0.643     0.629           0.676              0.600     0.710           0.590              0.692     0.564            0.46              0.277       0.5\n",
            "10              0.924              0.924     0.924           0.893              0.893     0.893           0.766              0.662     0.740           0.765              0.659     0.739           0.786              0.729     0.767           0.733              0.614     0.702           0.846              0.897     0.846            0.53              0.341       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          9.8         29.9\n",
            "Accuracy: 0.734 | Precision(attack): 0.736 | Recall(attack): 0.753 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         29.8          7.5\n",
            "True Attack         11.6         28.1\n",
            "Accuracy: 0.752 | Precision(attack): 0.789 | Recall(attack): 0.708 | Specificity(normal): 0.799\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.4         18.9\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.682 | Precision(attack): 0.643 | Recall(attack): 0.859 | Specificity(normal): 0.493\n",
            "\n",
            "########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.939              0.939     0.939           0.916              0.916     0.916           0.766              0.831     0.714           0.760              0.833     0.705           0.804              0.827     0.776           0.712              0.886     0.655           0.925              0.775     0.950            0.67              0.935       0.5\n",
            "2               0.920              0.920     0.920           0.919              0.919     0.919           0.844              0.844     0.766           0.840              0.844     0.758           0.864              0.850     0.812           0.792              0.850     0.696           0.950              0.850     0.975            0.70              0.963       0.5\n",
            "3               0.970              0.970     0.970           0.961              0.961     0.961           0.818              0.870     0.740           0.812              0.871     0.730           0.848              0.872     0.800           0.750              0.895     0.667           0.975              0.850     1.000            0.70              0.887       0.5\n",
            "4               0.994              0.994     0.994           0.993              0.993     0.993           0.883              0.948     0.779           0.878              0.949     0.770           0.899              0.949     0.825           0.816              0.974     0.702           1.000              0.925     1.000            0.70              0.883       0.5\n",
            "5               0.812              0.812     0.812           0.864              0.864     0.864           0.727              0.844     0.675           0.720              0.844     0.665           0.774              0.850     0.747           0.679              0.850     0.627           0.900              0.850     0.925            0.70              0.885       0.5\n",
            "6               0.943              0.943     0.943           0.932              0.932     0.932           0.766              0.831     0.766           0.759              0.829     0.757           0.809              0.843     0.816           0.704              0.814     0.690           0.950              0.875     1.000            0.70              0.907       0.5\n",
            "7               0.956              0.956     0.956           0.932              0.932     0.932           0.883              0.896     0.792           0.883              0.899     0.788           0.886              0.892     0.818           0.897              0.971     0.750           0.875              0.825     0.900            0.70              0.901       0.5\n",
            "8               0.942              0.942     0.942           0.916              0.916     0.916           0.818              0.857     0.701           0.817              0.858     0.698           0.833              0.845     0.763           0.778              0.938     0.638           0.897              0.769     0.949            0.69              0.915       0.5\n",
            "9               0.820              0.820     0.820           0.850              0.850     0.850           0.779              0.792     0.701           0.778              0.793     0.699           0.800              0.784     0.753           0.739              0.829     0.648           0.872              0.744     0.897            0.69              0.946       0.5\n",
            "10              0.966              0.966     0.966           0.958              0.958     0.958           0.831              0.883     0.727           0.830              0.883     0.724           0.851              0.883     0.784           0.771              0.895     0.655           0.949              0.872     0.974            0.70              0.906       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.7         18.6\n",
            "True Attack          1.7         38.0\n",
            "Accuracy: 0.736 | Precision(attack): 0.671 | Recall(attack): 0.957 | Specificity(normal): 0.501\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         25.6         11.7\n",
            "True Attack          2.8         36.9\n",
            "Accuracy: 0.812 | Precision(attack): 0.759 | Recall(attack): 0.929 | Specificity(normal): 0.686\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.1          4.2\n",
            "True Attack          6.6         33.1\n",
            "Accuracy: 0.860 | Precision(attack): 0.887 | Recall(attack): 0.834 | Specificity(normal): 0.887\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.162       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.144       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.148       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.096       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.128       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.132       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.126       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.112       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.178       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.102       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987             1.0              1.000       1.0           0.975              0.975     0.975            0.50              0.494       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              1.000     0.987           0.988              1.000     0.988           0.987              1.000     0.987             1.0              1.000       1.0           0.975              1.000     0.975            0.30              0.077       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.059       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.973       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.34              0.336       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.970       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              0.987     1.000           1.000              0.986     1.000           1.000              0.988     1.000             1.0              0.976       1.0           1.000              1.000     1.000            0.30              0.060       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.984       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision(attack): 0.997 | Recall(attack): 0.997 | Specificity(normal): 0.997\n",
            "\n",
            "########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  COMBINED (STRUCTURE + ATTRS; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.941              0.941     0.941           0.922              0.922     0.922           0.779              0.857     0.714           0.774              0.860     0.704           0.809              0.849     0.780           0.735              0.939     0.650           0.900              0.775     0.975            0.68              0.940       0.5\n",
            "2               0.925              0.925     0.925           0.923              0.923     0.923           0.844              0.857     0.753           0.840              0.857     0.745           0.864              0.861     0.800           0.792              0.872     0.691           0.950              0.850     0.950            0.68              0.964       0.5\n",
            "3               0.969              0.969     0.969           0.956              0.956     0.956           0.779              0.883     0.740           0.772              0.884     0.731           0.817              0.883     0.796           0.717              0.919     0.672           0.950              0.850     0.975            0.69              0.894       0.5\n",
            "4               0.989              0.989     0.989           0.986              0.986     0.986           0.844              0.961     0.766           0.838              0.961     0.757           0.870              0.962     0.816           0.769              0.974     0.690           1.000              0.950     1.000            0.70              0.880       0.5\n",
            "5               0.787              0.787     0.787           0.841              0.841     0.841           0.727              0.805     0.636           0.720              0.803     0.625           0.774              0.819     0.725           0.679              0.791     0.597           0.900              0.850     0.925            0.70              0.886       0.5\n",
            "6               0.945              0.945     0.945           0.932              0.932     0.932           0.766              0.831     0.753           0.759              0.831     0.743           0.809              0.835     0.808           0.704              0.846     0.678           0.950              0.825     1.000            0.70              0.920       0.5\n",
            "7               0.958              0.958     0.958           0.935              0.935     0.935           0.883              0.896     0.805           0.883              0.899     0.801           0.886              0.892     0.828           0.897              0.971     0.766           0.875              0.825     0.900            0.70              0.895       0.5\n",
            "8               0.956              0.956     0.956           0.943              0.943     0.943           0.818              0.870     0.701           0.817              0.871     0.698           0.837              0.865     0.768           0.766              0.914     0.633           0.923              0.821     0.974            0.70              0.889       0.5\n",
            "9               0.824              0.824     0.824           0.836              0.836     0.836           0.714              0.792     0.688           0.712              0.793     0.686           0.756              0.789     0.745           0.667              0.811     0.636           0.872              0.769     0.897            0.69              0.933       0.5\n",
            "10              0.964              0.964     0.964           0.956              0.956     0.956           0.831              0.870     0.727           0.830              0.870     0.724           0.851              0.872     0.784           0.771              0.872     0.655           0.949              0.872     0.974            0.69              0.900       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.1         19.2\n",
            "True Attack          1.7         38.0\n",
            "Accuracy: 0.729 | Precision(attack): 0.664 | Recall(attack): 0.957 | Specificity(normal): 0.485\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         24.7         12.6\n",
            "True Attack          2.9         36.8\n",
            "Accuracy: 0.799 | Precision(attack): 0.745 | Recall(attack): 0.927 | Specificity(normal): 0.662\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.1          4.2\n",
            "True Attack          6.4         33.3\n",
            "Accuracy: 0.862 | Precision(attack): 0.888 | Recall(attack): 0.839 | Specificity(normal): 0.887\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.128       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.168       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.136       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.076       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.130       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.154       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.132       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.126       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.160       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.136       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987             1.0              1.000       1.0           0.975              0.975     0.975            0.49              0.483       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              1.000     0.987           0.988              1.000     0.988           0.987              1.000     0.987             1.0              1.000       1.0           0.975              1.000     0.975            0.30              0.059       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.066       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.973       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.34              0.336       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.967       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              0.987     1.000           1.000              0.986     1.000           1.000              0.988     1.000             1.0              0.976       1.0           1.000              1.000     1.000            0.30              0.072       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.985       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision(attack): 0.997 | Recall(attack): 0.997 | Specificity(normal): 0.997\n",
            "\n",
            "[OK] Ran exactly with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CLEAN, LEAK-GUARDED EVAL\n",
        "# =========================\n",
        "\n",
        "# ---- Config toggles (set True for sanity passes) ----\n",
        "_FORCE_ZERO_CATS = True      # True → Force node/edge cat blocks to 0 columns (to isolate cat-bag leakage)\n",
        "_DO_ISOLATION_TEST = False    # True → Also run NUM-only and CATS-only isolation checks\n",
        "\n",
        "# ---- 0) Fix bundle_x/bundle_y if they slipped in\n",
        "if \"bundle_x\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_x\": \"bundle\"})\n",
        "if \"bundle_y\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_y\": \"bundle\"})\n",
        "\n",
        "# ---- 1) Define y EARLY and ONCE (aligned to merged)\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "print(\"[y] size:\", y.size, \"| pos rate:\", y.mean().round(3))\n",
        "\n",
        "# ---- 2) Transparent name-guard on df_num (drop only true label-like cols)\n",
        "LEAKY_NAME_SUBSTR = (\"cohort\", \"attack\", \"label\", \"target\", \"y_\", \"is_attack\", \"fold\", \"split\")\n",
        "if \"df_num\" in globals():\n",
        "    suspicious = []\n",
        "    for c in df_num.columns:\n",
        "        hits = [s for s in LEAKY_NAME_SUBSTR if s in c.lower()]\n",
        "        if hits:\n",
        "            suspicious.append((c, hits))\n",
        "    if suspicious:\n",
        "        print(\"[LEAK-GUARD] Name matches (column -> substrings):\")\n",
        "        for c, hits in suspicious:\n",
        "            print(\"   \", c, \"->\", hits)\n",
        "        to_drop = [c for c, _ in suspicious]\n",
        "        print(\"[LEAK-GUARD] Dropping numeric columns by name:\", to_drop)\n",
        "        df_num = df_num.drop(columns=to_drop, errors=\"ignore\")\n",
        "else:\n",
        "    raise RuntimeError(\"df_num is not defined in the environment.\")\n",
        "\n",
        "# ---- 3) Rebuild numeric ATTR matrix strictly from df_num (no labels)\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "num_cols_clean = [c for c in df_num.columns\n",
        "                  if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                  and pd.api.types.is_numeric_dtype(df_num[c])]\n",
        "X_num = csr_matrix(df_num[num_cols_clean].fillna(0.0).values) if num_cols_clean else csr_matrix((len(df_num),0))\n",
        "\n",
        "# ---- 4) Align all ATTR blocks to merged row order\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "\n",
        "# If any file is missing from map, you'll get a KeyError; better to expose it clearly:\n",
        "merged_files = merged[file_col_m].tolist()\n",
        "try:\n",
        "    row_indices = np.array([file_to_row[f] for f in merged_files], dtype=int)\n",
        "except KeyError as e:\n",
        "    missing = [f for f in merged_files if f not in file_to_row]\n",
        "    raise KeyError(f\"[ALIGN] Some files in `merged` are not in `df_num` (first few): {missing[:5]}\") from e\n",
        "\n",
        "# Optional precomputed matrices may exist; if not, substitute empty\n",
        "if \"X_node_cat\" in globals() and X_node_cat.shape[0] == len(df_num):\n",
        "    X_nodecat_blk = X_node_cat[row_indices]\n",
        "else:\n",
        "    X_nodecat_blk = csr_matrix((len(merged), 0))\n",
        "\n",
        "if \"X_edge_cat\" in globals() and X_edge_cat.shape[0] == len(df_num):\n",
        "    X_edgecat_blk = X_edge_cat[row_indices]\n",
        "else:\n",
        "    X_edgecat_blk = csr_matrix((len(merged), 0))\n",
        "\n",
        "X_num_blk = X_num[row_indices] if \"X_num\" in globals() and X_num.shape[0] == len(df_num) else csr_matrix((len(merged),0))\n",
        "print(\"[ATTR-ALIGNED] numeric:\", X_num_blk.shape, \"| node-cats:\", X_nodecat_blk.shape, \"| edge-cats:\", X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 5) Optionally kill cat bags to prove where the leak is\n",
        "if _FORCE_ZERO_CATS:\n",
        "    X_nodecat_blk = csr_matrix((len(y), 0))\n",
        "    X_edgecat_blk = csr_matrix((len(y), 0))\n",
        "    print(\"[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\")\n",
        "\n",
        "# ---- 6) Perfect-leak detector (works on sparse), now that y is properly defined\n",
        "def _drop_perfect_leaks(X, y_vec, name, max_to_print=20):\n",
        "    \"\"\"Drop columns that equal y or 1-y exactly (binary sparse-safe).\"\"\"\n",
        "    from scipy.sparse import issparse\n",
        "    if X.shape[1] == 0:\n",
        "        return X, []\n",
        "    leaks = []\n",
        "    if issparse(X):\n",
        "        Xcsr = X.tocsr()\n",
        "        Xcsc = Xcsr.tocsc()\n",
        "        y_vec = y_vec.astype(int)\n",
        "        y_comp = 1 - y_vec\n",
        "        sum_y  = int(y_vec.sum())\n",
        "        sum_yc = int(y_comp.sum())\n",
        "        idx_y  = np.where(y_vec == 1)[0]\n",
        "        idx_ny = np.where(y_comp == 1)[0]\n",
        "        for j in range(Xcsc.shape[1]):\n",
        "            col = Xcsc[:, j]\n",
        "            nnz = col.nnz\n",
        "            if nnz == 0:\n",
        "                continue\n",
        "            # candidate binary?\n",
        "            if not np.all((col.data == 0) | (col.data == 1)):\n",
        "                continue\n",
        "            col_sum = int(col.sum())\n",
        "            rows = col.indices  # positions of 1s\n",
        "            # equal to y?\n",
        "            if col_sum == sum_y and np.array_equal(np.sort(rows), idx_y):\n",
        "                leaks.append(j)\n",
        "                continue\n",
        "            # equal to 1-y?\n",
        "            if col_sum == sum_yc and np.array_equal(np.sort(rows), idx_ny):\n",
        "                leaks.append(j)\n",
        "                continue\n",
        "        if leaks:\n",
        "            print(f\"[LEAK-DETECT] {name}: dropping {len(leaks)} perfectly predictive columns (up to {max_to_print}): {leaks[:max_to_print]}\")\n",
        "            keep = np.setdiff1d(np.arange(Xcsc.shape[1]), np.array(leaks, dtype=int))\n",
        "            return Xcsc[:, keep].tocsr(), leaks\n",
        "        return Xcsr, []\n",
        "    else:\n",
        "        # Dense path (rare in this pipeline)\n",
        "        y_vec = y_vec.astype(int)\n",
        "        y_comp = 1 - y_vec\n",
        "        for j in range(X.shape[1]):\n",
        "            col = X[:, j]\n",
        "            uniq = np.unique(col)\n",
        "            if uniq.size > 3:\n",
        "                continue\n",
        "            if np.array_equal(col, y_vec) or np.array_equal(col, y_comp):\n",
        "                leaks.append(j)\n",
        "        if leaks:\n",
        "            print(f\"[LEAK-DETECT] {name}: dropping {len(leaks)} perfectly predictive columns (up to {max_to_print}): {leaks[:max_to_print]}\")\n",
        "            keep = np.setdiff1d(np.arange(X.shape[1]), np.array(leaks, dtype=int))\n",
        "            return X[:, keep], leaks\n",
        "        return X, []\n",
        "\n",
        "X_num_blk,     num_leaks  = _drop_perfect_leaks(X_num_blk,     y, \"X_num\")\n",
        "X_nodecat_blk, node_leaks = _drop_perfect_leaks(X_nodecat_blk, y, \"X_node_cat\")\n",
        "X_edgecat_blk, edge_leaks = _drop_perfect_leaks(X_edgecat_blk, y, \"X_edge_cat\")\n",
        "print(\"[LEAK-POST] shapes:\", X_num_blk.shape, X_nodecat_blk.shape, X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 7) Prepare evaluator inputs (NO TEXT)\n",
        "texts_series_empty = pd.Series([\"\"] * len(y))\n",
        "groups = globals().get(\"groups\", None)\n",
        "\n",
        "# ---- 8) Run the requested regimes\n",
        "print(\"\\n########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = csr_matrix((len(y),0)),   # no numeric attrs\n",
        "    X_nodecat_blk = csr_matrix((len(y),0)),   # no node cats\n",
        "    X_edgecat_blk = csr_matrix((len(y),0)),   # no edge cats\n",
        "    texts_series  = texts_series_empty,       # kills TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # ONLY topology features\n",
        "    y             = y,\n",
        "    tag           = \"STRUCTURE-ONLY (topology pack; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = None,                     # no topology here\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # add topology\n",
        "    y             = y,\n",
        "    tag           = \"COMBINED (STRUCTURE + ATTRS; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "# ---- 9) Safety assertions\n",
        "assert X_struct_blk.shape[1] > 0, \"STRUCTURE block is empty; check column names.\"\n",
        "if \"struct_num_cols\" in globals():\n",
        "    assert all((\"text\" not in c.lower()) and (\"moderator\" not in c.lower()) for c in struct_num_cols), \\\n",
        "        \"Structural set accidentally contains text/moderator fields.\"\n",
        "\n",
        "print(\"\\n[OK] Ran with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\")\n",
        "\n",
        "# ---- 10) Optional isolation tests\n",
        "if _DO_ISOLATION_TEST:\n",
        "    print(\"\\n########## NUM-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=X_num_blk, X_nodecat_blk=csr_matrix((len(y),0)), X_edgecat_blk=csr_matrix((len(y),0)),\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"NUM-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n",
        "\n",
        "    print(\"\\n########## CATS-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=csr_matrix((len(y),0)), X_nodecat_blk=X_nodecat_blk, X_edgecat_blk=X_edgecat_blk,\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"CATS-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BEdMnwnucn8X",
        "outputId": "9444bde8-0082-44c1-eae7-f89665f13cf7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[y] size: 770 | pos rate: 0.516\n",
            "[LEAK-GUARD] Name matches (column -> substrings):\n",
            "    node_dns_query_count__mean -> ['y_']\n",
            "    node_dns_query_count__std -> ['y_']\n",
            "    node_dns_query_count__min -> ['y_']\n",
            "    node_dns_query_count__max -> ['y_']\n",
            "    node_dns_query_count__sum -> ['y_']\n",
            "    node_dns_query_count__count -> ['y_']\n",
            "[LEAK-GUARD] Dropping numeric columns by name: ['node_dns_query_count__mean', 'node_dns_query_count__std', 'node_dns_query_count__min', 'node_dns_query_count__max', 'node_dns_query_count__sum', 'node_dns_query_count__count']\n",
            "[ATTR-ALIGNED] numeric: (770, 276) | node-cats: (770, 250) | edge-cats: (770, 250)\n",
            "[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\n",
            "[LEAK-POST] shapes: (770, 276) (770, 0) (770, 0)\n",
            "\n",
            "########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  STRUCTURE-ONLY (topology pack; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.896              0.896     0.896           0.819              0.819     0.819           0.766              0.688     0.519           0.766              0.684     0.506           0.775              0.727     0.648           0.775              0.667     0.523           0.775              0.800     0.850            0.69              0.643       0.5\n",
            "2               0.924              0.924     0.924           0.895              0.895     0.895           0.779              0.688     0.636           0.775              0.680     0.623           0.805              0.750     0.736           0.745              0.643     0.591           0.875              0.900     0.975            0.70              0.634       0.5\n",
            "3               0.844              0.844     0.844           0.786              0.786     0.786           0.662              0.688     0.558           0.662              0.684     0.543           0.675              0.727     0.691           0.675              0.667     0.543           0.675              0.800     0.950            0.70              0.641       0.5\n",
            "4               0.917              0.917     0.917           0.858              0.858     0.858           0.740              0.662     0.584           0.737              0.656     0.570           0.767              0.717     0.704           0.717              0.635     0.559           0.825              0.825     0.950            0.69              0.635       0.5\n",
            "5               0.935              0.935     0.935           0.907              0.907     0.907           0.792              0.727     0.649           0.790              0.721     0.635           0.810              0.769     0.748           0.773              0.686     0.597           0.850              0.875     1.000            0.69              0.642       0.5\n",
            "6               0.895              0.895     0.895           0.861              0.861     0.861           0.818              0.701     0.584           0.816              0.693     0.570           0.833              0.758     0.704           0.795              0.655     0.559           0.875              0.900     0.950            0.69              0.632       0.5\n",
            "7               0.930              0.930     0.930           0.891              0.891     0.891           0.818              0.766     0.623           0.819              0.762     0.611           0.821              0.795     0.718           0.842              0.729     0.587           0.800              0.875     0.925            0.69              0.637       0.5\n",
            "8               0.868              0.868     0.868           0.812              0.812     0.812           0.740              0.701     0.610           0.741              0.700     0.606           0.737              0.736     0.712           0.757              0.667     0.569           0.718              0.821     0.949            0.70              0.645       0.5\n",
            "9               0.821              0.821     0.821           0.758              0.758     0.758           0.675              0.662     0.545           0.675              0.661     0.540           0.691              0.705     0.673           0.667              0.633     0.529           0.718              0.795     0.923            0.67              0.644       0.5\n",
            "10              0.931              0.931     0.931           0.897              0.897     0.897           0.792              0.688     0.558           0.791              0.686     0.553           0.805              0.745     0.691           0.767              0.636     0.535           0.846              0.897     0.974            0.70              0.646       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          7.7         29.6\n",
            "True Attack          2.2         37.5\n",
            "Accuracy: 0.587 | Precision(attack): 0.559 | Recall(attack): 0.945 | Specificity(normal): 0.206\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.8         10.5\n",
            "True Attack          8.1         31.6\n",
            "Accuracy: 0.758 | Precision(attack): 0.751 | Recall(attack): 0.796 | Specificity(normal): 0.718\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         20.0         17.3\n",
            "True Attack          6.0         33.7\n",
            "Accuracy: 0.697 | Precision(attack): 0.661 | Recall(attack): 0.849 | Specificity(normal): 0.536\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.930              0.930     0.930           0.908              0.908     0.908           0.831              0.779     0.831           0.834              0.773     0.834           0.822              0.813     0.822           0.909              0.725     0.909           0.750              0.925     0.750            0.49              0.216       0.5\n",
            "2               0.925              0.925     0.925           0.891              0.891     0.891           0.805              0.701     0.857           0.801              0.693     0.857           0.828              0.758     0.861           0.766              0.655     0.872           0.900              0.900     0.850            0.39              0.204       0.5\n",
            "3               0.885              0.885     0.885           0.833              0.833     0.833           0.779              0.701     0.792           0.781              0.700     0.797           0.773              0.716     0.771           0.829              0.707     0.900           0.725              0.725     0.675            0.40              0.224       0.5\n",
            "4               0.920              0.920     0.920           0.874              0.874     0.874           0.805              0.662     0.831           0.803              0.655     0.832           0.819              0.723     0.831           0.791              0.630     0.865           0.850              0.850     0.800            0.43              0.290       0.5\n",
            "5               0.880              0.880     0.880           0.829              0.829     0.829           0.714              0.662     0.740           0.712              0.656     0.744           0.738              0.717     0.722           0.705              0.635     0.812           0.775              0.825     0.650            0.36              0.202       0.5\n",
            "6               0.859              0.859     0.859           0.825              0.825     0.825           0.701              0.766     0.714           0.706              0.761     0.718           0.667              0.800     0.694           0.793              0.720     0.781           0.575              0.900     0.625            0.60              0.208       0.5\n",
            "7               0.924              0.924     0.924           0.895              0.895     0.895           0.792              0.805     0.805           0.795              0.800     0.808           0.784              0.831     0.795           0.853              0.755     0.879           0.725              0.925     0.725            0.45              0.246       0.5\n",
            "8               0.889              0.889     0.889           0.861              0.861     0.861           0.766              0.714     0.766           0.768              0.712     0.768           0.727              0.761     0.735           0.889              0.660     0.862           0.615              0.897     0.641            0.68              0.202       0.5\n",
            "9               0.803              0.803     0.803           0.734              0.734     0.734           0.675              0.623     0.688           0.676              0.622     0.690           0.658              0.659     0.657           0.706              0.609     0.742           0.615              0.718     0.590            0.45              0.246       0.5\n",
            "10              0.919              0.919     0.919           0.882              0.882     0.882           0.818              0.727     0.805           0.818              0.725     0.805           0.816              0.764     0.805           0.838              0.680     0.816           0.795              0.872     0.795            0.53              0.234       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack         11.5         28.2\n",
            "Accuracy: 0.783 | Precision(attack): 0.844 | Recall(attack): 0.710 | Specificity(normal): 0.861\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.1          7.2\n",
            "True Attack         10.6         29.1\n",
            "Accuracy: 0.769 | Precision(attack): 0.802 | Recall(attack): 0.733 | Specificity(normal): 0.807\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         21.1         16.2\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.714 | Precision(attack): 0.677 | Recall(attack): 0.854 | Specificity(normal): 0.566\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.918              0.918     0.918           0.895              0.895     0.895           0.792              0.727     0.805           0.796              0.718     0.805           0.778              0.784     0.810           0.875              0.667     0.821           0.700              0.950     0.800            0.70              0.271       0.5\n",
            "2               0.924              0.924     0.924           0.893              0.893     0.893           0.779              0.662     0.779           0.776              0.652     0.776           0.800              0.740     0.800           0.756              0.617     0.756           0.850              0.925     0.850            0.52              0.204       0.5\n",
            "3               0.894              0.894     0.894           0.848              0.848     0.848           0.792              0.727     0.779           0.797              0.724     0.781           0.771              0.753     0.773           0.900              0.711     0.829           0.675              0.800     0.725            0.70              0.261       0.5\n",
            "4               0.900              0.900     0.900           0.839              0.839     0.839           0.753              0.623     0.714           0.752              0.613     0.711           0.765              0.707     0.744           0.756              0.593     0.696           0.775              0.875     0.800            0.64              0.294       0.5\n",
            "5               0.867              0.867     0.867           0.797              0.797     0.797           0.766              0.649     0.675           0.770              0.642     0.673           0.750              0.710     0.699           0.844              0.623     0.674           0.675              0.825     0.725            0.68              0.274       0.5\n",
            "6               0.875              0.875     0.875           0.843              0.843     0.843           0.740              0.727     0.727           0.745              0.719     0.725           0.714              0.779     0.747           0.833              0.673     0.721           0.625              0.925     0.775            0.67              0.276       0.5\n",
            "7               0.890              0.890     0.890           0.849              0.849     0.849           0.753              0.727     0.714           0.756              0.722     0.713           0.740              0.764     0.732           0.818              0.694     0.714           0.675              0.850     0.750            0.64              0.333       0.5\n",
            "8               0.872              0.872     0.872           0.828              0.828     0.828           0.727              0.701     0.740           0.728              0.699     0.741           0.712              0.742     0.730           0.765              0.660     0.771           0.667              0.846     0.692            0.55              0.263       0.5\n",
            "9               0.804              0.804     0.804           0.743              0.743     0.743           0.649              0.610     0.662           0.650              0.609     0.664           0.630              0.643     0.629           0.676              0.600     0.710           0.590              0.692     0.564            0.46              0.277       0.5\n",
            "10              0.924              0.924     0.924           0.893              0.893     0.893           0.766              0.662     0.740           0.765              0.659     0.739           0.786              0.729     0.767           0.733              0.614     0.702           0.846              0.897     0.846            0.53              0.341       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          9.8         29.9\n",
            "Accuracy: 0.734 | Precision(attack): 0.736 | Recall(attack): 0.753 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         29.8          7.5\n",
            "True Attack         11.6         28.1\n",
            "Accuracy: 0.752 | Precision(attack): 0.789 | Recall(attack): 0.708 | Specificity(normal): 0.799\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.4         18.9\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.682 | Precision(attack): 0.643 | Recall(attack): 0.859 | Specificity(normal): 0.493\n",
            "\n",
            "########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.942              0.942     0.942           0.909              0.909     0.909           0.857              0.844     0.779           0.856              0.845     0.774           0.864              0.846     0.809           0.854              0.868     0.735           0.875              0.825     0.900            0.66              0.770       0.5\n",
            "2               0.950              0.950     0.950           0.948              0.948     0.948           0.883              0.870     0.818           0.881              0.870     0.813           0.892              0.875     0.844           0.860              0.875     0.760           0.925              0.875     0.950            0.67              0.750       0.5\n",
            "3               0.974              0.974     0.974           0.966              0.966     0.966           0.883              0.883     0.844           0.881              0.881     0.839           0.892              0.892     0.867           0.860              0.860     0.780           0.925              0.925     0.975            0.70              0.651       0.5\n",
            "4               0.985              0.985     0.985           0.982              0.982     0.982           0.935              0.909     0.857           0.934              0.910     0.852           0.938              0.909     0.876           0.927              0.946     0.796           0.950              0.875     0.975            0.65              0.680       0.5\n",
            "5               0.850              0.850     0.850           0.884              0.884     0.884           0.831              0.844     0.792           0.828              0.842     0.786           0.847              0.857     0.826           0.800              0.818     0.731           0.900              0.900     0.950            0.66              0.720       0.5\n",
            "6               0.932              0.932     0.932           0.916              0.916     0.916           0.818              0.779     0.766           0.817              0.776     0.760           0.829              0.800     0.804           0.810              0.756     0.712           0.850              0.850     0.925            0.70              0.663       0.5\n",
            "7               0.946              0.946     0.946           0.922              0.922     0.922           0.909              0.909     0.870           0.910              0.910     0.870           0.909              0.909     0.875           0.946              0.946     0.875           0.875              0.875     0.875            0.70              0.727       0.5\n",
            "8               0.934              0.934     0.934           0.903              0.903     0.903           0.870              0.883     0.844           0.871              0.884     0.844           0.865              0.877     0.846           0.914              0.941     0.846           0.821              0.821     0.846            0.70              0.746       0.5\n",
            "9               0.827              0.827     0.827           0.859              0.859     0.859           0.805              0.792     0.792           0.805              0.793     0.791           0.810              0.784     0.810           0.800              0.829     0.756           0.821              0.744     0.872            0.65              0.780       0.5\n",
            "10              0.951              0.951     0.951           0.933              0.933     0.933           0.857              0.857     0.792           0.857              0.857     0.791           0.857              0.857     0.805           0.868              0.868     0.767           0.846              0.846     0.846            0.70              0.711       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          3.5         36.2\n",
            "Accuracy: 0.816 | Precision(attack): 0.772 | Recall(attack): 0.912 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.7          5.6\n",
            "True Attack          4.8         34.9\n",
            "Accuracy: 0.865 | Precision(attack): 0.862 | Recall(attack): 0.879 | Specificity(normal): 0.850\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.857 | Precision(attack): 0.867 | Recall(attack): 0.854 | Specificity(normal): 0.861\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.058       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.088       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.070       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.044       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.058       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.076       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.070       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.060       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.086       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.064       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987             1.0              1.000       1.0           0.975              0.975     0.975            0.47              0.467       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              1.000     0.987           0.988              1.000     0.988           0.987              1.000     0.987             1.0              1.000       1.0           0.975              1.000     0.975            0.30              0.060       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.059       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.973       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.34              0.336       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.971       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              0.987     1.000           1.000              0.986     1.000           1.000              0.988     1.000             1.0              0.976       1.0           1.000              1.000     1.000            0.30              0.056       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.984       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision(attack): 0.997 | Recall(attack): 0.997 | Specificity(normal): 0.997\n",
            "\n",
            "########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  COMBINED (STRUCTURE + ATTRS; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.943              0.943     0.943           0.911              0.911     0.911           0.844              0.844     0.792           0.843              0.845     0.789           0.854              0.846     0.814           0.833              0.868     0.761           0.875              0.825     0.875            0.67              0.782       0.5\n",
            "2               0.946              0.946     0.946           0.941              0.941     0.941           0.870              0.857     0.831           0.869              0.856     0.826           0.878              0.864     0.854           0.857              0.854     0.776           0.900              0.875     0.950            0.68              0.725       0.5\n",
            "3               0.979              0.979     0.979           0.975              0.975     0.975           0.909              0.896     0.857           0.909              0.894     0.851           0.911              0.905     0.879           0.923              0.864     0.784           0.900              0.950     1.000            0.70              0.597       0.5\n",
            "4               0.986              0.986     0.986           0.983              0.983     0.983           0.935              0.896     0.896           0.935              0.898     0.893           0.937              0.895     0.907           0.949              0.944     0.848           0.925              0.850     0.975            0.67              0.715       0.5\n",
            "5               0.872              0.872     0.872           0.905              0.905     0.905           0.844              0.870     0.805           0.842              0.869     0.799           0.857              0.878     0.835           0.818              0.857     0.745           0.900              0.900     0.950            0.66              0.727       0.5\n",
            "6               0.934              0.934     0.934           0.920              0.920     0.920           0.805              0.831     0.779           0.800              0.830     0.773           0.831              0.840     0.813           0.755              0.829     0.725           0.925              0.850     0.925            0.59              0.724       0.5\n",
            "7               0.948              0.948     0.948           0.927              0.927     0.927           0.909              0.909     0.883           0.910              0.910     0.882           0.909              0.909     0.889           0.946              0.946     0.878           0.875              0.875     0.900            0.69              0.719       0.5\n",
            "8               0.937              0.937     0.937           0.907              0.907     0.907           0.870              0.857     0.844           0.871              0.858     0.844           0.865              0.849     0.846           0.914              0.912     0.846           0.821              0.795     0.846            0.70              0.756       0.5\n",
            "9               0.910              0.910     0.910           0.885              0.885     0.885           0.818              0.792     0.818           0.818              0.793     0.817           0.825              0.784     0.829           0.805              0.829     0.791           0.846              0.744     0.872            0.64              0.781       0.5\n",
            "10              0.953              0.953     0.953           0.935              0.935     0.935           0.844              0.844     0.805           0.844              0.844     0.805           0.846              0.846     0.815           0.846              0.846     0.786           0.846              0.846     0.846            0.69              0.713       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         27.7          9.6\n",
            "True Attack          3.4         36.3\n",
            "Accuracy: 0.831 | Precision(attack): 0.791 | Recall(attack): 0.914 | Specificity(normal): 0.743\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.6          5.7\n",
            "True Attack          4.7         35.0\n",
            "Accuracy: 0.865 | Precision(attack): 0.860 | Recall(attack): 0.882 | Specificity(normal): 0.847\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.4          4.9\n",
            "True Attack          5.9         33.8\n",
            "Accuracy: 0.860 | Precision(attack): 0.873 | Recall(attack): 0.851 | Specificity(normal): 0.869\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.086       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.090       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.088       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.044       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.070       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.086       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.100       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.056       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.072       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             1.0                1.0       1.0             0.3              0.076       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.0         39.7\n",
            "Accuracy: 1.000 | Precision(attack): 1.000 | Recall(attack): 1.000 | Specificity(normal): 1.000\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987             1.0              1.000       1.0           0.975              0.975     0.975            0.48              0.479       0.5\n",
            "2                 1.0                1.0       1.0             1.0                1.0       1.0           0.987              1.000     0.987           0.988              1.000     0.988           0.987              1.000     0.987             1.0              1.000       1.0           0.975              1.000     0.975            0.30              0.049       0.5\n",
            "3                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.066       0.5\n",
            "4                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.973       0.5\n",
            "5                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.34              0.336       0.5\n",
            "6                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.964       0.5\n",
            "7                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              0.987     1.000           1.000              0.986     1.000           1.000              0.988     1.000             1.0              0.976       1.0           1.000              1.000     1.000            0.30              0.039       0.5\n",
            "8                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.985       0.5\n",
            "9                 1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "10                1.0                1.0       1.0             1.0                1.0       1.0           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000             1.0              1.000       1.0           1.000              1.000     1.000            0.30              0.000       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.2         39.5\n",
            "Accuracy: 0.997 | Precision(attack): 1.000 | Recall(attack): 0.995 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.1         39.6\n",
            "Accuracy: 0.997 | Precision(attack): 0.997 | Recall(attack): 0.997 | Specificity(normal): 0.997\n",
            "\n",
            "[OK] Ran with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpIhomRjcoAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CLEAN, LEAK-GUARDED EVAL (numeric-perfect-separation detector)\n",
        "# =========================\n",
        "\n",
        "# ---- Config toggles ----\n",
        "_FORCE_ZERO_CATS = True       # Force node/edge cat blocks to 0 columns (to isolate cat-bag leakage)\n",
        "_DO_ISOLATION_TEST = False    # Also run NUM-only and CATS-only isolation checks\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# ---- 0) Fix bundle_x/bundle_y if they slipped in\n",
        "if \"bundle_x\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_x\": \"bundle\"})\n",
        "if \"bundle_y\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_y\": \"bundle\"})\n",
        "\n",
        "# ---- 1) Define y EARLY and ONCE (aligned to merged)\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "print(\"[y] size:\", y.size, \"| pos rate:\", y.mean().round(3))\n",
        "\n",
        "# ---- 2) Name-guard on df_num (drop only true label-like cols)\n",
        "LEAKY_NAME_SUBSTR = (\"cohort\", \"attack\", \"label\", \"target\", \"y_\", \"is_attack\", \"fold\", \"split\")\n",
        "if \"df_num\" not in globals():\n",
        "    raise RuntimeError(\"df_num is not defined in the environment.\")\n",
        "\n",
        "suspicious = [(c, [s for s in LEAKY_NAME_SUBSTR if s in c.lower()])\n",
        "              for c in df_num.columns]\n",
        "suspicious = [(c,h) for c,h in suspicious if h]\n",
        "if suspicious:\n",
        "    print(\"[LEAK-GUARD] Name matches (column -> substrings):\")\n",
        "    for c,h in suspicious: print(\"   \", c, \"->\", h)\n",
        "    to_drop = [c for c,_ in suspicious]\n",
        "    print(\"[LEAK-GUARD] Dropping numeric columns by name:\", to_drop)\n",
        "    df_num = df_num.drop(columns=to_drop, errors=\"ignore\")\n",
        "\n",
        "# ---- 3) ALIGN df_num rows to merged, then run a STRICT numeric leak scan BEFORE building X_num\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "if \"file\" not in df_num.columns:\n",
        "    raise RuntimeError(\"df_num must contain a 'file' column to align with merged.\")\n",
        "\n",
        "# Exact row order alignment\n",
        "aligned_df_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "# Candidate numeric columns (exclude obvious keys)\n",
        "num_cols_all = [c for c in aligned_df_num.columns\n",
        "                if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                and pd.api.types.is_numeric_dtype(aligned_df_num[c])]\n",
        "\n",
        "# ---- Strict leak detection helpers\n",
        "def _perfectly_separable(col_vals, y_vec):\n",
        "    \"\"\"True if there exists a threshold with zero overlap between classes.\"\"\"\n",
        "    pos = col_vals[y_vec == 1]\n",
        "    neg = col_vals[y_vec == 0]\n",
        "    if len(pos) == 0 or len(neg) == 0:\n",
        "        return False\n",
        "    # Ignore NaNs (treat as overlap)\n",
        "    pos = pos[~np.isnan(pos)]\n",
        "    neg = neg[~np.isnan(neg)]\n",
        "    if pos.size == 0 or neg.size == 0:\n",
        "        return False\n",
        "    return (pos.min() > neg.max()) or (neg.min() > pos.max())\n",
        "\n",
        "def _two_valued_matches_y(col_vals, y_vec):\n",
        "    \"\"\"Catches columns with two unique values matching y or 1-y (not necessarily 0/1).\"\"\"\n",
        "    uv = pd.unique(col_vals[~pd.isna(col_vals)])\n",
        "    if uv.size != 2:\n",
        "        return False\n",
        "    a,b = uv[0], uv[1]\n",
        "    # Map to 0/1 in either direction and compare\n",
        "    m1 = np.where(col_vals == a, 0, np.where(col_vals == b, 1, np.nan))\n",
        "    m2 = np.where(col_vals == a, 1, np.where(col_vals == b, 0, np.nan))\n",
        "    return (np.nanmax(np.abs(m1 - y_vec)) == 0) or (np.nanmax(np.abs(m2 - y_vec)) == 0)\n",
        "\n",
        "def _column_auc_perfect(col_vals, y_vec):\n",
        "    \"\"\"AUC == 1 or 0 (within tiny tol). NaNs break perfection.\"\"\"\n",
        "    from sklearn.metrics import roc_auc_score\n",
        "    if np.isnan(col_vals).any():\n",
        "        return False\n",
        "    try:\n",
        "        auc = roc_auc_score(y_vec, col_vals)\n",
        "        return (auc >= 0.999999) or (auc <= 0.000001)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "leaky_num_cols = []\n",
        "for c in num_cols_all:\n",
        "    v = aligned_df_num[c].to_numpy()\n",
        "    # Fast checks first\n",
        "    if _two_valued_matches_y(v, y):\n",
        "        leaky_num_cols.append((c, \"two-valued==y/1-y\"))\n",
        "        continue\n",
        "    if _perfectly_separable(v, y):\n",
        "        leaky_num_cols.append((c, \"perfect-threshold\"))\n",
        "        continue\n",
        "    if _column_auc_perfect(v, y):\n",
        "        leaky_num_cols.append((c, \"AUC=1 or 0\"))\n",
        "\n",
        "if leaky_num_cols:\n",
        "    print(f\"[LEAK-DETECT:NUMERIC] Dropping {len(leaky_num_cols)} perfectly predictive numeric cols:\")\n",
        "    for c, why in leaky_num_cols[:30]:\n",
        "        print(f\"   - {c}  [{why}]\")\n",
        "    # Drop from both aligned and original df_num to keep everything consistent\n",
        "    drop_names = [c for c,_ in leaky_num_cols]\n",
        "    aligned_df_num = aligned_df_num.drop(columns=drop_names, errors=\"ignore\")\n",
        "    df_num = df_num.drop(columns=drop_names, errors=\"ignore\")\n",
        "else:\n",
        "    print(\"[LEAK-DETECT:NUMERIC] No perfectly predictive numeric columns found by strict checks.\")\n",
        "\n",
        "# Recompute numeric list after drops\n",
        "num_cols_clean = [c for c in aligned_df_num.columns\n",
        "                  if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                  and pd.api.types.is_numeric_dtype(aligned_df_num[c])]\n",
        "\n",
        "# ---- 4) Build X_num from the ALIGNED frame to guarantee order matches merged\n",
        "X_num_blk = csr_matrix(aligned_df_num[num_cols_clean].fillna(0.0).to_numpy()) if num_cols_clean else csr_matrix((len(aligned_df_num),0))\n",
        "\n",
        "# ---- 5) Cat blocks (aligned). Optionally nuked for isolation.\n",
        "if \"X_node_cat\" in globals() and X_node_cat.shape[0] == len(df_num):\n",
        "    # Re-align via merged order\n",
        "    file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "    row_indices = np.array([file_to_row[f] for f in merged[file_col_m].tolist()], dtype=int)\n",
        "    X_nodecat_blk = X_node_cat[row_indices]\n",
        "else:\n",
        "    X_nodecat_blk = csr_matrix((len(merged), 0))\n",
        "\n",
        "if \"X_edge_cat\" in globals() and X_edge_cat.shape[0] == len(df_num):\n",
        "    file_to_row = {f: i for i, f in enumerate(df_num[\"file\"].tolist())}\n",
        "    row_indices = np.array([file_to_row[f] for f in merged[file_col_m].tolist()], dtype=int)\n",
        "    X_edgecat_blk = X_edge_cat[row_indices]\n",
        "else:\n",
        "    X_edgecat_blk = csr_matrix((len(merged), 0))\n",
        "\n",
        "if _FORCE_ZERO_CATS:\n",
        "    X_nodecat_blk = csr_matrix((len(y), 0))\n",
        "    X_edgecat_blk = csr_matrix((len(y), 0))\n",
        "    print(\"[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\")\n",
        "\n",
        "print(\"[ATTR-ALIGNED] numeric:\", X_num_blk.shape, \"| node-cats:\", X_nodecat_blk.shape, \"| edge-cats:\", X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 6) (Optional) Sparse binary exact-match scan AFTER matrix build (kept for completeness)\n",
        "def _drop_perfect_binary_equals_y(X, y_vec, name, max_to_print=20):\n",
        "    from scipy.sparse import issparse\n",
        "    if X.shape[1] == 0:\n",
        "        return X, []\n",
        "    leaks = []\n",
        "    if issparse(X):\n",
        "        Xcsc = X.tocsc()\n",
        "        y_vec = y_vec.astype(int)\n",
        "        y_comp = 1 - y_vec\n",
        "        idx_y  = np.where(y_vec == 1)[0]\n",
        "        idx_ny = np.where(y_comp == 1)[0]\n",
        "        sum_y  = int(y_vec.sum())\n",
        "        sum_yc = int(y_comp.sum())\n",
        "        for j in range(Xcsc.shape[1]):\n",
        "            col = Xcsc[:, j]\n",
        "            if col.nnz == 0: continue\n",
        "            if not np.all((col.data == 0) | (col.data == 1)): continue\n",
        "            col_sum = int(col.sum())\n",
        "            rows = col.indices\n",
        "            if (col_sum == sum_y and np.array_equal(np.sort(rows), idx_y)) or \\\n",
        "               (col_sum == sum_yc and np.array_equal(np.sort(rows), idx_ny)):\n",
        "                leaks.append(j)\n",
        "        if leaks:\n",
        "            print(f\"[LEAK-DETECT] {name}: dropping {len(leaks)} binary==y columns (up to {max_to_print}): {leaks[:max_to_print]}\")\n",
        "            keep = np.setdiff1d(np.arange(Xcsc.shape[1]), np.array(leaks, dtype=int))\n",
        "            return Xcsc[:, keep].tocsr(), leaks\n",
        "        return X, []\n",
        "    else:\n",
        "        return X, []\n",
        "\n",
        "X_num_blk, num_bin_leaks = _drop_perfect_binary_equals_y(X_num_blk, y, \"X_num\")\n",
        "\n",
        "print(\"[LEAK-POST] shapes:\", X_num_blk.shape, X_nodecat_blk.shape, X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 7) Prepare evaluator inputs (NO TEXT)\n",
        "texts_series_empty = pd.Series([\"\"] * len(y))\n",
        "groups = globals().get(\"groups\", None)\n",
        "\n",
        "# ---- 8) Run the requested regimes\n",
        "print(\"\\n########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = csr_matrix((len(y),0)),   # no numeric attrs\n",
        "    X_nodecat_blk = csr_matrix((len(y),0)),   # no node cats\n",
        "    X_edgecat_blk = csr_matrix((len(y),0)),   # no edge cats\n",
        "    texts_series  = texts_series_empty,       # kills TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # ONLY topology features\n",
        "    y             = y,\n",
        "    tag           = \"STRUCTURE-ONLY (topology pack; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = None,                     # no topology here\n",
        "    y             = y,\n",
        "    tag           = \"ATTRS-ONLY (NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk     = X_num_blk,\n",
        "    X_nodecat_blk = X_nodecat_blk,\n",
        "    X_edgecat_blk = X_edgecat_blk,\n",
        "    texts_series  = texts_series_empty,       # NO TF-IDF\n",
        "    X_struct_blk  = X_struct_blk,             # add topology\n",
        "    y             = y,\n",
        "    tag           = \"COMBINED (STRUCTURE + ATTRS; NO TEXT)\",\n",
        "    class_weight  = {0:1, 1:2},\n",
        "    target_recall = 0.85,\n",
        "    groups        = groups,\n",
        ")\n",
        "\n",
        "# ---- 9) Safety assertions\n",
        "assert X_struct_blk.shape[1] > 0, \"STRUCTURE block is empty; check column names.\"\n",
        "if \"struct_num_cols\" in globals():\n",
        "    assert all((\"text\" not in c.lower()) and (\"moderator\" not in c.lower()) for c in struct_num_cols), \\\n",
        "        \"Structural set accidentally contains text/moderator fields.\"\n",
        "\n",
        "print(\"\\n[OK] Ran with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\")\n",
        "\n",
        "# ---- 10) Optional isolation tests\n",
        "if _DO_ISOLATION_TEST:\n",
        "    print(\"\\n########## NUM-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=X_num_blk, X_nodecat_blk=csr_matrix((len(y),0)), X_edgecat_blk=csr_matrix((len(y),0)),\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"NUM-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n",
        "    print(\"\\n########## CATS-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=csr_matrix((len(y),0)), X_nodecat_blk=X_nodecat_blk, X_edgecat_blk=X_edgecat_blk,\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"CATS-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n"
      ],
      "metadata": {
        "id": "hHgEiqXWcoEk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "d3ee04d3-9f3d-4850-9e1e-4137dfc47b9e"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[y] size: 770 | pos rate: 0.516\n",
            "[LEAK-DETECT:NUMERIC] Dropping 12 perfectly predictive numeric cols:\n",
            "   - node_first_ts__mean  [perfect-threshold]\n",
            "   - node_first_ts__min  [perfect-threshold]\n",
            "   - node_first_ts__max  [perfect-threshold]\n",
            "   - node_last_ts__mean  [perfect-threshold]\n",
            "   - node_last_ts__min  [perfect-threshold]\n",
            "   - node_last_ts__max  [perfect-threshold]\n",
            "   - edge_start_ts__mean  [perfect-threshold]\n",
            "   - edge_start_ts__min  [perfect-threshold]\n",
            "   - edge_start_ts__max  [perfect-threshold]\n",
            "   - edge_end_ts__mean  [perfect-threshold]\n",
            "   - edge_end_ts__min  [perfect-threshold]\n",
            "   - edge_end_ts__max  [perfect-threshold]\n",
            "[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\n",
            "[ATTR-ALIGNED] numeric: (770, 264) | node-cats: (770, 0) | edge-cats: (770, 0)\n",
            "[LEAK-POST] shapes: (770, 264) (770, 0) (770, 0)\n",
            "\n",
            "########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  STRUCTURE-ONLY (topology pack; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.896              0.896     0.896           0.819              0.819     0.819           0.766              0.688     0.519           0.766              0.684     0.506           0.775              0.727     0.648           0.775              0.667     0.523           0.775              0.800     0.850            0.69              0.643       0.5\n",
            "2               0.924              0.924     0.924           0.895              0.895     0.895           0.779              0.688     0.636           0.775              0.680     0.623           0.805              0.750     0.736           0.745              0.643     0.591           0.875              0.900     0.975            0.70              0.634       0.5\n",
            "3               0.844              0.844     0.844           0.786              0.786     0.786           0.662              0.688     0.558           0.662              0.684     0.543           0.675              0.727     0.691           0.675              0.667     0.543           0.675              0.800     0.950            0.70              0.641       0.5\n",
            "4               0.917              0.917     0.917           0.858              0.858     0.858           0.740              0.662     0.584           0.737              0.656     0.570           0.767              0.717     0.704           0.717              0.635     0.559           0.825              0.825     0.950            0.69              0.635       0.5\n",
            "5               0.935              0.935     0.935           0.907              0.907     0.907           0.792              0.727     0.649           0.790              0.721     0.635           0.810              0.769     0.748           0.773              0.686     0.597           0.850              0.875     1.000            0.69              0.642       0.5\n",
            "6               0.895              0.895     0.895           0.861              0.861     0.861           0.818              0.701     0.584           0.816              0.693     0.570           0.833              0.758     0.704           0.795              0.655     0.559           0.875              0.900     0.950            0.69              0.632       0.5\n",
            "7               0.930              0.930     0.930           0.891              0.891     0.891           0.818              0.766     0.623           0.819              0.762     0.611           0.821              0.795     0.718           0.842              0.729     0.587           0.800              0.875     0.925            0.69              0.637       0.5\n",
            "8               0.868              0.868     0.868           0.812              0.812     0.812           0.740              0.701     0.610           0.741              0.700     0.606           0.737              0.736     0.712           0.757              0.667     0.569           0.718              0.821     0.949            0.70              0.645       0.5\n",
            "9               0.821              0.821     0.821           0.758              0.758     0.758           0.675              0.662     0.545           0.675              0.661     0.540           0.691              0.705     0.673           0.667              0.633     0.529           0.718              0.795     0.923            0.67              0.644       0.5\n",
            "10              0.931              0.931     0.931           0.897              0.897     0.897           0.792              0.688     0.558           0.791              0.686     0.553           0.805              0.745     0.691           0.767              0.636     0.535           0.846              0.897     0.974            0.70              0.646       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          7.7         29.6\n",
            "True Attack          2.2         37.5\n",
            "Accuracy: 0.587 | Precision(attack): 0.559 | Recall(attack): 0.945 | Specificity(normal): 0.206\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.8         10.5\n",
            "True Attack          8.1         31.6\n",
            "Accuracy: 0.758 | Precision(attack): 0.751 | Recall(attack): 0.796 | Specificity(normal): 0.718\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         20.0         17.3\n",
            "True Attack          6.0         33.7\n",
            "Accuracy: 0.697 | Precision(attack): 0.661 | Recall(attack): 0.849 | Specificity(normal): 0.536\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.930              0.930     0.930           0.908              0.908     0.908           0.831              0.779     0.831           0.834              0.773     0.834           0.822              0.813     0.822           0.909              0.725     0.909           0.750              0.925     0.750            0.49              0.216       0.5\n",
            "2               0.925              0.925     0.925           0.891              0.891     0.891           0.805              0.701     0.857           0.801              0.693     0.857           0.828              0.758     0.861           0.766              0.655     0.872           0.900              0.900     0.850            0.39              0.204       0.5\n",
            "3               0.885              0.885     0.885           0.833              0.833     0.833           0.779              0.701     0.792           0.781              0.700     0.797           0.773              0.716     0.771           0.829              0.707     0.900           0.725              0.725     0.675            0.40              0.224       0.5\n",
            "4               0.920              0.920     0.920           0.874              0.874     0.874           0.805              0.662     0.831           0.803              0.655     0.832           0.819              0.723     0.831           0.791              0.630     0.865           0.850              0.850     0.800            0.43              0.290       0.5\n",
            "5               0.880              0.880     0.880           0.829              0.829     0.829           0.714              0.662     0.740           0.712              0.656     0.744           0.738              0.717     0.722           0.705              0.635     0.812           0.775              0.825     0.650            0.36              0.202       0.5\n",
            "6               0.859              0.859     0.859           0.825              0.825     0.825           0.701              0.766     0.714           0.706              0.761     0.718           0.667              0.800     0.694           0.793              0.720     0.781           0.575              0.900     0.625            0.60              0.208       0.5\n",
            "7               0.924              0.924     0.924           0.895              0.895     0.895           0.792              0.805     0.805           0.795              0.800     0.808           0.784              0.831     0.795           0.853              0.755     0.879           0.725              0.925     0.725            0.45              0.246       0.5\n",
            "8               0.889              0.889     0.889           0.861              0.861     0.861           0.766              0.714     0.766           0.768              0.712     0.768           0.727              0.761     0.735           0.889              0.660     0.862           0.615              0.897     0.641            0.68              0.202       0.5\n",
            "9               0.803              0.803     0.803           0.734              0.734     0.734           0.675              0.623     0.688           0.676              0.622     0.690           0.658              0.659     0.657           0.706              0.609     0.742           0.615              0.718     0.590            0.45              0.246       0.5\n",
            "10              0.919              0.919     0.919           0.882              0.882     0.882           0.818              0.727     0.805           0.818              0.725     0.805           0.816              0.764     0.805           0.838              0.680     0.816           0.795              0.872     0.795            0.53              0.234       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack         11.5         28.2\n",
            "Accuracy: 0.783 | Precision(attack): 0.844 | Recall(attack): 0.710 | Specificity(normal): 0.861\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.1          7.2\n",
            "True Attack         10.6         29.1\n",
            "Accuracy: 0.769 | Precision(attack): 0.802 | Recall(attack): 0.733 | Specificity(normal): 0.807\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         21.1         16.2\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.714 | Precision(attack): 0.677 | Recall(attack): 0.854 | Specificity(normal): 0.566\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.918              0.918     0.918           0.895              0.895     0.895           0.792              0.727     0.805           0.796              0.718     0.805           0.778              0.784     0.810           0.875              0.667     0.821           0.700              0.950     0.800            0.70              0.271       0.5\n",
            "2               0.924              0.924     0.924           0.893              0.893     0.893           0.779              0.662     0.779           0.776              0.652     0.776           0.800              0.740     0.800           0.756              0.617     0.756           0.850              0.925     0.850            0.52              0.204       0.5\n",
            "3               0.894              0.894     0.894           0.848              0.848     0.848           0.792              0.727     0.779           0.797              0.724     0.781           0.771              0.753     0.773           0.900              0.711     0.829           0.675              0.800     0.725            0.70              0.261       0.5\n",
            "4               0.900              0.900     0.900           0.839              0.839     0.839           0.753              0.623     0.714           0.752              0.613     0.711           0.765              0.707     0.744           0.756              0.593     0.696           0.775              0.875     0.800            0.64              0.294       0.5\n",
            "5               0.867              0.867     0.867           0.797              0.797     0.797           0.766              0.649     0.675           0.770              0.642     0.673           0.750              0.710     0.699           0.844              0.623     0.674           0.675              0.825     0.725            0.68              0.274       0.5\n",
            "6               0.875              0.875     0.875           0.843              0.843     0.843           0.740              0.727     0.727           0.745              0.719     0.725           0.714              0.779     0.747           0.833              0.673     0.721           0.625              0.925     0.775            0.67              0.276       0.5\n",
            "7               0.890              0.890     0.890           0.849              0.849     0.849           0.753              0.727     0.714           0.756              0.722     0.713           0.740              0.764     0.732           0.818              0.694     0.714           0.675              0.850     0.750            0.64              0.333       0.5\n",
            "8               0.872              0.872     0.872           0.828              0.828     0.828           0.727              0.701     0.740           0.728              0.699     0.741           0.712              0.742     0.730           0.765              0.660     0.771           0.667              0.846     0.692            0.55              0.263       0.5\n",
            "9               0.804              0.804     0.804           0.743              0.743     0.743           0.649              0.610     0.662           0.650              0.609     0.664           0.630              0.643     0.629           0.676              0.600     0.710           0.590              0.692     0.564            0.46              0.277       0.5\n",
            "10              0.924              0.924     0.924           0.893              0.893     0.893           0.766              0.662     0.740           0.765              0.659     0.739           0.786              0.729     0.767           0.733              0.614     0.702           0.846              0.897     0.846            0.53              0.341       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          9.8         29.9\n",
            "Accuracy: 0.734 | Precision(attack): 0.736 | Recall(attack): 0.753 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         29.8          7.5\n",
            "True Attack         11.6         28.1\n",
            "Accuracy: 0.752 | Precision(attack): 0.789 | Recall(attack): 0.708 | Specificity(normal): 0.799\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.4         18.9\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.682 | Precision(attack): 0.643 | Recall(attack): 0.859 | Specificity(normal): 0.493\n",
            "\n",
            "########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.941              0.941     0.941           0.908              0.908     0.908           0.857              0.844     0.805           0.856              0.844     0.801           0.864              0.850     0.828           0.854              0.850     0.766           0.875              0.850     0.900            0.66              0.744       0.5\n",
            "2               0.951              0.951     0.951           0.947              0.947     0.947           0.870              0.883     0.831           0.868              0.882     0.826           0.881              0.889     0.854           0.841              0.878     0.776           0.925              0.900     0.950            0.64              0.732       0.5\n",
            "3               0.973              0.973     0.973           0.966              0.966     0.966           0.883              0.883     0.831           0.881              0.880     0.826           0.892              0.894     0.854           0.860              0.844     0.776           0.925              0.950     0.950            0.69              0.613       0.5\n",
            "4               0.980              0.980     0.980           0.975              0.975     0.975           0.909              0.896     0.857           0.908              0.898     0.852           0.914              0.895     0.876           0.902              0.944     0.796           0.925              0.850     0.975            0.66              0.688       0.5\n",
            "5               0.865              0.865     0.865           0.899              0.899     0.899           0.857              0.857     0.805           0.854              0.854     0.799           0.871              0.871     0.835           0.822              0.822     0.745           0.925              0.925     0.950            0.66              0.698       0.5\n",
            "6               0.917              0.917     0.917           0.900              0.900     0.900           0.740              0.792     0.779           0.736              0.790     0.773           0.773              0.810     0.813           0.708              0.773     0.725           0.850              0.850     0.925            0.58              0.667       0.5\n",
            "7               0.956              0.956     0.956           0.926              0.926     0.926           0.909              0.909     0.857           0.910              0.910     0.856           0.909              0.909     0.864           0.946              0.946     0.854           0.875              0.875     0.875            0.69              0.693       0.5\n",
            "8               0.927              0.927     0.927           0.896              0.896     0.896           0.870              0.870     0.818           0.871              0.871     0.818           0.865              0.865     0.825           0.914              0.914     0.805           0.821              0.821     0.846            0.70              0.727       0.5\n",
            "9               0.851              0.851     0.851           0.853              0.853     0.853           0.779              0.792     0.779           0.779              0.793     0.778           0.785              0.784     0.800           0.775              0.829     0.739           0.795              0.744     0.872            0.68              0.757       0.5\n",
            "10              0.948              0.948     0.948           0.930              0.930     0.930           0.857              0.857     0.779           0.857              0.857     0.778           0.857              0.857     0.795           0.868              0.868     0.750           0.846              0.846     0.846            0.70              0.699       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          3.6         36.1\n",
            "Accuracy: 0.814 | Precision(attack): 0.771 | Recall(attack): 0.909 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.9          6.4\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.853 | Precision(attack): 0.845 | Recall(attack): 0.877 | Specificity(normal): 0.828\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.9          5.4\n",
            "True Attack          5.5         34.2\n",
            "Accuracy: 0.858 | Precision(attack): 0.864 | Recall(attack): 0.861 | Specificity(normal): 0.855\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.40              0.732       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.58              0.642       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000              1.000     1.000           0.975              0.975     0.975            0.68              0.776       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.46              0.454       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.52              0.614       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.55              0.540       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           0.987              1.000     0.987           0.986              1.000     0.986           0.988              1.000     0.988           0.976              1.000     0.976           1.000              1.000     1.000            0.44              0.636       0.5\n",
            "8               0.999              0.999     0.999           0.999              0.999     0.999           0.974              0.974     0.987           0.974              0.974     0.987           0.974              0.974     0.987           0.974              0.974     1.000           0.974              0.974     0.974            0.38              0.378       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.987              0.987     0.987           0.987              0.987     0.987           1.000              1.000     1.000           0.974              0.974     0.974            0.69              0.684       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000            0.48              0.628       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.995 | Precision(attack): 0.997 | Recall(attack): 0.992 | Specificity(normal): 0.997\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.994 | Precision(attack): 0.995 | Recall(attack): 0.992 | Specificity(normal): 0.995\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.995 | Precision(attack): 0.997 | Recall(attack): 0.992 | Specificity(normal): 0.997\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000                1.0     1.000           0.975              0.975     0.975            0.65              0.990       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           0.987              1.000     1.000           0.986              1.000     1.000           0.988              1.000     1.000           0.976                1.0     1.000           1.000              1.000     1.000            0.40              0.969       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000                1.0     1.000           0.975              0.975     0.975            0.30              0.999       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0     1.000           1.000              1.000     1.000            0.68              0.994       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           1.000              0.987     1.000           1.000              0.988     1.000           1.000              0.987     1.000           1.000                1.0     1.000           1.000              0.975     1.000            0.43              0.995       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0     1.000           1.000              1.000     1.000            0.70              0.996       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           0.987              1.000     0.987           0.986              1.000     0.986           0.988              1.000     0.988           0.976                1.0     0.976           1.000              1.000     1.000            0.30              0.989       0.5\n",
            "8               0.998              0.998     0.998           0.998              0.998     0.998           0.987              0.974     0.987           0.987              0.974     0.987           0.987              0.974     0.987           1.000                1.0     1.000           0.974              0.949     0.974            0.68              0.988       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.987              0.987     0.987           0.987              0.987     0.987           1.000                1.0     1.000           0.974              0.974     0.974            0.50              0.996       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           0.974              1.000     0.974           0.974              1.000     0.974           0.975              1.000     0.975           0.951                1.0     0.951           1.000              1.000     1.000            0.62              1.000       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.0          0.3\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.991 | Precision(attack): 0.992 | Recall(attack): 0.990 | Specificity(normal): 0.992\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         36.9          0.4\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.990 | Precision(attack): 0.990 | Recall(attack): 0.990 | Specificity(normal): 0.989\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.992 | Precision(attack): 1.000 | Recall(attack): 0.985 | Specificity(normal): 1.000\n",
            "\n",
            "########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  COMBINED (STRUCTURE + ATTRS; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.941              0.941     0.941           0.907              0.907     0.907           0.844              0.844     0.805           0.844              0.844     0.801           0.850              0.850     0.828           0.850              0.850     0.766           0.850              0.850     0.900            0.68              0.749       0.5\n",
            "2               0.949              0.949     0.949           0.945              0.945     0.945           0.857              0.870     0.831           0.855              0.869     0.826           0.867              0.878     0.854           0.837              0.857     0.776           0.900              0.900     0.950            0.66              0.720       0.5\n",
            "3               0.973              0.973     0.973           0.968              0.968     0.968           0.883              0.870     0.818           0.881              0.867     0.813           0.892              0.884     0.844           0.860              0.826     0.760           0.925              0.950     0.950            0.70              0.582       0.5\n",
            "4               0.980              0.980     0.980           0.975              0.975     0.975           0.896              0.896     0.857           0.896              0.898     0.852           0.900              0.895     0.876           0.900              0.944     0.796           0.900              0.850     0.975            0.66              0.700       0.5\n",
            "5               0.931              0.931     0.931           0.912              0.912     0.912           0.857              0.857     0.805           0.854              0.854     0.799           0.871              0.871     0.835           0.822              0.822     0.745           0.925              0.925     0.950            0.69              0.693       0.5\n",
            "6               0.917              0.917     0.917           0.900              0.900     0.900           0.740              0.805     0.779           0.736              0.804     0.773           0.773              0.815     0.813           0.708              0.805     0.725           0.850              0.825     0.925            0.58              0.692       0.5\n",
            "7               0.957              0.957     0.957           0.928              0.928     0.928           0.909              0.909     0.870           0.910              0.910     0.869           0.909              0.909     0.878           0.946              0.946     0.857           0.875              0.875     0.900            0.67              0.692       0.5\n",
            "8               0.927              0.927     0.927           0.896              0.896     0.896           0.870              0.870     0.818           0.871              0.871     0.818           0.865              0.865     0.825           0.914              0.914     0.805           0.821              0.821     0.846            0.70              0.725       0.5\n",
            "9               0.895              0.895     0.895           0.868              0.868     0.868           0.779              0.779     0.779           0.779              0.780     0.778           0.785              0.773     0.800           0.775              0.806     0.739           0.795              0.744     0.872            0.68              0.743       0.5\n",
            "10              0.948              0.948     0.948           0.930              0.930     0.930           0.857              0.857     0.779           0.857              0.857     0.778           0.857              0.857     0.795           0.868              0.868     0.750           0.846              0.846     0.846            0.70              0.709       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.5         10.8\n",
            "True Attack          3.5         36.2\n",
            "Accuracy: 0.814 | Precision(attack): 0.770 | Recall(attack): 0.912 | Specificity(normal): 0.710\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.9          6.4\n",
            "True Attack          5.2         34.5\n",
            "Accuracy: 0.849 | Precision(attack): 0.844 | Recall(attack): 0.869 | Specificity(normal): 0.828\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.8          5.5\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.856 | Precision(attack): 0.861 | Recall(attack): 0.859 | Specificity(normal): 0.853\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.43              0.716       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.61              0.604       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.974     0.987           0.988              0.975     0.988           0.987              0.974     0.987           1.000              1.000       1.0           0.975              0.950     0.975            0.67              0.770       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.47              0.464       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.54              0.610       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.60              0.592       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           0.987              1.000     1.000           0.986              1.000     1.000           0.988              1.000     1.000           0.976              1.000       1.0           1.000              1.000     1.000            0.42              0.716       0.5\n",
            "8               0.999              0.999     0.999           0.999              0.999     0.999           0.974              0.974     0.987           0.974              0.974     0.987           0.974              0.974     0.987           0.974              0.974       1.0           0.974              0.974     0.974            0.34              0.338       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.987              0.987     0.987           0.987              0.987     0.987           1.000              1.000       1.0           0.974              0.974     0.974            0.67              0.666       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000       1.0           1.000              1.000     1.000            0.61              0.602       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.996 | Precision(attack): 1.000 | Recall(attack): 0.992 | Specificity(normal): 1.000\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.1          0.2\n",
            "True Attack          0.3         39.4\n",
            "Accuracy: 0.994 | Precision(attack): 0.995 | Recall(attack): 0.992 | Specificity(normal): 0.995\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.2          0.1\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.994 | Precision(attack): 0.997 | Recall(attack): 0.990 | Specificity(normal): 0.997\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000                1.0     1.000           0.975              0.975     0.975            0.64              0.994       0.5\n",
            "2               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0     1.000           1.000              1.000     1.000            0.45              0.960       0.5\n",
            "3               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.988              0.988     0.988           0.987              0.987     0.987           1.000                1.0     1.000           0.975              0.975     0.975            0.30              0.999       0.5\n",
            "4               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0     1.000           1.000              1.000     1.000            0.30              0.995       0.5\n",
            "5               1.000              1.000     1.000           1.000              1.000     1.000           1.000              0.987     1.000           1.000              0.988     1.000           1.000              0.987     1.000           1.000                1.0     1.000           1.000              0.975     1.000            0.41              0.995       0.5\n",
            "6               1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000              1.000     1.000           1.000                1.0     1.000           1.000              1.000     1.000            0.61              0.996       0.5\n",
            "7               1.000              1.000     1.000           1.000              1.000     1.000           0.987              1.000     0.987           0.986              1.000     0.986           0.988              1.000     0.988           0.976                1.0     0.976           1.000              1.000     1.000            0.30              0.987       0.5\n",
            "8               0.998              0.998     0.998           0.998              0.998     0.998           0.987              0.974     0.987           0.987              0.974     0.987           0.987              0.974     0.987           1.000                1.0     1.000           0.974              0.949     0.974            0.55              0.985       0.5\n",
            "9               1.000              1.000     1.000           1.000              1.000     1.000           0.987              0.987     0.987           0.987              0.987     0.987           0.987              0.987     0.987           1.000                1.0     1.000           0.974              0.974     0.974            0.55              0.996       0.5\n",
            "10              1.000              1.000     1.000           1.000              1.000     1.000           0.974              1.000     0.974           0.974              1.000     0.974           0.975              1.000     0.975           0.951                1.0     0.951           1.000              1.000     1.000            0.44              0.999       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.0          0.3\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.991 | Precision(attack): 0.992 | Recall(attack): 0.990 | Specificity(normal): 0.992\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.0          0.3\n",
            "True Attack          0.4         39.3\n",
            "Accuracy: 0.991 | Precision(attack): 0.992 | Recall(attack): 0.990 | Specificity(normal): 0.992\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         37.3          0.0\n",
            "True Attack          0.6         39.1\n",
            "Accuracy: 0.992 | Precision(attack): 1.000 | Recall(attack): 0.985 | Specificity(normal): 1.000\n",
            "\n",
            "[OK] Ran with: topology-only STRUCTURE; ATTRS without text; and their COMBINATION. No TF-IDF used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### FUCK"
      ],
      "metadata": {
        "id": "xkH4ku8PcoHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# CLEAN, LEAK-GUARDED EVAL  (now with iterative shallow-tree purge)\n",
        "# =========================\n",
        "\n",
        "# ---- Config toggles ----\n",
        "_FORCE_ZERO_CATS = True\n",
        "_DO_ISOLATION_TEST = False\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# ---- 0) Fix bundle renames\n",
        "if \"bundle_x\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_x\": \"bundle\"})\n",
        "if \"bundle_y\" in merged.columns and \"bundle\" not in merged.columns:\n",
        "    merged = merged.rename(columns={\"bundle_y\": \"bundle\"})\n",
        "\n",
        "# ---- 1) Define y\n",
        "y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "print(\"[y] size:\", y.size, \"| pos rate:\", y.mean().round(3))\n",
        "\n",
        "# ---- 2) Name-guard on df_num\n",
        "LEAKY_NAME_SUBSTR = (\"cohort\",\"attack\",\"label\",\"target\",\"y_\",\"is_attack\",\"fold\",\"split\")\n",
        "if \"df_num\" not in globals():\n",
        "    raise RuntimeError(\"df_num is not defined.\")\n",
        "hits = [(c,[s for s in LEAKY_NAME_SUBSTR if s in c.lower()]) for c in df_num.columns]\n",
        "hits = [(c,h) for c,h in hits if h]\n",
        "if hits:\n",
        "    print(\"[LEAK-GUARD] Name matches:\")\n",
        "    for c,h in hits: print(\"   \", c, \"->\", h)\n",
        "    df_num = df_num.drop(columns=[c for c,_ in hits], errors=\"ignore\")\n",
        "\n",
        "# ---- 3) Align df_num rows to merged\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "if \"file\" not in df_num.columns:\n",
        "    raise RuntimeError(\"df_num must contain 'file' for alignment.\")\n",
        "aligned_df_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "\n",
        "# ---- 4) Single-column strict leak checks (exact/threshold/AUC)\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "num_cols_all = [c for c in aligned_df_num.columns\n",
        "                if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                and pd.api.types.is_numeric_dtype(aligned_df_num[c])]\n",
        "\n",
        "def _perfectly_separable(v, yv):\n",
        "    pos, neg = v[yv==1], v[yv==0]\n",
        "    pos = pos[~np.isnan(pos)]; neg = neg[~np.isnan(neg)]\n",
        "    if pos.size==0 or neg.size==0: return False\n",
        "    return (pos.min() > neg.max()) or (neg.min() > pos.max())\n",
        "\n",
        "def _two_valued_matches_y(v, yv):\n",
        "    uv = pd.unique(v[~pd.isna(v)])\n",
        "    if uv.size != 2: return False\n",
        "    a,b = uv[0], uv[1]\n",
        "    m1 = np.where(v==a, 0, np.where(v==b, 1, np.nan))\n",
        "    m2 = np.where(v==a, 1, np.where(v==b, 0, np.nan))\n",
        "    return (np.nanmax(np.abs(m1 - yv)) == 0) or (np.nanmax(np.abs(m2 - yv)) == 0)\n",
        "\n",
        "def _auc_perfect(v, yv):\n",
        "    if np.isnan(v).any(): return False\n",
        "    try:\n",
        "        auc = roc_auc_score(yv, v)\n",
        "        return (auc >= 0.999999) or (auc <= 0.000001)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "leaky_cols = []\n",
        "for c in num_cols_all:\n",
        "    v = aligned_df_num[c].to_numpy()\n",
        "    if _two_valued_matches_y(v, y) or _perfectly_separable(v, y) or _auc_perfect(v, y):\n",
        "        leaky_cols.append((c, \"single-col leak\"))\n",
        "\n",
        "if leaky_cols:\n",
        "    print(f\"[LEAK-DETECT:NUMERIC] Dropping {len(leaky_cols)} single-col leaks:\")\n",
        "    for c,_ in leaky_cols[:30]: print(\"   -\", c)\n",
        "    aligned_df_num = aligned_df_num.drop(columns=[c for c,_ in leaky_cols], errors=\"ignore\")\n",
        "    df_num = df_num.drop(columns=[c for c,_ in leaky_cols], errors=\"ignore\")\n",
        "\n",
        "# ---- 5) Iterative shallow-tree purge (catches small non-linear combos)\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "groups = globals().get(\"groups\", None)\n",
        "def _cv_splitter(n_splits=10):\n",
        "    if groups is not None and len(np.unique(groups)) > 1:\n",
        "        # Prefer group-aware + stratified if available\n",
        "        try:\n",
        "            return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y, groups)\n",
        "        except Exception:\n",
        "            return GroupKFold(n_splits=n_splits).split(np.zeros_like(y), groups=groups)\n",
        "    else:\n",
        "        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y)\n",
        "\n",
        "def _features_used_by_tree(tree, feature_names):\n",
        "    idxs = set(tree.tree_.feature[tree.tree_.feature >= 0].tolist())\n",
        "    return [feature_names[i] for i in idxs]\n",
        "\n",
        "PURGE_MAX_ITERS = 20\n",
        "PURGE_DEPTH     = 2\n",
        "PURGE_MIN_LEAF  = 2\n",
        "tol_perfect = 1.0  # Balanced Acc of 1.0 on VAL triggers drop\n",
        "\n",
        "X_df = aligned_df_num[[c for c in aligned_df_num.columns\n",
        "                       if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                       and pd.api.types.is_numeric_dtype(aligned_df_num[c])]].copy()\n",
        "def _sanitize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    # 1) Remove ±inf\n",
        "    Z = df.replace([np.inf, -np.inf], np.nan)\n",
        "    # 2) Median-impute NaNs (column-wise)\n",
        "    med = Z.median(numeric_only=True)\n",
        "    Z = Z.fillna(med)\n",
        "    # 3) Clip wild outliers to keep trees from making degenerate splits on huge sentinels\n",
        "    q01 = Z.quantile(0.01, numeric_only=True)\n",
        "    q99 = Z.quantile(0.99, numeric_only=True)\n",
        "    # Use aligned indexing for clip\n",
        "    for c in Z.columns:\n",
        "        lo = q01.get(c, None); hi = q99.get(c, None)\n",
        "        if lo is not None and hi is not None and np.isfinite([lo,hi]).all():\n",
        "            Z[c] = Z[c].clip(lo, hi)\n",
        "    # Ensure float64\n",
        "    return Z.astype(np.float64)\n",
        "\n",
        "X_df = _sanitize_df(X_df)\n",
        "X_df = X_df.fillna(np.inf)  # keep NaNs from faking overlap\n",
        "feat_drop_iter = []\n",
        "\n",
        "for it in range(PURGE_MAX_ITERS):\n",
        "    if X_df.shape[1] == 0: break\n",
        "    to_drop = set()\n",
        "    X_df = _sanitize_df(X_df)\n",
        "    for tr_idx, va_idx in _cv_splitter():\n",
        "        Xt, yt = X_df.iloc[tr_idx].to_numpy(), y[tr_idx]\n",
        "        Xv, yv = X_df.iloc[va_idx].to_numpy(), y[va_idx]\n",
        "        if Xt.shape[1] == 0: break\n",
        "        tree = DecisionTreeClassifier(max_depth=PURGE_DEPTH, min_samples_leaf=PURGE_MIN_LEAF, random_state=it)\n",
        "        tree.fit(Xt, yt)\n",
        "        # validate strictly on val\n",
        "        yv_pred = tree.predict(Xv)\n",
        "        acc = ((yv_pred==yv).mean())\n",
        "        # balanced acc:\n",
        "        tp = ((yv_pred==1)&(yv==1)).sum(); fn = ((yv_pred==0)&(yv==1)).sum()\n",
        "        tn = ((yv_pred==0)&(yv==0)).sum(); fp = ((yv_pred==1)&(yv==0)).sum()\n",
        "        rec = tp / max(tp+fn, 1)\n",
        "        spc = tn / max(tn+fp, 1)\n",
        "        bal_acc = 0.5*(rec + spc)\n",
        "        if bal_acc >= tol_perfect and acc >= 0.99:  # super strict\n",
        "            used = _features_used_by_tree(tree, X_df.columns.tolist())\n",
        "            to_drop.update(used)\n",
        "    if not to_drop:\n",
        "        print(f\"[TREE-PURGE] Iter {it}: no perfect-val splits found; stopping.\")\n",
        "        break\n",
        "    print(f\"[TREE-PURGE] Iter {it}: dropping {len(to_drop)} feature(s) used in perfect val split(s):\", sorted(list(to_drop))[:25])\n",
        "    X_df = X_df.drop(columns=list(to_drop), errors=\"ignore\")\n",
        "    feat_drop_iter.extend(sorted(list(to_drop)))\n",
        "else:\n",
        "    print(\"[TREE-PURGE] Reached max iterations; remaining features kept.\")\n",
        "\n",
        "# Apply final kept numeric set back to frames\n",
        "keep_num_cols = X_df.columns.tolist()\n",
        "\n",
        "# Rebuild a clean numeric matrix using the sanitizer\n",
        "clean_num = _sanitize_df(aligned_df_num[keep_num_cols])\n",
        "\n",
        "# (Optional but safe) drop any all-NaN columns that somehow remain after sanitize\n",
        "clean_num = clean_num.drop(columns=clean_num.columns[clean_num.isna().all()], errors=\"ignore\")\n",
        "\n",
        "# Stitch back the file column\n",
        "aligned_df_num = pd.concat([aligned_df_num[[\"file\"]], clean_num], axis=1)\n",
        "\n",
        "# Keep df_num in sync with sanitized columns\n",
        "df_num = df_num[[\"file\"] + [c for c in keep_num_cols if c in df_num.columns]]\n",
        "\n",
        "# ---- 6) Build blocks\n",
        "# ---- 6) Build blocks (robust to drift)\n",
        "keep_num_cols = X_df.columns.tolist()\n",
        "\n",
        "# Intersect with what's actually present right now\n",
        "available = [c for c in keep_num_cols if c in aligned_df_num.columns]\n",
        "missing   = [c for c in keep_num_cols if c not in aligned_df_num.columns]\n",
        "if missing:\n",
        "    print(f\"[WARN] {len(missing)} purge-kept cols missing from aligned_df_num; continuing without them.\")\n",
        "    print(\"        (showing first 20):\", missing[:20])\n",
        "\n",
        "# Sanitize only the columns we truly have\n",
        "if available:\n",
        "    clean_num = _sanitize_df(aligned_df_num[available])\n",
        "    # Optionally drop any all-NaN columns post-sanitize\n",
        "    all_nan_cols = clean_num.columns[clean_num.isna().all()].tolist()\n",
        "    if all_nan_cols:\n",
        "        print(f\"[WARN] Dropping {len(all_nan_cols)} all-NaN cols after sanitize:\", all_nan_cols[:20])\n",
        "        clean_num = clean_num.drop(columns=all_nan_cols, errors=\"ignore\")\n",
        "else:\n",
        "    clean_num = pd.DataFrame(index=aligned_df_num.index)\n",
        "\n",
        "# Stitch file back and keep df_num in sync\n",
        "aligned_df_num = pd.concat([aligned_df_num[[\"file\"]], clean_num], axis=1)\n",
        "df_num = df_num[[\"file\"] + [c for c in clean_num.columns if c in df_num.columns]]\n",
        "\n",
        "# Sparse block\n",
        "X_num_blk = csr_matrix(clean_num.to_numpy()) if clean_num.shape[1] else csr_matrix((len(aligned_df_num),0))\n",
        "\n",
        "\n",
        "# node/edge cats aligned (then optionally nuked)\n",
        "if \"X_node_cat\" in globals() and X_node_cat.shape[0] == len(df_num):\n",
        "    f2r = {f:i for i,f in enumerate(df_num[\"file\"].tolist())}\n",
        "    row_idx = np.array([f2r[f] for f in merged[file_col_m].tolist()], dtype=int)\n",
        "    X_nodecat_blk = X_node_cat[row_idx]\n",
        "else:\n",
        "    X_nodecat_blk = csr_matrix((len(merged),0))\n",
        "\n",
        "if \"X_edge_cat\" in globals() and X_edge_cat.shape[0] == len(df_num):\n",
        "    f2r = {f:i for i,f in enumerate(df_num[\"file\"].tolist())}\n",
        "    row_idx = np.array([f2r[f] for f in merged[file_col_m].tolist()], dtype=int)\n",
        "    X_edgecat_blk = X_edge_cat[row_idx]\n",
        "else:\n",
        "    X_edgecat_blk = csr_matrix((len(merged),0))\n",
        "\n",
        "if _FORCE_ZERO_CATS:\n",
        "    X_nodecat_blk = csr_matrix((len(y),0))\n",
        "    X_edgecat_blk = csr_matrix((len(y),0))\n",
        "    print(\"[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\")\n",
        "\n",
        "print(\"[ATTR-ALIGNED] numeric:\", X_num_blk.shape, \"| node-cats:\", X_nodecat_blk.shape, \"| edge-cats:\", X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 7) Optional: binary-equals-y sweep on sparse numeric (defensive)\n",
        "def _drop_perfect_binary_equals_y(X, y_vec, name, max_to_print=20):\n",
        "    from scipy.sparse import issparse\n",
        "    if X.shape[1] == 0: return X,[]\n",
        "    leaks=[]\n",
        "    if issparse(X):\n",
        "        Xcsc = X.tocsc()\n",
        "        y_vec=y_vec.astype(int); y_comp=1-y_vec\n",
        "        idx_y=np.where(y_vec==1)[0]; idx_ny=np.where(y_comp==1)[0]\n",
        "        sum_y=int(y_vec.sum()); sum_yc=int(y_comp.sum())\n",
        "        for j in range(Xcsc.shape[1]):\n",
        "            col=Xcsc[:,j]\n",
        "            if col.nnz==0: continue\n",
        "            if not np.all((col.data==0)|(col.data==1)): continue\n",
        "            if int(col.sum())==sum_y and np.array_equal(np.sort(col.indices), idx_y):\n",
        "                leaks.append(j); continue\n",
        "            if int(col.sum())==sum_yc and np.array_equal(np.sort(col.indices), idx_ny):\n",
        "                leaks.append(j); continue\n",
        "        if leaks:\n",
        "            print(f\"[LEAK-DETECT] {name}: dropping {len(leaks)} binary==y columns:\", leaks[:max_to_print])\n",
        "            keep=np.setdiff1d(np.arange(Xcsc.shape[1]), np.array(leaks, int))\n",
        "            return Xcsc[:,keep].tocsr(), leaks\n",
        "        return X, []\n",
        "    return X, []\n",
        "X_num_blk,_= _drop_perfect_binary_equals_y(X_num_blk, y, \"X_num\")\n",
        "\n",
        "print(\"[LEAK-POST] shapes:\", X_num_blk.shape, X_nodecat_blk.shape, X_edgecat_blk.shape)\n",
        "\n",
        "# ---- 8) Eval (unchanged)\n",
        "texts_series_empty = pd.Series([\"\"]*len(y))\n",
        "print(\"\\n########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk=csr_matrix((len(y),0)),\n",
        "    X_nodecat_blk=csr_matrix((len(y),0)),\n",
        "    X_edgecat_blk=csr_matrix((len(y),0)),\n",
        "    texts_series=texts_series_empty,\n",
        "    X_struct_blk=X_struct_blk,\n",
        "    y=y,\n",
        "    tag=\"STRUCTURE-ONLY (topology pack; NO TEXT)\",\n",
        "    class_weight={0:1,1:2},\n",
        "    target_recall=0.85,\n",
        "    groups=groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk=X_num_blk,\n",
        "    X_nodecat_blk=X_nodecat_blk,\n",
        "    X_edgecat_blk=X_edgecat_blk,\n",
        "    texts_series=texts_series_empty,\n",
        "    X_struct_blk=None,\n",
        "    y=y,\n",
        "    tag=\"ATTRS-ONLY (NO TEXT)\",\n",
        "    class_weight={0:1,1:2},\n",
        "    target_recall=0.85,\n",
        "    groups=groups,\n",
        ")\n",
        "\n",
        "print(\"\\n########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\")\n",
        "eval_with_text_rebuilt_per_fold(\n",
        "    X_num_blk=X_num_blk,\n",
        "    X_nodecat_blk=X_nodecat_blk,\n",
        "    X_edgecat_blk=X_edgecat_blk,\n",
        "    texts_series=texts_series_empty,\n",
        "    X_struct_blk=X_struct_blk,\n",
        "    y=y,\n",
        "    tag=\"COMBINED (STRUCTURE + ATTRS; NO TEXT)\",\n",
        "    class_weight={0:1,1:2},\n",
        "    target_recall=0.85,\n",
        "    groups=groups,\n",
        ")\n",
        "\n",
        "# ---- 9) Safety assertions\n",
        "assert X_struct_blk.shape[1] > 0, \"STRUCTURE block is empty; check column names.\"\n",
        "if \"struct_num_cols\" in globals():\n",
        "    assert all((\"text\" not in c.lower()) and (\"moderator\" not in c.lower()) for c in struct_num_cols), \\\n",
        "        \"Structural set accidentally contains text/moderator fields.\"\n",
        "\n",
        "print(\"\\n[OK] Ran structure-only, attrs-only, and combined with anti-leak guards. No TF-IDF used.\")\n",
        "\n",
        "# ---- 10) Optional isolations\n",
        "if _DO_ISOLATION_TEST:\n",
        "    print(\"\\n########## NUM-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=X_num_blk, X_nodecat_blk=csr_matrix((len(y),0)), X_edgecat_blk=csr_matrix((len(y),0)),\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"NUM-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n",
        "    print(\"\\n########## CATS-ONLY (isolation) ##########\")\n",
        "    eval_with_text_rebuilt_per_fold(\n",
        "        X_num_blk=csr_matrix((len(y),0)), X_nodecat_blk=X_nodecat_blk, X_edgecat_blk=X_edgecat_blk,\n",
        "        texts_series=texts_series_empty, X_struct_blk=None, y=y, tag=\"CATS-ONLY (isolation)\",\n",
        "        class_weight={0:1,1:2}, target_recall=0.85, groups=groups\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RuJmnYeqcoKv",
        "outputId": "ad997ec7-2ac9-4946-af3d-3a6f277da62d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[y] size: 770 | pos rate: 0.516\n",
            "[TREE-PURGE] Iter 0: dropping 2 feature(s) used in perfect val split(s): ['node_first_ts__sum', 'node_n_msgs__mean']\n",
            "[TREE-PURGE] Iter 1: dropping 2 feature(s) used in perfect val split(s): ['node_last_ts__sum', 'node_out_ratio__mean']\n",
            "[TREE-PURGE] Iter 2: dropping 2 feature(s) used in perfect val split(s): ['edge_end_ts__sum', 'node_n_out__mean']\n",
            "[TREE-PURGE] Iter 3: dropping 2 feature(s) used in perfect val split(s): ['edge_prompt_tokens__sum', 'edge_start_ts__sum']\n",
            "[TREE-PURGE] Iter 4: no perfect-val splits found; stopping.\n",
            "[WARN] 75 purge-kept cols missing from aligned_df_num; continuing without them.\n",
            "        (showing first 20): ['node_bps_mean__mean', 'node_bps_mean__std', 'node_bps_mean__min', 'node_bps_mean__max', 'node_bps_mean__sum', 'node_pps_mean__mean', 'node_pps_mean__std', 'node_pps_mean__min', 'node_pps_mean__max', 'node_pps_mean__sum', 'node_bytes_total__mean', 'node_bytes_total__std', 'node_bytes_total__min', 'node_bytes_total__max', 'node_bytes_total__sum', 'node_pkts_total__mean', 'node_pkts_total__std', 'node_pkts_total__min', 'node_pkts_total__max', 'node_pkts_total__sum']\n",
            "[SANITY] Forcing node/edge cat blocks to zero columns for leak isolation.\n",
            "[ATTR-ALIGNED] numeric: (770, 181) | node-cats: (770, 0) | edge-cats: (770, 0)\n",
            "[LEAK-POST] shapes: (770, 181) (770, 0) (770, 0)\n",
            "\n",
            "########## STRUCTURE-ONLY (topology pack; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  STRUCTURE-ONLY (topology pack; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.896              0.896     0.896           0.819              0.819     0.819           0.766              0.688     0.519           0.766              0.684     0.506           0.775              0.727     0.648           0.775              0.667     0.523           0.775              0.800     0.850            0.69              0.643       0.5\n",
            "2               0.924              0.924     0.924           0.895              0.895     0.895           0.779              0.688     0.636           0.775              0.680     0.623           0.805              0.750     0.736           0.745              0.643     0.591           0.875              0.900     0.975            0.70              0.634       0.5\n",
            "3               0.844              0.844     0.844           0.786              0.786     0.786           0.662              0.688     0.558           0.662              0.684     0.543           0.675              0.727     0.691           0.675              0.667     0.543           0.675              0.800     0.950            0.70              0.641       0.5\n",
            "4               0.917              0.917     0.917           0.858              0.858     0.858           0.740              0.662     0.584           0.737              0.656     0.570           0.767              0.717     0.704           0.717              0.635     0.559           0.825              0.825     0.950            0.69              0.635       0.5\n",
            "5               0.935              0.935     0.935           0.907              0.907     0.907           0.792              0.727     0.649           0.790              0.721     0.635           0.810              0.769     0.748           0.773              0.686     0.597           0.850              0.875     1.000            0.69              0.642       0.5\n",
            "6               0.895              0.895     0.895           0.861              0.861     0.861           0.818              0.701     0.584           0.816              0.693     0.570           0.833              0.758     0.704           0.795              0.655     0.559           0.875              0.900     0.950            0.69              0.632       0.5\n",
            "7               0.930              0.930     0.930           0.891              0.891     0.891           0.818              0.766     0.623           0.819              0.762     0.611           0.821              0.795     0.718           0.842              0.729     0.587           0.800              0.875     0.925            0.69              0.637       0.5\n",
            "8               0.868              0.868     0.868           0.812              0.812     0.812           0.740              0.701     0.610           0.741              0.700     0.606           0.737              0.736     0.712           0.757              0.667     0.569           0.718              0.821     0.949            0.70              0.645       0.5\n",
            "9               0.821              0.821     0.821           0.758              0.758     0.758           0.675              0.662     0.545           0.675              0.661     0.540           0.691              0.705     0.673           0.667              0.633     0.529           0.718              0.795     0.923            0.67              0.644       0.5\n",
            "10              0.931              0.931     0.931           0.897              0.897     0.897           0.792              0.688     0.558           0.791              0.686     0.553           0.805              0.745     0.691           0.767              0.636     0.535           0.846              0.897     0.974            0.70              0.646       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal          7.7         29.6\n",
            "True Attack          2.2         37.5\n",
            "Accuracy: 0.587 | Precision(attack): 0.559 | Recall(attack): 0.945 | Specificity(normal): 0.206\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.8         10.5\n",
            "True Attack          8.1         31.6\n",
            "Accuracy: 0.758 | Precision(attack): 0.751 | Recall(attack): 0.796 | Specificity(normal): 0.718\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         20.0         17.3\n",
            "True Attack          6.0         33.7\n",
            "Accuracy: 0.697 | Precision(attack): 0.661 | Recall(attack): 0.849 | Specificity(normal): 0.536\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.930              0.930     0.930           0.908              0.908     0.908           0.831              0.779     0.831           0.834              0.773     0.834           0.822              0.813     0.822           0.909              0.725     0.909           0.750              0.925     0.750            0.49              0.216       0.5\n",
            "2               0.925              0.925     0.925           0.891              0.891     0.891           0.805              0.701     0.857           0.801              0.693     0.857           0.828              0.758     0.861           0.766              0.655     0.872           0.900              0.900     0.850            0.39              0.204       0.5\n",
            "3               0.885              0.885     0.885           0.833              0.833     0.833           0.779              0.701     0.792           0.781              0.700     0.797           0.773              0.716     0.771           0.829              0.707     0.900           0.725              0.725     0.675            0.40              0.224       0.5\n",
            "4               0.920              0.920     0.920           0.874              0.874     0.874           0.805              0.662     0.831           0.803              0.655     0.832           0.819              0.723     0.831           0.791              0.630     0.865           0.850              0.850     0.800            0.43              0.290       0.5\n",
            "5               0.880              0.880     0.880           0.829              0.829     0.829           0.714              0.662     0.740           0.712              0.656     0.744           0.738              0.717     0.722           0.705              0.635     0.812           0.775              0.825     0.650            0.36              0.202       0.5\n",
            "6               0.859              0.859     0.859           0.825              0.825     0.825           0.701              0.766     0.714           0.706              0.761     0.718           0.667              0.800     0.694           0.793              0.720     0.781           0.575              0.900     0.625            0.60              0.208       0.5\n",
            "7               0.924              0.924     0.924           0.895              0.895     0.895           0.792              0.805     0.805           0.795              0.800     0.808           0.784              0.831     0.795           0.853              0.755     0.879           0.725              0.925     0.725            0.45              0.246       0.5\n",
            "8               0.889              0.889     0.889           0.861              0.861     0.861           0.766              0.714     0.766           0.768              0.712     0.768           0.727              0.761     0.735           0.889              0.660     0.862           0.615              0.897     0.641            0.68              0.202       0.5\n",
            "9               0.803              0.803     0.803           0.734              0.734     0.734           0.675              0.623     0.688           0.676              0.622     0.690           0.658              0.659     0.657           0.706              0.609     0.742           0.615              0.718     0.590            0.45              0.246       0.5\n",
            "10              0.919              0.919     0.919           0.882              0.882     0.882           0.818              0.727     0.805           0.818              0.725     0.805           0.816              0.764     0.805           0.838              0.680     0.816           0.795              0.872     0.795            0.53              0.234       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.1          5.2\n",
            "True Attack         11.5         28.2\n",
            "Accuracy: 0.783 | Precision(attack): 0.844 | Recall(attack): 0.710 | Specificity(normal): 0.861\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         30.1          7.2\n",
            "True Attack         10.6         29.1\n",
            "Accuracy: 0.769 | Precision(attack): 0.802 | Recall(attack): 0.733 | Specificity(normal): 0.807\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         21.1         16.2\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.714 | Precision(attack): 0.677 | Recall(attack): 0.854 | Specificity(normal): 0.566\n",
            "\n",
            "=== STRUCTURE-ONLY (topology pack; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.918              0.918     0.918           0.895              0.895     0.895           0.792              0.727     0.805           0.796              0.718     0.805           0.778              0.784     0.810           0.875              0.667     0.821           0.700              0.950     0.800            0.70              0.271       0.5\n",
            "2               0.924              0.924     0.924           0.893              0.893     0.893           0.779              0.662     0.779           0.776              0.652     0.776           0.800              0.740     0.800           0.756              0.617     0.756           0.850              0.925     0.850            0.52              0.204       0.5\n",
            "3               0.894              0.894     0.894           0.848              0.848     0.848           0.792              0.727     0.779           0.797              0.724     0.781           0.771              0.753     0.773           0.900              0.711     0.829           0.675              0.800     0.725            0.70              0.261       0.5\n",
            "4               0.900              0.900     0.900           0.839              0.839     0.839           0.753              0.623     0.714           0.752              0.613     0.711           0.765              0.707     0.744           0.756              0.593     0.696           0.775              0.875     0.800            0.64              0.294       0.5\n",
            "5               0.867              0.867     0.867           0.797              0.797     0.797           0.766              0.649     0.675           0.770              0.642     0.673           0.750              0.710     0.699           0.844              0.623     0.674           0.675              0.825     0.725            0.68              0.274       0.5\n",
            "6               0.875              0.875     0.875           0.843              0.843     0.843           0.740              0.727     0.727           0.745              0.719     0.725           0.714              0.779     0.747           0.833              0.673     0.721           0.625              0.925     0.775            0.67              0.276       0.5\n",
            "7               0.890              0.890     0.890           0.849              0.849     0.849           0.753              0.727     0.714           0.756              0.722     0.713           0.740              0.764     0.732           0.818              0.694     0.714           0.675              0.850     0.750            0.64              0.333       0.5\n",
            "8               0.872              0.872     0.872           0.828              0.828     0.828           0.727              0.701     0.740           0.728              0.699     0.741           0.712              0.742     0.730           0.765              0.660     0.771           0.667              0.846     0.692            0.55              0.263       0.5\n",
            "9               0.804              0.804     0.804           0.743              0.743     0.743           0.649              0.610     0.662           0.650              0.609     0.664           0.630              0.643     0.629           0.676              0.600     0.710           0.590              0.692     0.564            0.46              0.277       0.5\n",
            "10              0.924              0.924     0.924           0.893              0.893     0.893           0.766              0.662     0.740           0.765              0.659     0.739           0.786              0.729     0.767           0.733              0.614     0.702           0.846              0.897     0.846            0.53              0.341       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.6         10.7\n",
            "True Attack          9.8         29.9\n",
            "Accuracy: 0.734 | Precision(attack): 0.736 | Recall(attack): 0.753 | Specificity(normal): 0.713\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         29.8          7.5\n",
            "True Attack         11.6         28.1\n",
            "Accuracy: 0.752 | Precision(attack): 0.789 | Recall(attack): 0.708 | Specificity(normal): 0.799\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         18.4         18.9\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.682 | Precision(attack): 0.643 | Recall(attack): 0.859 | Specificity(normal): 0.493\n",
            "\n",
            "########## ATTRS-ONLY (numeric/node-cat/edge-cat; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  ATTRS-ONLY (NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.937              0.937     0.937           0.905              0.905     0.905           0.844              0.831     0.779           0.844              0.832     0.774           0.850              0.831     0.809           0.850              0.865     0.735           0.850              0.800     0.900            0.70              0.791       0.5\n",
            "2               0.956              0.956     0.956           0.952              0.952     0.952           0.896              0.883     0.831           0.895              0.882     0.826           0.902              0.889     0.854           0.881              0.878     0.776           0.925              0.900     0.950            0.68              0.714       0.5\n",
            "3               0.980              0.980     0.980           0.973              0.973     0.973           0.896              0.870     0.818           0.895              0.867     0.813           0.902              0.884     0.844           0.881              0.826     0.760           0.925              0.950     0.950            0.70              0.606       0.5\n",
            "4               0.975              0.975     0.975           0.970              0.970     0.970           0.883              0.883     0.857           0.883              0.883     0.853           0.886              0.886     0.874           0.897              0.897     0.809           0.875              0.875     0.950            0.69              0.691       0.5\n",
            "5               0.954              0.954     0.954           0.937              0.937     0.937           0.857              0.844     0.844           0.856              0.842     0.841           0.864              0.857     0.860           0.854              0.818     0.804           0.875              0.900     0.925            0.69              0.664       0.5\n",
            "6               0.915              0.915     0.915           0.905              0.905     0.905           0.805              0.831     0.779           0.803              0.830     0.773           0.819              0.840     0.813           0.791              0.829     0.725           0.850              0.850     0.925            0.67              0.706       0.5\n",
            "7               0.947              0.947     0.947           0.918              0.918     0.918           0.870              0.870     0.870           0.870              0.872     0.870           0.875              0.868     0.875           0.875              0.917     0.875           0.875              0.825     0.875            0.60              0.659       0.5\n",
            "8               0.929              0.929     0.929           0.896              0.896     0.896           0.844              0.857     0.792           0.845              0.858     0.792           0.838              0.849     0.800           0.886              0.912     0.780           0.795              0.795     0.821            0.70              0.726       0.5\n",
            "9               0.897              0.897     0.897           0.866              0.866     0.866           0.792              0.779     0.740           0.792              0.779     0.739           0.800              0.779     0.767           0.780              0.789     0.702           0.821              0.769     0.846            0.63              0.753       0.5\n",
            "10              0.957              0.957     0.957           0.940              0.940     0.940           0.857              0.844     0.844           0.857              0.844     0.843           0.857              0.846     0.857           0.868              0.846     0.800           0.846              0.846     0.923            0.70              0.628       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         26.8         10.5\n",
            "True Attack          3.7         36.0\n",
            "Accuracy: 0.816 | Precision(attack): 0.774 | Recall(attack): 0.907 | Specificity(normal): 0.718\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.5          5.8\n",
            "True Attack          5.4         34.3\n",
            "Accuracy: 0.855 | Precision(attack): 0.855 | Recall(attack): 0.864 | Specificity(normal): 0.845\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.6          5.7\n",
            "True Attack          5.9         33.8\n",
            "Accuracy: 0.849 | Precision(attack): 0.856 | Recall(attack): 0.851 | Specificity(normal): 0.847\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.960              0.960     0.960           0.949              0.949     0.949           0.883              0.870     0.857           0.884              0.873     0.860           0.883              0.865     0.849           0.919              0.941     0.939           0.850              0.800     0.775            0.40              0.476       0.5\n",
            "2               0.969              0.969     0.969           0.965              0.965     0.965           0.857              0.870     0.870           0.858              0.872     0.872           0.857              0.868     0.868           0.892              0.917     0.917           0.825              0.825     0.825            0.44              0.490       0.5\n",
            "3               0.971              0.971     0.971           0.964              0.964     0.964           0.896              0.896     0.883           0.896              0.898     0.885           0.900              0.895     0.880           0.900              0.944     0.943           0.900              0.850     0.825            0.39              0.482       0.5\n",
            "4               0.988              0.988     0.988           0.984              0.984     0.984           0.935              0.961     0.948           0.935              0.962     0.950           0.937              0.961     0.947           0.949              1.000     1.000           0.925              0.925     0.900            0.43              0.480       0.5\n",
            "5               0.976              0.976     0.976           0.974              0.974     0.974           0.870              0.883     0.909           0.865              0.879     0.908           0.889              0.897     0.914           0.800              0.830     0.902           1.000              0.975     0.925            0.33              0.422       0.5\n",
            "6               0.940              0.940     0.940           0.928              0.928     0.928           0.831              0.857     0.844           0.832              0.856     0.845           0.831              0.864     0.846           0.865              0.854     0.868           0.800              0.875     0.825            0.51              0.444       0.5\n",
            "7               0.977              0.977     0.977           0.963              0.963     0.963           0.935              0.948     0.935           0.936              0.950     0.938           0.935              0.947     0.933           0.973              1.000     1.000           0.900              0.900     0.875            0.43              0.458       0.5\n",
            "8               0.943              0.943     0.943           0.920              0.920     0.920           0.870              0.870     0.870           0.871              0.871     0.871           0.865              0.865     0.865           0.914              0.914     0.914           0.821              0.821     0.821            0.43              0.466       0.5\n",
            "9               0.936              0.936     0.936           0.928              0.928     0.928           0.831              0.805     0.792           0.832              0.806     0.794           0.827              0.789     0.771           0.861              0.875     0.871           0.795              0.718     0.692            0.42              0.458       0.5\n",
            "10              0.980              0.980     0.980           0.977              0.977     0.977           0.896              0.896     0.896           0.896              0.896     0.896           0.900              0.900     0.900           0.878              0.878     0.878           0.923              0.923     0.923            0.40              0.440       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.5          2.8\n",
            "True Attack          6.4         33.3\n",
            "Accuracy: 0.881 | Precision(attack): 0.922 | Recall(attack): 0.839 | Specificity(normal): 0.925\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.1          4.2\n",
            "True Attack          5.0         34.7\n",
            "Accuracy: 0.881 | Precision(attack): 0.892 | Recall(attack): 0.874 | Specificity(normal): 0.887\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.0          3.3\n",
            "True Attack          5.5         34.2\n",
            "Accuracy: 0.886 | Precision(attack): 0.912 | Recall(attack): 0.861 | Specificity(normal): 0.912\n",
            "\n",
            "=== ATTRS-ONLY (NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.967              0.967     0.967           0.960              0.960     0.960           0.883              0.870     0.883           0.884              0.872     0.884           0.883              0.868     0.883           0.919              0.917     0.919           0.850              0.825     0.850            0.41              0.649       0.5\n",
            "2               0.968              0.968     0.968           0.961              0.961     0.961           0.909              0.909     0.896           0.909              0.909     0.896           0.911              0.911     0.900           0.923              0.923     0.900           0.900              0.900     0.900            0.69              0.730       0.5\n",
            "3               0.987              0.987     0.987           0.984              0.984     0.984           0.935              0.935     0.922           0.936              0.936     0.923           0.935              0.935     0.923           0.973              0.973     0.947           0.900              0.900     0.900            0.60              0.615       0.5\n",
            "4               0.987              0.987     0.987           0.984              0.984     0.984           0.922              0.922     0.922           0.921              0.922     0.922           0.927              0.925     0.925           0.905              0.925     0.925           0.950              0.925     0.925            0.39              0.567       0.5\n",
            "5               0.966              0.966     0.966           0.957              0.957     0.957           0.870              0.870     0.870           0.869              0.870     0.869           0.878              0.875     0.878           0.857              0.875     0.857           0.900              0.875     0.900            0.53              0.702       0.5\n",
            "6               0.935              0.935     0.935           0.923              0.923     0.923           0.857              0.831     0.831           0.854              0.829     0.829           0.871              0.843     0.843           0.822              0.814     0.814           0.925              0.875     0.875            0.30              0.554       0.5\n",
            "7               0.972              0.972     0.972           0.960              0.960     0.960           0.909              0.909     0.909           0.909              0.910     0.909           0.911              0.909     0.911           0.923              0.946     0.923           0.900              0.875     0.900            0.42              0.602       0.5\n",
            "8               0.932              0.932     0.932           0.906              0.906     0.906           0.818              0.818     0.831           0.818              0.819     0.831           0.816              0.811     0.831           0.838              0.857     0.842           0.795              0.769     0.821            0.62              0.694       0.5\n",
            "9               0.928              0.928     0.928           0.912              0.912     0.912           0.818              0.805     0.805           0.820              0.807     0.807           0.788              0.776     0.783           0.963              0.929     0.900           0.667              0.667     0.692            0.70              0.680       0.5\n",
            "10              0.987              0.987     0.987           0.984              0.984     0.984           0.896              0.922     0.909           0.895              0.921     0.908           0.905              0.927     0.916           0.844              0.884     0.864           0.974              0.974     0.974            0.38              0.621       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         32.9          4.4\n",
            "True Attack          5.0         34.7\n",
            "Accuracy: 0.878 | Precision(attack): 0.887 | Recall(attack): 0.874 | Specificity(normal): 0.882\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.1          4.2\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.882 | Precision(attack): 0.892 | Recall(attack): 0.877 | Specificity(normal): 0.887\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.6          3.7\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.879 | Precision(attack): 0.902 | Recall(attack): 0.859 | Specificity(normal): 0.901\n",
            "\n",
            "########## COMBINED (STRUCTURE + ATTRS; NO TEXT) ##########\n",
            "\n",
            "\n",
            "====================  COMBINED (STRUCTURE + ATTRS; NO TEXT) (per-fold rebuilds + group-aware CV + train-picked thresholds)  ====================\n",
            "[NOTE] node/edge categorical bags not provided → using precomputed cat matrices (possible leakage if those were built globally).\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | LogReg: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.938              0.938     0.938           0.905              0.905     0.905           0.831              0.831     0.779           0.830              0.832     0.774           0.840              0.831     0.809           0.829              0.865     0.735           0.850              0.800     0.900            0.69              0.792       0.5\n",
            "2               0.956              0.956     0.956           0.953              0.953     0.953           0.883              0.883     0.844           0.881              0.882     0.840           0.892              0.889     0.864           0.860              0.878     0.792           0.925              0.900     0.950            0.67              0.717       0.5\n",
            "3               0.982              0.982     0.982           0.976              0.976     0.976           0.909              0.870     0.805           0.907              0.867     0.799           0.916              0.884     0.835           0.884              0.826     0.745           0.950              0.950     0.950            0.70              0.610       0.5\n",
            "4               0.975              0.975     0.975           0.970              0.970     0.970           0.883              0.883     0.857           0.883              0.883     0.853           0.886              0.886     0.874           0.897              0.897     0.809           0.875              0.875     0.950            0.70              0.701       0.5\n",
            "5               0.950              0.950     0.950           0.938              0.938     0.938           0.857              0.857     0.857           0.856              0.854     0.853           0.864              0.871     0.874           0.854              0.822     0.809           0.875              0.925     0.950            0.70              0.661       0.5\n",
            "6               0.914              0.914     0.914           0.904              0.904     0.904           0.831              0.831     0.779           0.830              0.830     0.773           0.840              0.840     0.813           0.829              0.829     0.725           0.850              0.850     0.925            0.70              0.694       0.5\n",
            "7               0.952              0.952     0.952           0.924              0.924     0.924           0.883              0.857     0.883           0.883              0.858     0.883           0.886              0.857     0.886           0.897              0.892     0.897           0.875              0.825     0.875            0.58              0.657       0.5\n",
            "8               0.929              0.929     0.929           0.895              0.895     0.895           0.844              0.857     0.792           0.845              0.858     0.792           0.838              0.849     0.800           0.886              0.912     0.780           0.795              0.795     0.821            0.69              0.727       0.5\n",
            "9               0.903              0.903     0.903           0.874              0.874     0.874           0.805              0.792     0.753           0.805              0.793     0.752           0.810              0.789     0.776           0.800              0.811     0.717           0.821              0.769     0.846            0.63              0.753       0.5\n",
            "10              0.955              0.955     0.955           0.938              0.938     0.938           0.857              0.844     0.831           0.857              0.844     0.830           0.857              0.846     0.843           0.868              0.846     0.795           0.846              0.846     0.897            0.69              0.647       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         27.0         10.3\n",
            "True Attack          3.7         36.0\n",
            "Accuracy: 0.818 | Precision(attack): 0.778 | Recall(attack): 0.907 | Specificity(normal): 0.724\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.7          5.6\n",
            "True Attack          5.3         34.4\n",
            "Accuracy: 0.858 | Precision(attack): 0.860 | Recall(attack): 0.866 | Specificity(normal): 0.850\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         31.6          5.7\n",
            "True Attack          5.8         33.9\n",
            "Accuracy: 0.851 | Precision(attack): 0.856 | Recall(attack): 0.854 | Specificity(normal): 0.847\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | RF: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.960              0.960     0.960           0.950              0.950     0.950           0.883              0.870     0.844           0.884              0.872     0.848           0.883              0.868     0.833           0.919              0.917     0.938           0.850              0.825     0.750            0.39              0.470       0.5\n",
            "2               0.968              0.968     0.968           0.965              0.965     0.965           0.870              0.870     0.870           0.872              0.873     0.873           0.868              0.865     0.865           0.917              0.941     0.941           0.825              0.800     0.800            0.43              0.492       0.5\n",
            "3               0.970              0.970     0.970           0.961              0.961     0.961           0.922              0.896     0.896           0.923              0.898     0.898           0.923              0.895     0.895           0.947              0.944     0.944           0.900              0.850     0.850            0.42              0.480       0.5\n",
            "4               0.986              0.986     0.986           0.980              0.980     0.980           0.935              0.948     0.935           0.935              0.949     0.938           0.937              0.949     0.933           0.949              0.974     1.000           0.925              0.925     0.875            0.42              0.462       0.5\n",
            "5               0.982              0.982     0.982           0.980              0.980     0.980           0.896              0.909     0.935           0.892              0.906     0.934           0.909              0.918     0.938           0.833              0.867     0.927           1.000              0.975     0.950            0.38              0.444       0.5\n",
            "6               0.943              0.943     0.943           0.933              0.933     0.933           0.805              0.857     0.818           0.808              0.855     0.820           0.795              0.867     0.816           0.879              0.837     0.861           0.725              0.900     0.775            0.53              0.432       0.5\n",
            "7               0.979              0.979     0.979           0.967              0.967     0.967           0.935              0.948     0.935           0.936              0.950     0.938           0.935              0.947     0.933           0.973              1.000     1.000           0.900              0.900     0.875            0.38              0.440       0.5\n",
            "8               0.946              0.946     0.946           0.926              0.926     0.926           0.844              0.857     0.844           0.844              0.858     0.845           0.842              0.849     0.833           0.865              0.912     0.909           0.821              0.795     0.769            0.41              0.492       0.5\n",
            "9               0.935              0.935     0.935           0.926              0.926     0.926           0.818              0.805     0.805           0.818              0.807     0.807           0.816              0.783     0.783           0.838              0.900     0.900           0.795              0.692     0.692            0.37              0.480       0.5\n",
            "10              0.979              0.979     0.979           0.976              0.976     0.976           0.896              0.896     0.896           0.896              0.896     0.896           0.900              0.900     0.900           0.878              0.878     0.878           0.923              0.923     0.923            0.40              0.464       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.8          2.5\n",
            "True Attack          6.9         32.8\n",
            "Accuracy: 0.878 | Precision(attack): 0.929 | Recall(attack): 0.826 | Specificity(normal): 0.933\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.4          3.9\n",
            "True Attack          5.3         34.4\n",
            "Accuracy: 0.881 | Precision(attack): 0.898 | Recall(attack): 0.866 | Specificity(normal): 0.895\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.1          3.2\n",
            "True Attack          5.6         34.1\n",
            "Accuracy: 0.886 | Precision(attack): 0.914 | Recall(attack): 0.859 | Specificity(normal): 0.914\n",
            "\n",
            "=== COMBINED (STRUCTURE + ATTRS; NO TEXT) | HGB: Per-fold metrics (K=10) ===\n",
            "                   AP                                          AUC                                          Acc                                       BalAcc                                           F1                                         Prec                                          Rec                                          thr                             \n",
            "which F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50 F1-tuned(train) Recall≥0.85(train) fixed0.50\n",
            "fold                                                                                                                                                                                                                                                                                                                                                                         \n",
            "1               0.972              0.972     0.972           0.967              0.967     0.967           0.883              0.883     0.857           0.885              0.885     0.858           0.880              0.880     0.857           0.943              0.943     0.892           0.825              0.825     0.825            0.69              0.728       0.5\n",
            "2               0.976              0.976     0.976           0.971              0.971     0.971           0.922              0.922     0.922           0.922              0.922     0.922           0.925              0.925     0.925           0.925              0.925     0.925           0.925              0.925     0.925            0.57              0.664       0.5\n",
            "3               0.982              0.982     0.982           0.978              0.978     0.978           0.909              0.909     0.922           0.910              0.910     0.923           0.909              0.909     0.923           0.946              0.946     0.947           0.875              0.875     0.900            0.57              0.608       0.5\n",
            "4               0.986              0.986     0.986           0.983              0.983     0.983           0.922              0.922     0.922           0.922              0.923     0.922           0.925              0.923     0.925           0.925              0.947     0.925           0.925              0.900     0.925            0.51              0.622       0.5\n",
            "5               0.966              0.966     0.966           0.957              0.957     0.957           0.870              0.883     0.870           0.869              0.882     0.869           0.878              0.889     0.878           0.857              0.878     0.857           0.900              0.900     0.900            0.49              0.612       0.5\n",
            "6               0.947              0.947     0.947           0.940              0.940     0.940           0.883              0.870     0.870           0.881              0.868     0.868           0.892              0.881     0.881           0.860              0.841     0.841           0.925              0.925     0.925            0.57              0.484       0.5\n",
            "7               0.970              0.970     0.970           0.957              0.957     0.957           0.909              0.909     0.922           0.910              0.910     0.923           0.909              0.909     0.923           0.946              0.946     0.947           0.875              0.875     0.900            0.56              0.557       0.5\n",
            "8               0.941              0.941     0.941           0.919              0.919     0.919           0.883              0.883     0.831           0.884              0.884     0.831           0.880              0.880     0.835           0.917              0.917     0.825           0.846              0.846     0.846            0.69              0.727       0.5\n",
            "9               0.946              0.946     0.946           0.937              0.937     0.937           0.818              0.805     0.792           0.820              0.807     0.794           0.788              0.776     0.765           0.963              0.929     0.897           0.667              0.667     0.667            0.68              0.548       0.5\n",
            "10              0.989              0.989     0.989           0.987              0.987     0.987           0.922              0.909     0.909           0.921              0.909     0.909           0.927              0.911     0.914           0.884              0.900     0.881           0.974              0.923     0.949            0.30              0.625       0.5\n",
            "\n",
            "=== Mean CM (fixed 0.50) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         33.1          4.2\n",
            "True Attack          4.9         34.8\n",
            "Accuracy: 0.882 | Precision(attack): 0.892 | Recall(attack): 0.877 | Specificity(normal): 0.887\n",
            "\n",
            "=== Mean CM (F1-tuned on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.0          3.3\n",
            "True Attack          5.0         34.7\n",
            "Accuracy: 0.892 | Precision(attack): 0.913 | Recall(attack): 0.874 | Specificity(normal): 0.912\n",
            "\n",
            "=== Mean CM (Recall≥0.85 on train) ===\n",
            "rows=true [normal, attack]; cols=pred [normal, attack]\n",
            "             Pred Normal  Pred Attack\n",
            "True Normal         34.1          3.2\n",
            "True Attack          5.3         34.4\n",
            "Accuracy: 0.890 | Precision(attack): 0.915 | Recall(attack): 0.866 | Specificity(normal): 0.914\n",
            "\n",
            "[OK] Ran structure-only, attrs-only, and combined with anti-leak guards. No TF-IDF used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "########### Now lets attribute them"
      ],
      "metadata": {
        "id": "yeVO0a9hjuS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================\n",
        "# Explainability (leak-safe CV)\n",
        "# ==============================\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, accuracy_score\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib, os, json, warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ---------- 0) Inputs assumed from your session ----------\n",
        "# merged, df_num, groups (optional), X_struct_blk (optional)\n",
        "# y was defined in your eval code. If not, define here:\n",
        "if 'y' not in globals():\n",
        "    y = (merged[\"cohort\"].astype(str).str.lower() == \"attack\").astype(int).values\n",
        "\n",
        "# We’ll explain the ATTRS-only numeric block you already built:\n",
        "# aligned_df_num and keep_num_cols were produced above. If not, recompute minimal alignment:\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "assert \"file\" in df_num.columns, \"df_num must contain 'file'\"\n",
        "aligned_df_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "\n",
        "# numeric feature set (exclude obvious identifiers)\n",
        "num_cols = [c for c in aligned_df_num.columns\n",
        "            if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "            and pd.api.types.is_numeric_dtype(aligned_df_num[c])]\n",
        "\n",
        "X_df = aligned_df_num[num_cols].copy()\n",
        "\n",
        "# Sanitize exactly like your eval (keep behavior consistent)\n",
        "def _sanitize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    Z = df.replace([np.inf, -np.inf], np.nan)\n",
        "    med = Z.median(numeric_only=True)\n",
        "    Z = Z.fillna(med)\n",
        "    q01 = Z.quantile(0.01, numeric_only=True)\n",
        "    q99 = Z.quantile(0.99, numeric_only=True)\n",
        "    for c in Z.columns:\n",
        "        lo = q01.get(c, None); hi = q99.get(c, None)\n",
        "        if lo is not None and hi is not None and np.isfinite([lo,hi]).all():\n",
        "            Z[c] = Z[c].clip(lo, hi)\n",
        "    return Z.astype(np.float64)\n",
        "\n",
        "X_df = _sanitize_df(X_df)\n",
        "\n",
        "# ---------- 1) CV splitter (same logic you used) ----------\n",
        "def _cv_splitter(n_splits=10):\n",
        "    g = globals().get(\"groups\", None)\n",
        "    if g is not None and len(np.unique(g)) > 1:\n",
        "        try:\n",
        "            return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y, g)\n",
        "        except Exception:\n",
        "            return GroupKFold(n_splits=n_splits).split(np.zeros_like(y), g)\n",
        "    else:\n",
        "        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y)\n",
        "\n",
        "# ---------- 2) Models we’ll explain ----------\n",
        "models = {\n",
        "    \"HGB\": HistGradientBoostingClassifier(max_depth=None, learning_rate=0.1, max_iter=300, random_state=42),\n",
        "    \"RF\":  RandomForestClassifier(n_estimators=600, max_depth=None, min_samples_leaf=2, n_jobs=-1, random_state=42),\n",
        "}\n",
        "\n",
        "# ---------- 3) Storage ----------\n",
        "out_dir = pathlib.Path(\"explain_report\")\n",
        "out_dir.mkdir(exist_ok=True)\n",
        "feature_names = X_df.columns.tolist()\n",
        "\n",
        "def _safe_to_dense(X):\n",
        "    return X.toarray() if issparse(X) else np.asarray(X)\n",
        "\n",
        "# ---------- 4) Run CV → fit on train, EXPLAIN on validation ----------\n",
        "results = {}\n",
        "for name, base_model in models.items():\n",
        "    print(f\"\\n=== Explainability for {name} ===\")\n",
        "    fold_importance_perm = []     # permutation importance on val\n",
        "    fold_shap_meanabs = []        # mean |SHAP| on val\n",
        "    shap_samples_per_fold = []    # keep a slice of local SHAP (for beeswarm)\n",
        "    metrics = []\n",
        "\n",
        "    for k, (tr_idx, va_idx) in enumerate(_cv_splitter(), start=1):\n",
        "        X_tr, y_tr = X_df.iloc[tr_idx], y[tr_idx]\n",
        "        X_va, y_va = X_df.iloc[va_idx], y[va_idx]\n",
        "\n",
        "        # Fit\n",
        "        model = base_model.__class__(**base_model.get_params())\n",
        "        model.fit(X_tr, y_tr)\n",
        "\n",
        "        # Predict on validation\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            p_va = model.predict_proba(X_va)[:,1]\n",
        "        else:\n",
        "            # HGB has predict_proba; but guard anyway with decision_function\n",
        "            try:\n",
        "                p_va = model.predict_proba(X_va)[:,1]\n",
        "            except Exception:\n",
        "                p_raw = model.decision_function(X_va)\n",
        "                # map to 0..1\n",
        "                p_va = (p_raw - p_raw.min())/(p_raw.max()-p_raw.min()+1e-12)\n",
        "\n",
        "        y_hat = (p_va >= 0.5).astype(int)\n",
        "\n",
        "        # Fold metrics (sanity)\n",
        "        metrics.append({\n",
        "            \"fold\": k,\n",
        "            \"auc\": roc_auc_score(y_va, p_va),\n",
        "            \"ap\":  average_precision_score(y_va, p_va),\n",
        "            \"acc\": accuracy_score(y_va, y_hat),\n",
        "            \"f1\":  f1_score(y_va, y_hat),\n",
        "        })\n",
        "\n",
        "        # ---- Permutation importance (on validation only) ----\n",
        "        pi = permutation_importance(model, X_va, y_va, scoring=\"roc_auc\", n_repeats=20, random_state=123)\n",
        "        imp_df = pd.DataFrame({\n",
        "            \"feature\": feature_names,\n",
        "            \"importance_mean\": pi.importances_mean,\n",
        "            \"importance_std\":  pi.importances_std,\n",
        "            \"fold\": k\n",
        "        }).sort_values(\"importance_mean\", ascending=False)\n",
        "        fold_importance_perm.append(imp_df)\n",
        "\n",
        "        # ---- SHAP on validation ----\n",
        "        # TreeExplainer handles RF & (most) GBDTs well.\n",
        "        try:\n",
        "            explainer = shap.TreeExplainer(model)\n",
        "            # Sample a subset if val is large (keep runtime predictable)\n",
        "            take = min(300, len(X_va))\n",
        "            idx_sub = np.random.RandomState(777+k).choice(len(X_va), size=take, replace=False)\n",
        "            X_sub = X_va.iloc[idx_sub]\n",
        "            shap_vals = explainer.shap_values(X_sub)\n",
        "            # For binary: shap_values can be array or list; unify to positive-class contribs\n",
        "            if isinstance(shap_vals, list) and len(shap_vals)==2:\n",
        "                shap_pos = shap_vals[1]\n",
        "            else:\n",
        "                shap_pos = shap_vals\n",
        "            mean_abs = np.abs(shap_pos).mean(axis=0)\n",
        "            fold_shap_meanabs.append(pd.DataFrame({\n",
        "                \"feature\": feature_names,\n",
        "                \"mean_abs_shap\": mean_abs,\n",
        "                \"fold\": k\n",
        "            }).sort_values(\"mean_abs_shap\", ascending=False))\n",
        "\n",
        "            shap_samples_per_fold.append((X_sub, shap_pos))\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN][fold {k}] SHAP failed: {e}\")\n",
        "\n",
        "    # Aggregate across folds\n",
        "    perm_all = pd.concat(fold_importance_perm, ignore_index=True)\n",
        "    shap_all = pd.concat(fold_shap_meanabs, ignore_index=True) if fold_shap_meanabs else None\n",
        "    perm_rank = (perm_all.groupby(\"feature\")[\"importance_mean\"]\n",
        "                       .agg([\"mean\",\"std\"])\n",
        "                       .sort_values(\"mean\", ascending=False)\n",
        "                       .reset_index())\n",
        "    if shap_all is not None:\n",
        "        shap_rank = (shap_all.groupby(\"feature\")[\"mean_abs_shap\"]\n",
        "                            .agg([\"mean\",\"std\"])\n",
        "                            .sort_values(\"mean\", ascending=False)\n",
        "                            .reset_index())\n",
        "    else:\n",
        "        shap_rank = None\n",
        "\n",
        "    # Save tables\n",
        "    perm_rank.to_csv(out_dir / f\"{name}_perm_importance.csv\", index=False)\n",
        "    if shap_rank is not None:\n",
        "        shap_rank.to_csv(out_dir / f\"{name}_shap_importance.csv\", index=False)\n",
        "    pd.DataFrame(metrics).to_csv(out_dir / f\"{name}_fold_metrics.csv\", index=False)\n",
        "\n",
        "    # Quick prints\n",
        "    print(\"\\nTop 15 – permutation (val):\")\n",
        "    display(perm_rank.head(15))\n",
        "    if shap_rank is not None:\n",
        "        print(\"\\nTop 15 – SHAP |mean| (val):\")\n",
        "        display(shap_rank.head(15))\n",
        "\n",
        "    # ---- Global plots ----\n",
        "    # Permutation bar\n",
        "    plt.figure(figsize=(8,6))\n",
        "    top = perm_rank.head(20)\n",
        "    plt.barh(top[\"feature\"][::-1], top[\"mean\"][::-1])\n",
        "    plt.title(f\"{name} – Permutation importance (validation, mean AUC drop)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_dir / f\"{name}_perm_bar.png\", dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # SHAP beeswarm (aggregate a few folds’ samples)\n",
        "    if shap_samples_per_fold:\n",
        "        X_bee = pd.concat([x for x,_ in shap_samples_per_fold], axis=0)\n",
        "        shap_bee = np.vstack([s for _,s in shap_samples_per_fold])\n",
        "        shap.summary_plot(shap_bee, X_bee, feature_names=feature_names, show=False, max_display=20)\n",
        "        plt.title(f\"{name} – SHAP beeswarm (validation samples)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"{name}_shap_beeswarm.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "        # SHAP bar (top 20)\n",
        "        shap.summary_plot(shap_bee, X_bee, feature_names=feature_names, show=False, plot_type=\"bar\", max_display=20)\n",
        "        plt.title(f\"{name} – SHAP bar (validation samples)\")\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out_dir / f\"{name}_shap_bar.png\", dpi=150, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "\n",
        "    # ---- PDP/ICE for the top 6 perm features ----\n",
        "    top_feats = top[\"feature\"].head(6).tolist()\n",
        "    # fit once on ALL data for smoother PDP (PDP isn't used for evaluation, only visualization)\n",
        "    model_all = base_model.__class__(**base_model.get_params())\n",
        "    model_all.fit(X_df, y)\n",
        "    for f in top_feats:\n",
        "        try:\n",
        "            disp = PartialDependenceDisplay.from_estimator(model_all, X_df, [f], kind=\"both\", subsample=1000, random_state=0)\n",
        "            disp.figure_.suptitle(f\"{name} – PDP/ICE: {f}\")\n",
        "            disp.figure_.tight_layout()\n",
        "            disp.figure_.savefig(out_dir / f\"{name}_pdp_{f}.png\", dpi=150)\n",
        "            plt.close(disp.figure_)\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] PDP failed for {f}: {e}\")\n",
        "\n",
        "    # Store in results dict\n",
        "    results[name] = {\n",
        "        \"metrics\": metrics,\n",
        "        \"perm_rank\": perm_rank,\n",
        "        \"shap_rank\": shap_rank,\n",
        "        \"plots\": {\n",
        "            \"perm_bar\": str(out_dir / f\"{name}_perm_bar.png\"),\n",
        "            \"shap_beeswarm\": str(out_dir / f\"{name}_shap_beeswarm.png\") if shap_samples_per_fold else None,\n",
        "            \"shap_bar\": str(out_dir / f\"{name}_shap_bar.png\") if shap_samples_per_fold else None,\n",
        "            \"pdps\": [str(out_dir / f\"{name}_pdp_{f}.png\") for f in top_feats]\n",
        "        }\n",
        "    }\n",
        "\n",
        "# ---------- 5) (Optional) Minimal HTML report ----------\n",
        "html = [\"<html><head><meta charset='utf-8'><title>Explainability Report</title></head><body>\"]\n",
        "html.append(\"<h1>Explainability Report</h1>\")\n",
        "for name, res in results.items():\n",
        "    html.append(f\"<h2>{name}</h2>\")\n",
        "    html.append(\"<h3>Permutation Importance (validation mean)</h3>\")\n",
        "    html.append(f\"<img src='{os.path.basename(res['plots']['perm_bar'])}' width='800'/>\")\n",
        "    if res['plots']['shap_bar']:\n",
        "        html.append(\"<h3>SHAP (validation samples)</h3>\")\n",
        "        html.append(f\"<img src='{os.path.basename(res['plots']['shap_bar'])}' width='800'/>\")\n",
        "        html.append(f\"<img src='{os.path.basename(res['plots']['shap_beeswarm'])}' width='800'/>\")\n",
        "    html.append(\"<h3>PDP/ICE (top features)</h3>\")\n",
        "    for p in res['plots']['pdps']:\n",
        "        html.append(f\"<img src='{os.path.basename(p)}' width='600' style='margin:8px'/>\")\n",
        "html.append(\"</body></html>\")\n",
        "\n",
        "with open(out_dir / \"index.html\", \"w\") as f:\n",
        "    f.write(\"\\n\".join(html))\n",
        "\n",
        "print(f\"\\nReport written to: {out_dir/'index.html'}\")\n"
      ],
      "metadata": {
        "id": "PZ1FeBnGjuYW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "e2f484fe-f3c7-4cd2-efd9-004a73f1bfd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_num' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1482426128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# aligned_df_num and keep_num_cols were produced above. If not, recompute minimal alignment:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mfile_col_m\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m\"file\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"df_num must contain 'file'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0maligned_df_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_num\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmerged\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_num' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#a local SHAP breakdown (top features + waterfall) from the ATTRS model\n",
        "\n",
        "# simple what-if deltas (replace each feature with a neutral value and see how the attack score changes)\n",
        "\n",
        "# a quick block attribution: “how much of the score seems attributable to ATTRS vs STRUCTURE” using the same CV fold where that graph was in validation\n",
        "\n",
        "# This uses only the validation fold for the chosen sample, so it stays leakage-safe."
      ],
      "metadata": {
        "id": "Rn9z6yz-jud8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, GroupKFold, StratifiedGroupKFold\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from scipy.sparse import issparse, csr_matrix\n",
        "import shap, matplotlib.pyplot as plt\n",
        "\n",
        "# ---------- Inputs assumed ----------\n",
        "# merged, df_num, X_struct_blk (optional), groups (optional), and y already exist from your run.\n",
        "# We'll identify the graph by its file/path value:\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "\n",
        "# Pick the specific graph you care about:\n",
        "GRAPH_ID = merged.loc[y==1, file_col_m].iloc[0]  # <-- change this to the actual file/path you want to explain\n",
        "\n",
        "# ---------- Prepare aligned ATTRS matrix (same sanitization as eval) ----------\n",
        "def _sanitize_df(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    Z = df.replace([np.inf, -np.inf], np.nan)\n",
        "    med = Z.median(numeric_only=True)\n",
        "    Z = Z.fillna(med)\n",
        "    q01 = Z.quantile(0.01, numeric_only=True)\n",
        "    q99 = Z.quantile(0.99, numeric_only=True)\n",
        "    for c in Z.columns:\n",
        "        lo = q01.get(c, None); hi = q99.get(c, None)\n",
        "        if lo is not None and hi is not None and np.isfinite([lo,hi]).all():\n",
        "            Z[c] = Z[c].clip(lo, hi)\n",
        "    return Z.astype(np.float64)\n",
        "\n",
        "assert \"file\" in df_num.columns, \"df_num must contain 'file'\"\n",
        "aligned_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "\n",
        "num_cols = [c for c in aligned_num.columns\n",
        "            if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "            and pd.api.types.is_numeric_dtype(aligned_num[c])]\n",
        "X_attr = _sanitize_df(aligned_num[num_cols].copy())\n",
        "feat_names = X_attr.columns.tolist()\n",
        "\n",
        "# STRUCT block (optional)\n",
        "has_struct = ('X_struct_blk' in globals()) and (X_struct_blk is not None) and (X_struct_blk.shape[0]==len(merged))\n",
        "\n",
        "# locate the row for this graph\n",
        "row_idx = int(merged.index[merged[file_col_m]==GRAPH_ID][0])\n",
        "\n",
        "# ---------- CV splitter (same logic) ----------\n",
        "def _cv_splitter(n_splits=10):\n",
        "    g = globals().get(\"groups\", None)\n",
        "    if g is not None and len(np.unique(g)) > 1:\n",
        "        try:\n",
        "            return StratifiedGroupKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y, g)\n",
        "        except Exception:\n",
        "            return GroupKFold(n_splits=n_splits).split(np.zeros_like(y), g)\n",
        "    else:\n",
        "        return StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42).split(np.zeros_like(y), y)\n",
        "\n",
        "# ---------- Find the fold where this row is in validation ----------\n",
        "fold_found = None\n",
        "for k, (tr_idx, va_idx) in enumerate(_cv_splitter(), start=1):\n",
        "    if row_idx in va_idx:\n",
        "        fold_found = (k, tr_idx, va_idx)\n",
        "        break\n",
        "assert fold_found is not None, \"Row was not found in any validation split.\"\n",
        "k, tr_idx, va_idx = fold_found\n",
        "print(f\"[INFO] Explaining row {row_idx} (file={GRAPH_ID}) in validation fold {k}\")\n",
        "\n",
        "# ---------- Train models on that fold's TRAIN only ----------\n",
        "Xtr_attr, ytr = X_attr.iloc[tr_idx], y[tr_idx]\n",
        "Xva_attr, yva = X_attr.iloc[va_idx], y[va_idx]\n",
        "\n",
        "# ATTRS model (tree, good SHAP support)\n",
        "attr_model = HistGradientBoostingClassifier(max_iter=300, random_state=42)\n",
        "attr_model.fit(Xtr_attr, ytr)\n",
        "\n",
        "# STRUCT model (optional): simple RF on structure block if available\n",
        "if has_struct:\n",
        "    X_struct = X_struct_blk\n",
        "    # slice to fold\n",
        "    Xtr_struct = X_struct[tr_idx]\n",
        "    Xva_struct = X_struct[va_idx]\n",
        "    # RandomForest can consume dense or sparse; ensure CSR\n",
        "    if not issparse(Xtr_struct): Xtr_struct = csr_matrix(Xtr_struct)\n",
        "    if not issparse(Xva_struct): Xva_struct = csr_matrix(Xva_struct)\n",
        "    struct_model = RandomForestClassifier(n_estimators=500, min_samples_leaf=2, n_jobs=-1, random_state=42)\n",
        "    struct_model.fit(Xtr_struct, ytr)\n",
        "else:\n",
        "    struct_model = None\n",
        "\n",
        "# ---------- Get predictions for our target row ----------\n",
        "i_local = int(np.where(va_idx==row_idx)[0][0])  # index inside this fold's validation arrays\n",
        "p_attr = attr_model.predict_proba(Xva_attr)[:,1][i_local]\n",
        "print(f\"[ATTR] P(attack) for this graph: {p_attr:.3f}\")\n",
        "\n",
        "if struct_model is not None:\n",
        "    p_struct = struct_model.predict_proba(Xva_struct)[:,1][i_local]\n",
        "    print(f\"[STRUCT] P(attack) (structure-only model): {p_struct:.3f}\")\n",
        "else:\n",
        "    p_struct = np.nan\n",
        "\n",
        "# ---------- Local SHAP (ATTRS) ----------\n",
        "try:\n",
        "    explainer = shap.Explainer(attr_model, Xtr_attr, feature_names=feat_names)\n",
        "    shap_vals = explainer(Xva_attr.iloc[[i_local]])\n",
        "    # Waterfall (saved to disk)\n",
        "    shap.plots.waterfall(shap_vals[0], show=False, max_display=20)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(\"local_attr_shap_waterfall.png\", dpi=150, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    # Top contributions table\n",
        "    contrib = pd.DataFrame({\n",
        "        \"feature\": feat_names,\n",
        "        \"shap_value\": shap_vals.values[0],\n",
        "        \"value\": Xva_attr.iloc[i_local].values\n",
        "    }).sort_values(\"shap_value\", key=np.abs, ascending=False)\n",
        "    print(\"\\nTop local contributors (ATTRS):\")\n",
        "    display(contrib.head(15))\n",
        "except Exception as e:\n",
        "    print(f\"[WARN] SHAP local failed: {e}\")\n",
        "\n",
        "# ---------- Local what-if deltas (ATTRS) ----------\n",
        "# For each top-k feature, replace its value with a neutral baseline (train median) and measure delta in P(attack)\n",
        "k_top = 10\n",
        "train_medians = Xtr_attr.median()\n",
        "baseline_row = Xva_attr.iloc[i_local].copy()\n",
        "\n",
        "deltas = []\n",
        "for f in contrib.head(k_top)[\"feature\"]:\n",
        "    x_mod = baseline_row.copy()\n",
        "    x_mod[f] = train_medians[f]\n",
        "    p_mod = attr_model.predict_proba(pd.DataFrame([x_mod], columns=feat_names))[:,1][0]\n",
        "    deltas.append((f, float(p_attr - p_mod)))\n",
        "whatif = pd.DataFrame(deltas, columns=[\"feature\",\"delta_prob_if_set_to_train_median\"])\\\n",
        "         .sort_values(\"delta_prob_if_set_to_train_median\", ascending=False)\n",
        "print(\"\\nLocal what-if (ATTRS): how much the attack probability would DROP if we neutralize the feature:\")\n",
        "display(whatif)\n",
        "\n",
        "# ---------- Simple block attribution (probability-level) ----------\n",
        "# Not a perfect decomposition, but useful intuition:\n",
        "#   ATTRS-only prob vs STRUCT-only prob. Higher => the block is strongly predictive for THIS graph.\n",
        "print(\"\\n=== Block-level signal (rough) ===\")\n",
        "if struct_model is None:\n",
        "    print(f\"- ATTRS model P(attack)={p_attr:.3f}. STRUCT model not available in this session.\")\n",
        "else:\n",
        "    print(f\"- ATTRS P={p_attr:.3f} | STRUCT P={p_struct:.3f} (same validation fold)\")\n",
        "print(\"Saved figure: local_attr_shap_waterfall.png\")\n"
      ],
      "metadata": {
        "id": "_rfPzHpMmT59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- pick a graph the model really flags as attack (same fold k you just used) ---\n",
        "assert list(Xva_attr.columns) == list(Xtr_attr.columns)\n",
        "assert Xva_attr.shape[1] == Xtr_attr.shape[1]\n",
        "\n",
        "import shap\n",
        "import numpy as np\n",
        "from scipy.special import expit\n",
        "\n",
        "# Optional: small background sample for speed (but from TRAIN, not val)\n",
        "bg = Xtr_attr\n",
        "if len(bg) > 2000:\n",
        "    bg = bg.sample(2000, random_state=0)\n",
        "\n",
        "# IMPORTANT: for tree models, use TreeExplainer with the training matrix as background\n",
        "explainer = shap.TreeExplainer(\n",
        "    attr_model,       # your fitted HGB model\n",
        "    data=bg.values,   # background must match training preprocessing\n",
        "    feature_names=Xtr_attr.columns.tolist(),\n",
        "    model_output=\"raw\"   # SHAP sums to the raw (log-odds) margin\n",
        ")"
      ],
      "metadata": {
        "id": "ItyKg5dumT9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "row = Xva_attr.iloc[[i_local]].values\n",
        "sh = explainer(row, check_additivity=False)   # <= key change\n",
        "\n",
        "phi   = sh.values[0]         # shap values per feature (raw margin space)\n",
        "base  = float(sh.base_values[0])\n",
        "margin = base + phi.sum()    # should be very close to model’s raw output\n",
        "\n",
        "# Map margin → probability so you can compare to predict_proba\n",
        "p_from_shap = float(expit(margin))\n",
        "p_model     = float(attr_model.predict_proba(row)[:,1])\n",
        "\n",
        "print(f\"[DEBUG] p_model={p_model:.6f} | p_from_shap={p_from_shap:.6f} | base(raw)={base:.6f}\")\n"
      ],
      "metadata": {
        "id": "qVUjZcTwmT_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Positive phi => pushes toward attack; Negative => toward normal\n",
        "order = np.argsort(-phi)   # descending by contribution\n",
        "top = [(Xva_attr.columns[j], float(phi[j]), float(Xva_attr.iloc[i_local, j])) for j in order[:20]]\n",
        "\n",
        "import pandas as pd\n",
        "top_df = pd.DataFrame(top, columns=[\"feature\",\"shap_value\",\"value\"])\n",
        "print(\"\\nTop local contributors (ATTRS):\")\n",
        "print(top_df.to_string(index=False))\n"
      ],
      "metadata": {
        "id": "dil624gimUCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare this graph’s feature values to train percentiles\n",
        "top_feats = [\n",
        "    \"node_power_w_mean__max\",\"edge_prompt_tokens__mean\",\"node_cs_mean__min\",\n",
        "    \"edge_timing_ms__max\",\"node_cs_mean__mean\",\"node_us_mean__max\",\n",
        "    \"edge_vis_width__max\",\"edge_prompt_tokens__std\",\"node_last_ts__std\",\n",
        "    \"edge_completion_tokens__sum\",\"node_us_mean__std\",\n",
        "    \"node_gpu_util_mean__mean\",\"node_gpu_util_mean__max\",\n",
        "    \"edge_vis_width__std\",\"node_us_mean__sum\"\n",
        "]\n",
        "\n",
        "summ = []\n",
        "for f in top_feats:\n",
        "    v = float(Xva_attr.iloc[i_local][f])\n",
        "    q50 = Xtr_attr[f].median()\n",
        "    q90 = Xtr_attr[f].quantile(0.90)\n",
        "    q95 = Xtr_attr[f].quantile(0.95)\n",
        "    z = (v - Xtr_attr[f].mean()) / (Xtr_attr[f].std() + 1e-9)\n",
        "    summ.append((f, v, q50, q90, q95, z))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(summ, columns=[\"feature\",\"value\",\"train_p50\",\"train_p90\",\"train_p95\",\"zscore\"]).sort_values(\"zscore\", ascending=False)\n"
      ],
      "metadata": {
        "id": "uTfHeXVAmUFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exactly which node and edge"
      ],
      "metadata": {
        "id": "ltz3lYlcmUH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Attack_bundle/_exports/0679622fdd9bdd41acd5bd43417d4b47.gpickle\"\n",
        "\n",
        "def load_gpickle(path):\n",
        "    # 1) Preferred (if exposed)\n",
        "    if hasattr(nx, \"read_gpickle\"):\n",
        "        return nx.read_gpickle(path)\n",
        "    # 2) Module-level import\n",
        "    try:\n",
        "        from networkx.readwrite import gpickle as nx_gpickle\n",
        "        return nx_gpickle.read_gpickle(path)\n",
        "    except Exception:\n",
        "        pass\n",
        "    # 3) Raw pickle fallback\n",
        "    import pickle\n",
        "    with open(path, \"rb\") as f:\n",
        "        obj = pickle.load(f)\n",
        "    # In case the file stored only the graph object directly\n",
        "    return obj\n",
        "\n",
        "G = load_gpickle(file_path)\n",
        "print(type(G), getattr(G, \"number_of_nodes\", lambda: \"n/a\")(), getattr(G, \"number_of_edges\", lambda: \"n/a\")())\n"
      ],
      "metadata": {
        "id": "gWp1AjifqcLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import networkx as nx, numpy as np, pandas as pd\n",
        "\n",
        "# Use the graph you've already loaded\n",
        "# G = load_gpickle(file_path)\n",
        "# --- 0) Rebuild top_local_df for the picked sample i_local ---\n",
        "# requires: explainer, Xva_attr, i_local\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Compute SHAP for just the chosen row; skip strict additivity check\n",
        "sv = explainer(Xva_attr.iloc[[i_local]], check_additivity=False)\n",
        "\n",
        "# Grab the vector of SHAP values for the positive class\n",
        "if isinstance(sv, list):               # multiclass\n",
        "    # assume class index 1 = \"attack\" (adjust if your label order differs)\n",
        "    phi = np.array(sv[1].values)[0]\n",
        "else:                                   # binary/proba or margin\n",
        "    phi = np.array(sv.values)[0]\n",
        "\n",
        "feat_names = Xva_attr.columns\n",
        "feat_vals  = Xva_attr.iloc[i_local].values\n",
        "\n",
        "top_local_df = pd.DataFrame({\n",
        "    \"feature\": feat_names,\n",
        "    \"shap_value\": phi,\n",
        "    \"value\": feat_vals\n",
        "})\n",
        "# Sort by absolute impact (largest first)\n",
        "top_local_df = top_local_df.reindex(\n",
        "    top_local_df[\"shap_value\"].abs().sort_values(ascending=False).index\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(top_local_df.head(15))\n",
        "\n",
        "# ---- 1) Build per-node / per-edge frames (adjust keys to your schema) ----\n",
        "def _to_float(x):\n",
        "    try:\n",
        "        return float(x)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "node_rows = []\n",
        "for n, d in G.nodes(data=True):\n",
        "    node_rows.append({\n",
        "        \"node_id\": n,\n",
        "        \"power_w_mean\": _to_float(d.get(\"power_w_mean\")),\n",
        "        \"cs_mean\": _to_float(d.get(\"cs_mean\")),\n",
        "        \"us_mean\": _to_float(d.get(\"us_mean\")),\n",
        "        \"gpu_util_mean\": _to_float(d.get(\"gpu_util_mean\")),\n",
        "        \"last_ts\": _to_float(d.get(\"last_ts\")),\n",
        "    })\n",
        "node_df = pd.DataFrame(node_rows)\n",
        "\n",
        "edge_rows = []\n",
        "for u, v, d in G.edges(data=True):\n",
        "    edge_rows.append({\n",
        "        \"u\": u, \"v\": v,\n",
        "        \"prompt_tokens\": _to_float(d.get(\"prompt_tokens\")),\n",
        "        \"completion_tokens\": _to_float(d.get(\"completion_tokens\")),\n",
        "        \"timing_ms\": _to_float(d.get(\"timing_ms\")),\n",
        "        \"vis_width\": _to_float(d.get(\"vis_width\")),\n",
        "        \"us_mean\": _to_float(d.get(\"us_mean\")),  # only if you used this\n",
        "    })\n",
        "edge_df = pd.DataFrame(edge_rows)\n",
        "\n",
        "# Empty-graph guard\n",
        "if node_df.empty: node_df = pd.DataFrame(columns=[\"node_id\",\"power_w_mean\",\"cs_mean\",\"us_mean\",\"gpu_util_mean\",\"last_ts\"])\n",
        "if edge_df.empty: edge_df = pd.DataFrame(columns=[\"u\",\"v\",\"prompt_tokens\",\"completion_tokens\",\"timing_ms\",\"vis_width\",\"us_mean\"])\n",
        "\n",
        "# ---- 2) Local SHAP table from your earlier step ----\n",
        "local = top_local_df.copy()  # columns: [\"feature\",\"shap_value\",\"value\"]\n",
        "\n",
        "# ---- 3) Helpers for safe argmax/argmin and weights ----\n",
        "def _safe_idxmax(s: pd.Series):\n",
        "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "    if s_num.notna().any():\n",
        "        return s_num.idxmax()\n",
        "    return None\n",
        "\n",
        "def _safe_idxmin(s: pd.Series):\n",
        "    s_num = pd.to_numeric(s, errors=\"coerce\")\n",
        "    if s_num.notna().any():\n",
        "        return s_num.idxmin()\n",
        "    return None\n",
        "\n",
        "def _safe_norm(s: pd.Series):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\").fillna(0.0)\n",
        "    total = float(s.sum())\n",
        "    if total == 0.0:\n",
        "        # uniform weights across present entries (avoids div by zero)\n",
        "        return pd.Series(1.0 / len(s), index=s.index) if len(s) else s\n",
        "    return s / total\n",
        "\n",
        "def _safe_dev_norm(s: pd.Series):\n",
        "    s = pd.to_numeric(s, errors=\"coerce\")\n",
        "    mu = s.mean()\n",
        "    dev = (s - mu).abs()\n",
        "    dev = dev.fillna(0.0)\n",
        "    total = float(dev.sum())\n",
        "    if total == 0.0:\n",
        "        return pd.Series(1.0 / len(dev), index=dev.index) if len(dev) else dev\n",
        "    return dev / total\n",
        "\n",
        "# ---- 4) Map each global feature name to per-node/edge weights ----\n",
        "def weights_for_feature(feat: str):\n",
        "    eps = 1e-12\n",
        "    if feat == \"node_power_w_mean__max\":\n",
        "        idx = _safe_idxmax(node_df[\"power_w_mean\"])\n",
        "        if idx is None: return (\"node\", None)\n",
        "        w = pd.Series(0.0, index=node_df.index); w.loc[idx] = 1.0\n",
        "        return (\"node\", w)\n",
        "\n",
        "    if feat == \"node_cs_mean__min\":\n",
        "        idx = _safe_idxmin(node_df[\"cs_mean\"])\n",
        "        if idx is None: return (\"node\", None)\n",
        "        w = pd.Series(0.0, index=node_df.index); w.loc[idx] = 1.0\n",
        "        return (\"node\", w)\n",
        "\n",
        "    if feat == \"node_us_mean__max\":\n",
        "        idx = _safe_idxmax(node_df[\"us_mean\"])\n",
        "        if idx is None: return (\"node\", None)\n",
        "        w = pd.Series(0.0, index=node_df.index); w.loc[idx] = 1.0\n",
        "        return (\"node\", w)\n",
        "\n",
        "    if feat == \"node_us_mean__std\":\n",
        "        return (\"node\", _safe_dev_norm(node_df[\"us_mean\"]))\n",
        "\n",
        "    if feat == \"node_gpu_util_mean__mean\":\n",
        "        return (\"node\", _safe_norm(node_df[\"gpu_util_mean\"]))\n",
        "\n",
        "    if feat == \"edge_timing_ms__max\":\n",
        "        idx = _safe_idxmax(edge_df[\"timing_ms\"])\n",
        "        if idx is None: return (\"edge\", None)\n",
        "        w = pd.Series(0.0, index=edge_df.index); w.loc[idx] = 1.0\n",
        "        return (\"edge\", w)\n",
        "\n",
        "    if feat == \"edge_completion_tokens__sum\":\n",
        "        return (\"edge\", _safe_norm(edge_df[\"completion_tokens\"]))\n",
        "\n",
        "    if feat == \"edge_prompt_tokens__mean\":\n",
        "        return (\"edge\", _safe_norm(edge_df[\"prompt_tokens\"]))\n",
        "\n",
        "    if feat == \"edge_prompt_tokens__std\":\n",
        "        return (\"edge\", _safe_dev_norm(edge_df[\"prompt_tokens\"]))\n",
        "\n",
        "    if feat == \"edge_vis_width__max\":\n",
        "        idx = _safe_idxmax(edge_df[\"vis_width\"])\n",
        "        if idx is None: return (\"edge\", None)\n",
        "        w = pd.Series(0.0, index=edge_df.index); w.loc[idx] = 1.0\n",
        "        return (\"edge\", w)\n",
        "\n",
        "    if feat == \"edge_vis_width__std\":\n",
        "        return (\"edge\", _safe_dev_norm(edge_df[\"vis_width\"]))\n",
        "\n",
        "    # not mapped → ignore\n",
        "    return (None, None)\n",
        "\n",
        "# ---- 5) Aggregate local responsibility across the important features ----\n",
        "node_score = pd.Series(0.0, index=node_df.index)\n",
        "edge_score = pd.Series(0.0, index=edge_df.index)\n",
        "\n",
        "for _, r in local.iterrows():\n",
        "    feat = str(r[\"feature\"])\n",
        "    phi  = float(r[\"shap_value\"])  # signed local contribution\n",
        "    kind, w = weights_for_feature(feat)\n",
        "    if w is None:\n",
        "        continue\n",
        "    if kind == \"node\":\n",
        "        node_score = node_score.add(phi * w, fill_value=0.0)\n",
        "    elif kind == \"edge\":\n",
        "        edge_score = edge_score.add(phi * w, fill_value=0.0)\n",
        "\n",
        "# ---- 6) Report top culprits (positive pushes toward \"attack\"; negative protects) ----\n",
        "top_nodes = node_df.assign(resp=node_score).sort_values(\"resp\", ascending=False).head(10)\n",
        "top_edges = edge_df.assign(resp=edge_score).sort_values(\"resp\", ascending=False).head(10)\n",
        "\n",
        "print(\"Top nodes by local responsibility:\\n\", top_nodes[[\"node_id\",\"resp\",\"power_w_mean\",\"cs_mean\",\"us_mean\",\"gpu_util_mean\"]])\n",
        "print(\"\\nTop edges by local responsibility:\\n\", top_edges[[\"u\",\"v\",\"resp\",\"prompt_tokens\",\"completion_tokens\",\"timing_ms\",\"vis_width\"]])\n"
      ],
      "metadata": {
        "id": "Qi4TVLPjp-xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make it better"
      ],
      "metadata": {
        "id": "iXaz8K4Xp-0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_node = (top_nodes.assign(pos=lambda d: d[\"resp\"].clip(lower=0)))\n",
        "pos_edge = (top_edges.assign(pos=lambda d: d[\"resp\"].clip(lower=0)))\n",
        "node_share = pos_node[\"pos\"] / max(pos_node[\"pos\"].sum(), 1e-9)\n",
        "edge_share = pos_edge[\"pos\"] / max(pos_edge[\"pos\"].sum(), 1e-9)\n",
        "print(\"Node share (top 5):\")\n",
        "print(pd.concat([top_nodes[\"node_id\"], node_share.rename(\"share\")], axis=1).head(5))\n",
        "print(\"\\nEdge share (top 5):\")\n",
        "print(pd.concat([top_edges[[\"u\",\"v\"]], edge_share.rename(\"share\")], axis=1).head(5))\n"
      ],
      "metadata": {
        "id": "7MMI2fXs5c50"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_node = (top_nodes.assign(pos=lambda d: d[\"resp\"].clip(lower=0)))\n",
        "pos_edge = (top_edges.assign(pos=lambda d: d[\"resp\"].clip(lower=0)))\n",
        "node_share = pos_node[\"pos\"] / max(pos_node[\"pos\"].sum(), 1e-9)\n",
        "edge_share = pos_edge[\"pos\"] / max(pos_edge[\"pos\"].sum(), 1e-9)\n",
        "print(\"Node share (top 5):\")\n",
        "print(pd.concat([top_nodes[\"node_id\"], node_share.rename(\"share\")], axis=1).head(5))\n",
        "print(\"\\nEdge share (top 5):\")\n",
        "print(pd.concat([top_edges[[\"u\",\"v\"]], edge_share.rename(\"share\")], axis=1).head(5))\n"
      ],
      "metadata": {
        "id": "PfOChqdG5c8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_node = top_nodes.iloc[0][\"node_id\"]\n",
        "nbrs = list(G.neighbors(top_node))\n",
        "E = [(top_node, n) for n in nbrs if G.has_edge(top_node, n)]\n",
        "print(\"Top node:\", top_node)\n",
        "print(\"Neighbors:\", nbrs)\n",
        "edge_tbl = edge_df.merge(\n",
        "    pd.DataFrame(E, columns=[\"u\",\"v\"]),\n",
        "    how=\"inner\", on=[\"u\",\"v\"]\n",
        ").merge(\n",
        "    top_edges[[\"u\",\"v\",\"resp\"]],\n",
        "    how=\"left\", on=[\"u\",\"v\"]\n",
        ").sort_values(\"resp\", ascending=False)\n",
        "print(edge_tbl[[\"u\",\"v\",\"prompt_tokens\",\"completion_tokens\",\"timing_ms\",\"vis_width\",\"resp\"]].head(10))\n"
      ],
      "metadata": {
        "id": "C-jJnbeQ5c-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############ Just something for my report   ##############################################################################################"
      ],
      "metadata": {
        "id": "SDqw5nfzp-3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After your leak-guard pipeline and alignment\n",
        "feat_STRUCT = list(struct_num_cols) if 'struct_num_cols' in globals() else [f\"struct_{i}\" for i in range(X_struct_blk.shape[1])]\n",
        "feat_ATTRS  = list(keep_num_cols)   # from your leak-guard code\n",
        "# Combined = STRUCT + ATTRS\n",
        "feat_COMBO  = feat_STRUCT + feat_ATTRS\n",
        "\n",
        "print(\"STRUCTURE feature count:\", len(feat_STRUCT))\n",
        "print(\"ATTRS feature count:\", len(feat_ATTRS))\n",
        "print(\"COMBINED feature count:\", len(feat_COMBO))\n"
      ],
      "metadata": {
        "id": "ekApoP1P9OlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Recompute the usable list at the last moment\n",
        "keep_num_cols = list(c for c in keep_num_cols if c in aligned_df_num.columns)\n",
        "\n",
        "# (Optional) visibility for debugging\n",
        "missing = sorted(set(keep_num_cols_original) - set(aligned_df_num.columns)) if 'keep_num_cols_original' in globals() else []\n",
        "if missing:\n",
        "    print(f\"[WARN] {len(missing)} requested cols missing from aligned_df_num; first 15:\", missing[:15])\n",
        "\n",
        "# Now build safely\n",
        "X_ATTRS_full = (aligned_df_num[['file'] + keep_num_cols]\n",
        "                .set_index('file'))\n"
      ],
      "metadata": {
        "id": "2Fz8tjnQ_za8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "usable = list(aligned_df_num.columns.intersection(keep_num_cols))\n",
        "X_ATTRS_full = aligned_df_num[['file'] + usable].set_index('file')\n"
      ],
      "metadata": {
        "id": "FG1V89Cj_0wl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "want = set(keep_num_cols)\n",
        "have = set(aligned_df_num.columns)\n",
        "print(\"want:\", len(want), \"have:\", len(have), \"usable:\", len(want & have))\n",
        "missing = sorted(want - have)\n",
        "print(\"missing (first 25):\", missing[:25])\n"
      ],
      "metadata": {
        "id": "GI9PwDie_3Gb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMuIY53K_3PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import issparse\n",
        "\n",
        "# --- 1) Figure out the file column in merged\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "\n",
        "# --- 2) Make sure 'file' exists in aligned_df_num and is unique\n",
        "if \"file\" not in aligned_df_num.columns:\n",
        "    raise RuntimeError(\"aligned_df_num must have a 'file' column.\")\n",
        "if aligned_df_num[\"file\"].duplicated().any():\n",
        "    aligned_df_num = aligned_df_num.drop_duplicates(\"file\")\n",
        "\n",
        "# --- 3) Intersect feature lists with actual columns you have\n",
        "keep_num_cols = list(dict.fromkeys(keep_num_cols))  # drop dupes, preserve order\n",
        "keep_num_cols_present = [c for c in keep_num_cols if c in aligned_df_num.columns]\n",
        "\n",
        "missing = sorted(set(keep_num_cols) - set(keep_num_cols_present))\n",
        "if missing:\n",
        "    print(f\"[WARN] {len(missing)} keep cols are missing from aligned_df_num; they’ll be skipped.\")\n",
        "    print(\"        (first 20):\", missing[:20])\n",
        "\n",
        "# --- 4) Build ATTRS as a DataFrame indexed by file, then reindex to merged order\n",
        "X_ATTRS_full = (aligned_df_num[[\"file\"] + keep_num_cols_present]\n",
        "                .set_index(\"file\")\n",
        "                .reindex(merged[file_col_m]))\n",
        "assert len(X_ATTRS_full) == len(merged), \"Row count mismatch after reindex.\"\n",
        "\n",
        "# --- 5) STRUCT block -> DataFrame with names\n",
        "if 'struct_num_cols' not in globals() or len(struct_num_cols) != X_struct_blk.shape[1]:\n",
        "    struct_num_cols = [f\"struct_{i}\" for i in range(X_struct_blk.shape[1])]\n",
        "X_STRUCT_arr = X_struct_blk.toarray() if issparse(X_struct_blk) else np.asarray(X_struct_blk)\n",
        "X_STRUCT_full = pd.DataFrame(X_STRUCT_arr, columns=struct_num_cols, index=merged[file_col_m])\n",
        "\n",
        "# --- 6) Combined\n",
        "X_COMBO_full = pd.concat([X_STRUCT_full, X_ATTRS_full], axis=1)\n",
        "\n",
        "# --- 7) Feature name lists\n",
        "feat_ATTRS  = list(X_ATTRS_full.columns)\n",
        "feat_STRUCT = list(X_STRUCT_full.columns)\n",
        "feat_COMBO  = list(X_COMBO_full.columns)\n",
        "\n",
        "print(\"STRUCTURE feature count:\", len(feat_STRUCT))\n",
        "print(\"ATTRS feature count:\", len(feat_ATTRS))\n",
        "print(\"COMBINED feature count:\", len(feat_COMBO))\n"
      ],
      "metadata": {
        "id": "zaAGq6Ih-74v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prereqs (install if needed)\n",
        "# !pip install shap==0.44.1\n",
        "\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import csr_matrix, issparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, accuracy_score\n",
        "from sklearn.inspection import permutation_importance\n",
        "import matplotlib.pyplot as plt\n",
        "import shap\n",
        "import pickle\n",
        "import networkx as nx\n",
        "\n",
        "# ---- You already have these in your notebook/session ----\n",
        "# y                 -> np.array of shape (n_samples,)\n",
        "# merged            -> DataFrame with a \"file\" (or path) column\n",
        "# aligned_df_num    -> DataFrame that aligns to merged rows, with numeric features\n",
        "# keep_num_cols     -> list of ATTRS feature names you kept after leak purge\n",
        "# X_struct_blk      -> sparse (n_samples, n_struct_feats)\n",
        "# struct_num_cols   -> list of STRUCT feature names (or build a list of that length)\n",
        "\n",
        "# Safety: build STRUCT feature names if missing\n",
        "if 'struct_num_cols' not in globals() or len(struct_num_cols) != X_struct_blk.shape[1]:\n",
        "    struct_num_cols = [f\"struct_{i}\" for i in range(X_struct_blk.shape[1])]\n",
        "\n",
        "# Build dense DataFrames for modeling\n",
        "X_ATTRS_full  = aligned_df_num[['file'] + keep_num_cols].set_index('file')\n",
        "X_STRUCT_full = pd.DataFrame(\n",
        "    X_struct_blk.toarray() if issparse(X_struct_blk) else X_struct_blk,\n",
        "    index=merged['file' if 'file' in merged.columns else next(c for c in merged.columns if c.lower() in ('file','path'))],\n",
        "    columns=struct_num_cols\n",
        ")\n",
        "# Make sure same row order (by merged)\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "X_ATTRS_full = X_ATTRS_full.loc[merged[file_col_m]]\n",
        "X_STRUCT_full = X_STRUCT_full.loc[merged[file_col_m]]\n",
        "\n",
        "# Combined\n",
        "X_COMBO_full = pd.concat([X_STRUCT_full, X_ATTRS_full], axis=1)\n",
        "feat_STRUCT = list(X_STRUCT_full.columns)\n",
        "feat_ATTRS  = list(X_ATTRS_full.columns)\n",
        "feat_COMBO  = list(X_COMBO_full.columns)\n",
        "\n",
        "print(\"STRUCTURE feature count:\", len(feat_STRUCT))\n",
        "print(\"ATTRS feature count:\", len(feat_ATTRS))\n",
        "print(\"COMBINED feature count:\", len(feat_COMBO))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "dCvHyV1Q-Hij",
        "outputId": "4a6acca7-3ed4-4eae-91f5-809d86ba4239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'X_struct_blk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1794458990.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Safety: build STRUCT feature names if missing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'struct_num_cols'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstruct_num_cols\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mX_struct_blk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mstruct_num_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34mf\"struct_{i}\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_struct_blk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Build dense DataFrames for modeling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_struct_blk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === 0) Build blocks with names (robust to sparse STRUCT) ===\n",
        "import numpy as np, pandas as pd\n",
        "from scipy.sparse import issparse\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.inspection import permutation_importance\n",
        "import shap, matplotlib.pyplot as plt\n",
        "\n",
        "# safety: STRUCT feature names if missing\n",
        "if 'struct_num_cols' not in globals() or len(struct_num_cols) != (X_struct_blk.shape[1] if hasattr(X_struct_blk, \"shape\") else X_struct_blk.shape[1]):\n",
        "    struct_num_cols = [f\"struct_{i}\" for i in range(X_struct_blk.shape[1])]\n",
        "\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "\n",
        "# ATTRS (dense DataFrame)\n",
        "X_ATTRS_full = aligned_df_num[['file'] + list(keep_num_cols)].set_index('file')\n",
        "X_ATTRS_full = X_ATTRS_full.loc[merged[file_col_m]]\n",
        "\n",
        "# STRUCT (ensure DataFrame with names)\n",
        "X_STRUCT_arr = X_struct_blk.toarray() if issparse(X_struct_blk) else np.asarray(X_struct_blk)\n",
        "X_STRUCT_full = pd.DataFrame(X_STRUCT_arr, index=merged[file_col_m], columns=struct_num_cols)\n",
        "\n",
        "# COMBINED (concat columns)\n",
        "X_COMBO_full = pd.concat([X_STRUCT_full, X_ATTRS_full], axis=1)\n",
        "\n",
        "feat_STRUCT = list(X_STRUCT_full.columns)\n",
        "feat_ATTRS  = list(X_ATTRS_full.columns)\n",
        "feat_COMBO  = list(X_COMBO_full.columns)\n",
        "\n",
        "print(\"STRUCTURE feature count:\", len(feat_STRUCT))\n",
        "print(\"ATTRS feature count:\", len(feat_ATTRS))\n",
        "print(\"COMBINED feature count:\", len(feat_COMBO))\n",
        "\n",
        "# === 1) One stratified split ===\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "train_idx, val_idx = next(skf.split(np.zeros_like(y), y))\n",
        "\n",
        "def split_block(df):\n",
        "    Xtr = df.iloc[train_idx].copy()\n",
        "    Xva = df.iloc[val_idx].copy()\n",
        "    return Xtr, Xva\n",
        "\n",
        "Xtr_S, Xva_S = split_block(X_STRUCT_full)\n",
        "Xtr_A, Xva_A = split_block(X_ATTRS_full)\n",
        "Xtr_C, Xva_C = split_block(X_COMBO_full)\n",
        "ytr, yva     = y[train_idx], y[val_idx]\n",
        "\n",
        "# === 2) Fit models (S / A / C) ===\n",
        "def new_hgb():\n",
        "    # sane defaults; tweak if you want\n",
        "    return HistGradientBoostingClassifier(\n",
        "        max_depth=None, learning_rate=0.1, max_iter=300,\n",
        "        l2_regularization=0.0, random_state=42\n",
        "    )\n",
        "\n",
        "hgb_S = new_hgb().fit(Xtr_S, ytr)\n",
        "hgb_A = new_hgb().fit(Xtr_A, ytr)\n",
        "hgb_C = new_hgb().fit(Xtr_C, ytr)\n",
        "\n",
        "# === 3) Permutation importance helper ===\n",
        "from scipy.sparse import issparse\n",
        "def perm_imp_table(model, X, y, feature_names, n_repeats=10, random_state=42):\n",
        "    Xv = X.toarray() if issparse(X) else (X.values if isinstance(X, pd.DataFrame) else np.asarray(X))\n",
        "    pi = permutation_importance(model, Xv, y, n_repeats=n_repeats,\n",
        "                                random_state=random_state, scoring=\"roc_auc\")\n",
        "    df = (pd.DataFrame({\"feature\": list(feature_names),\n",
        "                        \"mean\": pi.importances_mean,\n",
        "                        \"std\":  pi.importances_std})\n",
        "            .sort_values(\"mean\", ascending=False)\n",
        "            .reset_index(drop=True))\n",
        "    return df\n",
        "\n",
        "# === 4) SHAP global helper ===\n",
        "def shap_global_table(model, X_df, top=15):\n",
        "    if not isinstance(X_df, pd.DataFrame):\n",
        "        X_df = pd.DataFrame(X_df)\n",
        "    explainer = shap.TreeExplainer(model, feature_perturbation=\"tree_path_dependent\")\n",
        "    phi = explainer.shap_values(X_df, check_additivity=False)\n",
        "    if isinstance(phi, list) and len(phi) == 2:\n",
        "        phi = phi[1]\n",
        "    shap_mean = np.abs(phi).mean(axis=0)\n",
        "    out = (pd.DataFrame({\"feature\": X_df.columns, \"mean\": shap_mean})\n",
        "           .sort_values(\"mean\", ascending=False)\n",
        "           .reset_index(drop=True))\n",
        "    return out.head(top)\n",
        "\n",
        "# === 5) Compute global explainability (Permutation + SHAP) ===\n",
        "perm_S = perm_imp_table(hgb_S, Xva_S, yva, feat_STRUCT)\n",
        "perm_A = perm_imp_table(hgb_A, Xva_A, yva, feat_ATTRS)\n",
        "perm_C = perm_imp_table(hgb_C, Xva_C, yva, feat_COMBO)\n",
        "\n",
        "# For SHAP, use DataFrames with names\n",
        "Xva_S_df = Xva_S if isinstance(Xva_S, pd.DataFrame) else pd.DataFrame(Xva_S, columns=feat_STRUCT)\n",
        "Xva_A_df = Xva_A if isinstance(Xva_A, pd.DataFrame) else pd.DataFrame(Xva_A, columns=feat_ATTRS)\n",
        "Xva_C_df = Xva_C if isinstance(Xva_C, pd.DataFrame) else pd.DataFrame(Xva_C, columns=feat_COMBO)\n",
        "\n",
        "shap_S = shap_global_table(hgb_S, Xva_S_df, top=15)\n",
        "shap_A = shap_global_table(hgb_A, Xva_A_df, top=15)\n",
        "shap_C = shap_global_table(hgb_C, Xva_C_df, top=15)\n",
        "\n",
        "print(\"\\n=== STRUCTURE – Permutation top 15 ===\");  display(perm_S.head(15))\n",
        "print(\"\\n=== ATTRS – Permutation top 15 ===\");      display(perm_A.head(15))\n",
        "print(\"\\n=== COMBINED – Permutation top 15 ===\");   display(perm_C.head(15))\n",
        "\n",
        "print(\"\\n=== STRUCTURE – SHAP |mean| top 15 ===\");  display(shap_S)\n",
        "print(\"\\n=== ATTRS – SHAP |mean| top 15 ===\");      display(shap_A)\n",
        "print(\"\\n=== COMBINED – SHAP |mean| top 15 ===\");   display(shap_C)\n",
        "\n",
        "# === 6) Save quick PNG bars for your report ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def bar_top(df, val_col, title, n=15, fname=None):\n",
        "    d = df.head(n).iloc[::-1]\n",
        "    plt.figure(figsize=(7,5))\n",
        "    plt.barh(d[\"feature\"], d[val_col])\n",
        "    plt.title(title); plt.tight_layout()\n",
        "    if fname: plt.savefig(fname, dpi=160)\n",
        "    plt.show()\n",
        "\n",
        "bar_top(perm_S, \"mean\", \"Permutation – STRUCTURE (top15)\", fname=\"perm_struct_top15.png\")\n",
        "bar_top(perm_A, \"mean\", \"Permutation – ATTRS (top15)\",    fname=\"perm_attrs_top15.png\")\n",
        "bar_top(perm_C, \"mean\", \"Permutation – COMBINED (top15)\", fname=\"perm_comb_top15.png\")\n",
        "\n",
        "bar_top(shap_S, \"mean\", \"SHAP |mean| – STRUCTURE (top15)\", fname=\"shap_struct_top15.png\")\n",
        "bar_top(shap_A, \"mean\", \"SHAP |mean| – ATTRS (top15)\",     fname=\"shap_attrs_top15.png\")\n",
        "bar_top(shap_C, \"mean\", \"SHAP |mean| – COMBINED (top15)\",  fname=\"shap_comb_top15.png\")\n"
      ],
      "metadata": {
        "id": "U5NxeTsvAERG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4) Local explanation for your specific graph"
      ],
      "metadata": {
        "id": "y5Rzv0nEAEUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =======================\n",
        "# 4) Local explanation (ATTRS) for your specific graph\n",
        "# =======================\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import shap, networkx as nx, pickle\n",
        "\n",
        "target_file = \"/content/drive/MyDrive/Attack_bundle/_exports/0679622fdd9bdd41acd5bd43417d4b47.gpickle\"\n",
        "\n",
        "# Ensure the target file is in the current validation split\n",
        "files_val = merged.iloc[val_idx][file_col_m].tolist()\n",
        "try:\n",
        "    i_local_rel = files_val.index(target_file)   # index within *validation* set\n",
        "except ValueError:\n",
        "    raise RuntimeError(\n",
        "        \"Target file is not in the current validation fold. \"\n",
        "        \"Either change target_file, or re-run the split until it falls in val.\"\n",
        "    )\n",
        "\n",
        "# Local SHAP on ATTRS\n",
        "x_row = Xva_A.iloc[[i_local_rel]]\n",
        "\n",
        "# Create (once) a SHAP explainer for ATTRS model\n",
        "shap_expl_A = shap.TreeExplainer(hgb_A, feature_perturbation=\"tree_path_dependent\")\n",
        "\n",
        "# Handle the two common binary outputs:\n",
        "if isinstance(phi, list):                         # e.g. [phi_class0, phi_class1]\n",
        "    phi_row = np.asarray(phi[1]).reshape(-1)      # explain the positive class\n",
        "    ev = shap_expl_A.expected_value\n",
        "    base = float(np.array(ev)[1]) if isinstance(ev, (list, np.ndarray)) else float(ev)\n",
        "else:                                             # e.g. (1, n_features)\n",
        "    phi_row = np.asarray(phi).reshape(-1)\n",
        "    ev = shap_expl_A.expected_value\n",
        "    base = float(ev if np.isscalar(ev) else np.array(ev).reshape(-1)[0])\n",
        "\n",
        "# Construct an Explanation so we don’t fight legacy API\n",
        "ex = shap.Explanation(\n",
        "    values=phi_row,\n",
        "    base_values=base,\n",
        "    data=x_row.values[0],\n",
        "    feature_names=list(Xva_A.columns)\n",
        ")\n",
        "\n",
        "# Plot + save\n",
        "shap.plots.waterfall(ex, max_display=20, show=False)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"local_attr_shap_waterfall.png\", dpi=160)\n",
        "plt.close()\n",
        "# phi = shap_expl_A.shap_values(x_row, check_additivity=False)\n",
        "# # binary classifier: phi could be (n,F) or [phi0, phi1]\n",
        "# if isinstance(phi, list) and len(phi) == 2:\n",
        "#     phi = phi[1]\n",
        "# base = (shap_expl_A.expected_value[1]\n",
        "#         if (hasattr(shap_expl_A, \"expected_value\") and isinstance(shap_expl_A.expected_value, (list, np.ndarray)) and len(shap_expl_A.expected_value) > 1)\n",
        "#         else (shap_expl_A.expected_value if hasattr(shap_expl_A, \"expected_value\") else 0.0))\n",
        "\n",
        "# top_local_df = (pd.DataFrame({\n",
        "#     \"feature\": Xva_A.columns,\n",
        "#     \"shap_value\": np.asarray(phi).flatten(),\n",
        "#     \"value\": x_row.values.flatten()\n",
        "# }).sort_values(\"shap_value\", ascending=False))\n",
        "\n",
        "# print(\"Top local contributors (ATTRS):\")\n",
        "# display(top_local_df.head(20))\n",
        "\n",
        "# # Waterfall figure\n",
        "# shap.plots._waterfall.waterfall_legacy(base, np.asarray(phi).flatten(),\n",
        "#                                        feature_names=Xva_A.columns, max_display=20)\n",
        "# plt.tight_layout()\n",
        "# plt.savefig(\"local_attr_shap_waterfall.png\", dpi=160)\n",
        "\n",
        "# P(attack) checks\n",
        "p_attr   = float(hgb_A.predict_proba(x_row)[0, 1])\n",
        "p_struct = float(hgb_S.predict_proba(Xva_S.iloc[[i_local_rel]])[0, 1])\n",
        "print(f\"[ATTR] P(attack): {p_attr:.3f} | [STRUCT] P(attack): {p_struct:.3f}\")\n",
        "\n",
        "# =======================\n",
        "# 5) Node/Edge responsibility for this graph\n",
        "# =======================\n",
        "\n",
        "def load_gpickle(path):\n",
        "    # Try networkx gpickle, then raw pickle\n",
        "    if hasattr(nx, \"read_gpickle\"):\n",
        "        try:\n",
        "            return nx.read_gpickle(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "G = load_gpickle(target_file)\n",
        "\n",
        "# Build node/edge frames (edit keys if your graph schema differs)\n",
        "node_df = pd.DataFrame([\n",
        "    {\"node\": n,\n",
        "     \"power_w_mean\":   G.nodes[n].get(\"power_w_mean\"),\n",
        "     \"cs_mean\":        G.nodes[n].get(\"cs_mean\"),\n",
        "     \"us_mean\":        G.nodes[n].get(\"us_mean\"),\n",
        "     \"gpu_util_mean\":  G.nodes[n].get(\"gpu_util_mean\")}\n",
        "    for n in G.nodes()\n",
        "])\n",
        "edge_df = pd.DataFrame([\n",
        "    {\"u\": u, \"v\": v,\n",
        "     \"prompt_tokens\":     G[u][v].get(\"prompt_tokens\"),\n",
        "     \"completion_tokens\": G[u][v].get(\"completion_tokens\"),\n",
        "     \"timing_ms\":         G[u][v].get(\"timing_ms\"),\n",
        "     \"vis_width\":         G[u][v].get(\"vis_width\")}\n",
        "    for u, v in G.edges()\n",
        "])\n",
        "\n",
        "def safe_argmax(s):\n",
        "    idx = s.fillna(-np.inf).astype(float).idxmax()\n",
        "    w = pd.Series(0.0, index=s.index); w.loc[idx] = 1.0\n",
        "    return w\n",
        "\n",
        "def safe_argmin(s):\n",
        "    idx = s.fillna(+np.inf).astype(float).idxmin()\n",
        "    w = pd.Series(0.0, index=s.index); w.loc[idx] = 1.0\n",
        "    return w\n",
        "\n",
        "def weights_for_feature(feat):\n",
        "    eps = 1e-12\n",
        "    # NODE features\n",
        "    if feat == \"node_power_w_mean__max\":   return (\"node\", safe_argmax(node_df[\"power_w_mean\"]))\n",
        "    if feat == \"node_cs_mean__min\":        return (\"node\", safe_argmin(node_df[\"cs_mean\"]))\n",
        "    if feat == \"node_cs_mean__mean\":\n",
        "        x = node_df[\"cs_mean\"].fillna(0).astype(float); return (\"node\", x / (x.sum() + eps))\n",
        "    if feat == \"node_us_mean__max\":        return (\"node\", safe_argmax(node_df[\"us_mean\"]))\n",
        "    if feat == \"node_us_mean__std\":\n",
        "        x = node_df[\"us_mean\"].astype(float)\n",
        "        d = (x - x.mean()).abs(); return (\"node\", d / (d.sum() + eps))\n",
        "    if feat == \"node_gpu_util_mean__mean\":\n",
        "        x = node_df[\"gpu_util_mean\"].fillna(0).astype(float); return (\"node\", x / (x.sum() + eps))\n",
        "    # EDGE features\n",
        "    if feat == \"edge_prompt_tokens__mean\":\n",
        "        x = edge_df[\"prompt_tokens\"].fillna(0).astype(float); return (\"edge\", x / (x.sum() + eps))\n",
        "    if feat == \"edge_prompt_tokens__std\":\n",
        "        x = edge_df[\"prompt_tokens\"].astype(float); d = (x - x.mean()).abs(); return (\"edge\", d / (d.sum() + eps))\n",
        "    if feat == \"edge_timing_ms__max\":      return (\"edge\", safe_argmax(edge_df[\"timing_ms\"]))\n",
        "    if feat == \"edge_vis_width__max\":      return (\"edge\", safe_argmax(edge_df[\"vis_width\"]))\n",
        "    if feat == \"edge_completion_tokens__sum\":\n",
        "        x = edge_df[\"completion_tokens\"].fillna(0).astype(float); return (\"edge\", x / (x.sum() + eps))\n",
        "    return (None, None)\n",
        "\n",
        "# Aggregate local responsibility from top_local_df\n",
        "node_score = pd.Series(0.0, index=node_df.index)\n",
        "edge_score = pd.Series(0.0, index=edge_df.index)\n",
        "for _, r in top_local_df.iterrows():\n",
        "    feat = str(r[\"feature\"])\n",
        "    shap_val = float(r[\"shap_value\"])\n",
        "    kind, w = weights_for_feature(feat)\n",
        "    if w is None: continue\n",
        "    if kind == \"node\": node_score += shap_val * w\n",
        "    if kind == \"edge\": edge_score += shap_val * w\n",
        "\n",
        "# Shares among positives (what pushes toward attack)\n",
        "node_pos = node_score.clip(lower=0); node_share = (node_pos / (node_pos.sum() or 1.0)).fillna(0)\n",
        "edge_pos = edge_score.clip(lower=0); edge_share = (edge_pos / (edge_pos.sum() or 1.0)).fillna(0)\n",
        "\n",
        "top_nodes = node_df.assign(resp=node_score, share=node_share).sort_values(\"share\", ascending=False).head(5)\n",
        "top_edges = edge_df.assign(resp=edge_score, share=edge_share).sort_values(\"share\", ascending=False).head(5)\n",
        "\n",
        "print(\"\\nNode share (top 5):\")\n",
        "display(top_nodes[[\"node\",\"share\"]])\n",
        "print(\"\\nEdge share (top 5):\")\n",
        "display(top_edges[[\"u\",\"v\",\"share\"]])\n",
        "\n",
        "# =======================\n",
        "# 6) Counterfactuals (quick “patch test”) on this row\n",
        "# =======================\n",
        "\n",
        "train_medians_A = Xtr_A.median(numeric_only=True)\n",
        "\n",
        "def cf_delta(model, x_row_df, feat, to_val):\n",
        "    p0 = float(model.predict_proba(x_row_df)[0,1])\n",
        "    x_cf = x_row_df.copy()\n",
        "    x_cf.loc[:, feat] = to_val\n",
        "    p1 = float(model.predict_proba(x_cf)[0,1])\n",
        "    return p1 - p0, p0, p1\n",
        "\n",
        "top_pos_feats = (top_local_df.query(\"shap_value > 0\")\n",
        "                 .sort_values(\"shap_value\", ascending=False)\n",
        "                 .head(5)[\"feature\"].tolist())\n",
        "\n",
        "rows = []\n",
        "for f in top_pos_feats:\n",
        "    if f not in x_row.columns:\n",
        "        rows.append((f, np.nan, np.nan, np.nan, \"[skip: missing col]\")); continue\n",
        "    to_val = train_medians_A.get(f, np.nan)\n",
        "    if not np.isfinite(to_val):\n",
        "        rows.append((f, np.nan, np.nan, np.nan, \"[skip: no finite train median]\")); continue\n",
        "    d, p0, p1 = cf_delta(hgb_A, x_row, f, to_val)\n",
        "    rows.append((f, float(to_val), p0, p1, d))\n",
        "\n",
        "cf_table = pd.DataFrame(rows, columns=[\"feature\", \"set_to_median\", \"p_now\", \"p_cf\", \"delta_prob\"])\n",
        "print(\"\\nCounterfactual table (ATTRS on this graph):\")\n",
        "display(cf_table)\n"
      ],
      "metadata": {
        "id": "znlsORCdCtpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "found = False\n",
        "for rs in range(1000, 1020):  # try a few seeds\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rs)\n",
        "    for train_idx2, val_idx2 in skf.split(np.zeros_like(y), y):\n",
        "        files_val2 = merged.iloc[val_idx2][file_col_m].tolist()\n",
        "        if target_file in files_val2:\n",
        "            print(\"Use random_state=\", rs)\n",
        "            train_idx, val_idx = train_idx2, val_idx2\n",
        "            found = True\n",
        "            break\n",
        "    if found: break\n",
        "if not found:\n",
        "    print(\"Did not find a split with that file in val; you can still explain it by using the TRAIN split instead.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "0TkXR-w8Cw0q",
        "outputId": "dde18be1-85db-4b42-88f0-84f9c293ea71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1562160044.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# try a few seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mskf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_idx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_idx2\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mskf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mfiles_val2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_idx2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile_col_m\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtarget_file\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles_val2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'y' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############################################################################################################################################"
      ],
      "metadata": {
        "id": "bPcXHC8P9O3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##############################################  Lets get back to our code to see how we can find the suspicious node and edge ##########################"
      ],
      "metadata": {
        "id": "yy2yI6ELLwRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def build_train_median_from_train(X_train_attr: pd.DataFrame) -> pd.Series:\n",
        "    # best practice: use TRAIN medians\n",
        "    return X_train_attr.median(numeric_only=True)\n",
        "\n",
        "def counterfactual_deltas_no_fold(model, x_row_df, train_median: pd.Series, feats, scoring=\"proba\"):\n",
        "    \"\"\"\n",
        "    x_row_df: DataFrame with a SINGLE row from the same column order used to fit `model`.\n",
        "    train_median: Series with TRAIN medians indexed by column name.\n",
        "    feats: list of feature names to neutralize.\n",
        "    scoring: \"proba\" (probability drop) or \"logit\" (raw decision_function drop, if supported).\n",
        "    \"\"\"\n",
        "    assert isinstance(x_row_df, pd.DataFrame) and x_row_df.shape[0] == 1, \"x_row_df must be single-row DataFrame\"\n",
        "    use_cols = x_row_df.columns\n",
        "    # base score\n",
        "    if scoring == \"proba\":\n",
        "        p_base = float(model.predict_proba(x_row_df)[0, 1])\n",
        "        def score(X): return float(model.predict_proba(X)[0, 1])\n",
        "    else:\n",
        "        # for linear/gbdt raw scores if you prefer; fall back to proba if not available\n",
        "        if hasattr(model, \"decision_function\"):\n",
        "            p_base = float(model.decision_function(x_row_df)[0])\n",
        "            def score(X): return float(model.decision_function(X)[0])\n",
        "        else:\n",
        "            p_base = float(model.predict_proba(x_row_df)[0, 1])\n",
        "            def score(X): return float(model.predict_proba(X)[0, 1])\n",
        "\n",
        "    rows = []\n",
        "    for f in feats:\n",
        "        if f not in use_cols:\n",
        "            rows.append((f, np.nan, \"[skip: not in columns]\"))\n",
        "            continue\n",
        "        x_cf = x_row_df.copy()\n",
        "        x_cf.loc[:, f] = train_median.get(f, np.nan)\n",
        "        p_cf = score(x_cf)\n",
        "        rows.append((f, p_cf - p_base, train_median.get(f, np.nan)))\n",
        "    return pd.DataFrame(rows, columns=[\"feature\", \"delta_prob\", \"set_to\"])\n",
        "\n",
        "# --- HOW TO CALL IT QUICKLY ---\n",
        "# You already have:\n",
        "#   - hgb_A (your fitted ATTRS HGB model)\n",
        "#   - Xtr_A (train), Xva_A (val)\n",
        "#   - i_local_rel (index within Xva_A you explained)\n",
        "#   - top_local_df (DataFrame with local SHAP)\n",
        "\n",
        "x0 = Xva_A.iloc[[i_local_rel]].copy()\n",
        "train_median_A = build_train_median_from_train(Xtr_A)\n",
        "\n",
        "top_feats = (top_local_df.sort_values(\"shap_value\", ascending=False)\n",
        "                           .query(\"shap_value > 0\")\n",
        "                           .head(5)[\"feature\"].tolist())\n",
        "\n",
        "cf_table = counterfactual_deltas_no_fold(hgb_A, x0, train_median_A, top_feats, scoring=\"proba\")\n",
        "print(cf_table)\n"
      ],
      "metadata": {
        "id": "t1urnxJTLwWB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"folds stored:\", list(attr_models_by_fold.keys()))\n"
      ],
      "metadata": {
        "id": "84SL3DiILwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########### Validate “culprit node/edge” across all graphs (before patching)"
      ],
      "metadata": {
        "id": "SfYNj46ZLwb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "# --- reuse your weights_for_feature / safe_argmax / safe_argmin from before ---\n",
        "def safe_argmax(s):\n",
        "    idx = s.fillna(-np.inf).astype(float).idxmax()\n",
        "    w = pd.Series(0.0, index=s.index); w.loc[idx]=1.0; return w\n",
        "\n",
        "def safe_argmin(s):\n",
        "    idx = s.fillna(+np.inf).astype(float).idxmin()\n",
        "    w = pd.Series(0.0, index=s.index); w.loc[idx]=1.0; return w\n",
        "\n",
        "def weights_for_feature(feat, node_df, edge_df):\n",
        "    eps = 1e-12\n",
        "    if feat == \"node_power_w_mean__max\":      return (\"node\", safe_argmax(node_df[\"power_w_mean\"]))\n",
        "    if feat == \"node_cs_mean__min\":           return (\"node\", safe_argmin(node_df[\"cs_mean\"]))\n",
        "    if feat == \"node_cs_mean__mean\":          return (\"node\", node_df[\"cs_mean\"].fillna(0)/ (node_df[\"cs_mean\"].fillna(0).sum()+eps))\n",
        "    if feat == \"node_us_mean__max\":           return (\"node\", safe_argmax(node_df[\"us_mean\"]))\n",
        "    if feat == \"node_us_mean__std\":\n",
        "        dev = (node_df[\"us_mean\"]-node_df[\"us_mean\"].mean()).abs()\n",
        "        return (\"node\", dev / (dev.sum()+eps))\n",
        "    if feat == \"node_gpu_util_mean__mean\":\n",
        "        x = node_df[\"gpu_util_mean\"].fillna(0); return (\"node\", x / (x.sum()+eps))\n",
        "    if feat == \"edge_prompt_tokens__mean\":\n",
        "        x = edge_df[\"prompt_tokens\"].fillna(0); return (\"edge\", x / (x.sum()+eps))\n",
        "    if feat == \"edge_prompt_tokens__std\":\n",
        "        dev = (edge_df[\"prompt_tokens\"]-edge_df[\"prompt_tokens\"].mean()).abs()\n",
        "        return (\"edge\", dev / (dev.sum()+eps))\n",
        "    if feat == \"edge_timing_ms__max\":         return (\"edge\", safe_argmax(edge_df[\"timing_ms\"]))\n",
        "    if feat == \"edge_vis_width__max\":         return (\"edge\", safe_argmax(edge_df[\"vis_width\"]))\n",
        "    if feat == \"edge_completion_tokens__sum\":\n",
        "        x = edge_df[\"completion_tokens\"].fillna(0); return (\"edge\", x / (x.sum()+eps))\n",
        "    # default: ignore\n",
        "    return (None, None)\n",
        "\n",
        "def load_gpickle_any(path):\n",
        "    if hasattr(nx, \"read_gpickle\"):\n",
        "        try: return nx.read_gpickle(path)\n",
        "        except Exception: pass\n",
        "    with open(path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# local SHAP -> node/edge shares for ONE row\n",
        "def local_responsibility(top_local_df, G):\n",
        "    node_df = pd.DataFrame([{\n",
        "        \"node\": n,\n",
        "        \"power_w_mean\":   G.nodes[n].get(\"power_w_mean\"),\n",
        "        \"cs_mean\":        G.nodes[n].get(\"cs_mean\"),\n",
        "        \"us_mean\":        G.nodes[n].get(\"us_mean\"),\n",
        "        \"gpu_util_mean\":  G.nodes[n].get(\"gpu_util_mean\")}\n",
        "        for n in G.nodes()\n",
        "    ])\n",
        "    edge_df = pd.DataFrame([{\n",
        "        \"u\": u, \"v\": v,\n",
        "        \"prompt_tokens\":     G[u][v].get(\"prompt_tokens\"),\n",
        "        \"completion_tokens\": G[u][v].get(\"completion_tokens\"),\n",
        "        \"timing_ms\":         G[u][v].get(\"timing_ms\"),\n",
        "        \"vis_width\":         G[u][v].get(\"vis_width\")}\n",
        "        for u, v in G.edges()\n",
        "    ])\n",
        "\n",
        "    node_score = pd.Series(0.0, index=node_df.index)\n",
        "    edge_score = pd.Series(0.0, index=edge_df.index)\n",
        "\n",
        "    for _, r in top_local_df.iterrows():\n",
        "        feat, phi = str(r[\"feature\"]), float(r[\"shap_value\"])\n",
        "        kind, w = weights_for_feature(feat, node_df, edge_df)\n",
        "        if kind == \"node\" and w is not None: node_score += phi * w\n",
        "        if kind == \"edge\" and w is not None: edge_score += phi * w\n",
        "\n",
        "    node_pos = node_score.clip(lower=0)\n",
        "    edge_pos = edge_score.clip(lower=0)\n",
        "    node_share = (node_pos / (node_pos.sum() if node_pos.sum() > 0 else 1.0)).fillna(0)\n",
        "    edge_share = (edge_pos / (edge_pos.sum() if edge_pos.sum() > 0 else 1.0)).fillna(0)\n",
        "\n",
        "    top_node_idx = int(node_share.idxmax()) if len(node_share) else -1\n",
        "    top_edge_idx = int(edge_share.idxmax()) if len(edge_share) else -1\n",
        "    top_node = node_df.loc[top_node_idx, \"node\"] if top_node_idx >= 0 else None\n",
        "    top_edge = (edge_df.loc[top_edge_idx, \"u\"], edge_df.loc[top_edge_idx, \"v\"]) if top_edge_idx >= 0 else (None,None)\n",
        "\n",
        "    return {\n",
        "        \"top_node\": top_node,\n",
        "        \"top_node_share\": float(node_share.max() if len(node_share) else 0.0),\n",
        "        \"top_edge_u\": top_edge[0],\n",
        "        \"top_edge_v\": top_edge[1],\n",
        "        \"top_edge_share\": float(edge_share.max() if len(edge_share) else 0.0),\n",
        "    }\n",
        "\n",
        "# === BATCH over a split ===\n",
        "# we’ll use ATTRS model for selection; you can switch to COMBINED if preferred.\n",
        "results = []\n",
        "prob_thresh = 0.90   # analyze graphs the model believes are attacks\n",
        "K_LOCAL = 20         # how many local features to consider when projecting to nodes/edges\n",
        "\n",
        "# (Assumes you already have shap_expl_A created and Xva_A, files_val aligned)\n",
        "files_val = merged.iloc[val_idx][file_col_m].tolist()\n",
        "\n",
        "# Predict probabilities (for selection)\n",
        "p_attr_all = hgb_A.predict_proba(Xva_A)[:,1]\n",
        "\n",
        "for i_rel, file_path in enumerate(files_val):\n",
        "    # we can also gate by true label if you prefer: yva[i_rel] == 1\n",
        "    if p_attr_all[i_rel] < prob_thresh:\n",
        "        continue\n",
        "\n",
        "    # local SHAP for this row\n",
        "    x_row = Xva_A.iloc[[i_rel]]\n",
        "    phi_i = shap_expl_A.shap_values(x_row, check_additivity=False)\n",
        "    phi_i = phi_i if isinstance(phi_i, np.ndarray) else phi_i[0]\n",
        "    local_i = (pd.DataFrame({\"feature\": Xva_A.columns,\n",
        "                             \"shap_value\": phi_i.flatten(),\n",
        "                             \"value\": x_row.values.flatten()})\n",
        "               .sort_values(\"shap_value\", ascending=False)\n",
        "               .head(K_LOCAL))\n",
        "\n",
        "    # map to node/edge\n",
        "    G = load_gpickle_any(file_path)\n",
        "    res = local_responsibility(local_i, G)\n",
        "    res.update({\n",
        "        \"file\": file_path,\n",
        "        \"p_attr\": float(p_attr_all[i_rel]),\n",
        "        \"top_feats\": \";\".join(local_i[\"feature\"].head(5).tolist()),\n",
        "    })\n",
        "    results.append(res)\n",
        "\n",
        "summary_df = pd.DataFrame(results)\n",
        "print(\"Summaries on high-probability attacks:\")\n",
        "print(summary_df[[\"top_node_share\",\"top_edge_share\"]].describe())\n",
        "\n",
        "# Coverage metrics:\n",
        "frac_node_dominant = (summary_df[\"top_node_share\"] >= 0.5).mean() if len(summary_df) else np.nan\n",
        "frac_edge_dominant = (summary_df[\"top_edge_share\"] >= 0.4).mean() if len(summary_df) else np.nan\n",
        "print(f\"Fraction with top_node_share ≥ 0.5: {frac_node_dominant:.3f}\")\n",
        "print(f\"Fraction with top_edge_share ≥ 0.4: {frac_edge_dominant:.3f}\")\n",
        "\n",
        "# Feature stability:\n",
        "from collections import Counter\n",
        "feat_counts = Counter()\n",
        "for s in summary_df[\"top_feats\"]:\n",
        "    for f in s.split(\";\"): feat_counts[f] += 1\n",
        "print(\"Most frequent local top features:\", feat_counts.most_common(10))\n",
        "\n",
        "# Save for inspection\n",
        "summary_df.to_csv(\"culprit_node_edge_summary.csv\", index=False)\n",
        "print(\"Wrote culprit_node_edge_summary.csv\")\n"
      ],
      "metadata": {
        "id": "qdvmttRzLwej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets see if it is always the same node and edge ?"
      ],
      "metadata": {
        "id": "BVUM4AYYTuS6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, ast\n",
        "\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "\n",
        "# If your 'top_node' column looks like \"('graphid', 3)\", parse the tuple and take the second element (index)\n",
        "def parse_tuple(s):\n",
        "    try: return ast.literal_eval(s)\n",
        "    except: return (None, None)\n",
        "\n",
        "tmp = df['top_node'].apply(parse_tuple)\n",
        "df['top_node_graph'], df['top_node_idx'] = zip(*tmp)\n",
        "\n",
        "# If you stored a stable label (e.g., df['top_node_agent']), count repeats:\n",
        "if 'top_node_agent' in df.columns:\n",
        "    print(df['top_node_agent'].value_counts().head(10))\n",
        "\n",
        "# Same idea for edges (parse both ends), then map to agent/hostnames if available\n",
        "import pandas as pd\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "print(df.columns.tolist())\n",
        "print(df.head(3))\n"
      ],
      "metadata": {
        "id": "kKLFmiW-Lwh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, ast, pickle\n",
        "import networkx as nx\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/Attack_bundle/_exports\"  # <- adjust if different\n",
        "\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "\n",
        "def parse_tuple(s):\n",
        "    try:\n",
        "        return ast.literal_eval(s)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Handle top_node stored like \"('0679...b47', 1)\"\n",
        "if 'top_node' in df.columns:\n",
        "    tup = df['top_node'].apply(parse_tuple)\n",
        "    df['top_node_graph'] = tup.apply(lambda x: x[0] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "    df['top_node_idx']   = tup.apply(lambda x: x[1] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "\n",
        "# Handle top_edge in either one or two columns\n",
        "if 'top_edge' in df.columns:\n",
        "    etup = df['top_edge'].apply(parse_tuple)\n",
        "    df['top_edge_u'] = etup.apply(lambda x: x[0] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "    df['top_edge_v'] = etup.apply(lambda x: x[1] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "if 'top_edge_u' in df.columns and isinstance(df['top_edge_u'].iloc[0], str):\n",
        "    df['top_edge_u'] = df['top_edge_u'].apply(parse_tuple)\n",
        "if 'top_edge_v' in df.columns and isinstance(df['top_edge_v'].iloc[0], str):\n",
        "    df['top_edge_v'] = df['top_edge_v'].apply(parse_tuple)\n",
        "\n",
        "# Lazy graph loader with cache\n",
        "_graph_cache = {}\n",
        "def load_graph(gid):\n",
        "    if gid in _graph_cache:\n",
        "        return _graph_cache[gid]\n",
        "    path = os.path.join(BASE_DIR, f\"{gid}.gpickle\")\n",
        "    G = None\n",
        "    if hasattr(nx, \"read_gpickle\"):\n",
        "        try:\n",
        "            G = nx.read_gpickle(path)\n",
        "        except Exception:\n",
        "            pass\n",
        "    if G is None:\n",
        "        with open(path, \"rb\") as f:\n",
        "            G = pickle.load(f)\n",
        "    _graph_cache[gid] = G\n",
        "    return G\n",
        "\n",
        "# Choose a stable label in priority order\n",
        "NODE_LABEL_KEYS = [\"agent\", \"agent_id\", \"hostname\", \"name\", \"role\", \"label\"]\n",
        "\n",
        "def node_label(G, n):\n",
        "    d = G.nodes[n]\n",
        "    for k in NODE_LABEL_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            return str(d[k])\n",
        "    return str(n)\n",
        "\n",
        "def label_from_tuple(tup):\n",
        "    \"\"\"tup is ('graph_id', node_idx)\"\"\"\n",
        "    if not (isinstance(tup, tuple) and len(tup)==2):\n",
        "        return None\n",
        "    gid, idx = tup\n",
        "    try:\n",
        "        G = load_graph(gid)\n",
        "        return node_label(G, idx)\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "# Build labels\n",
        "if 'top_node_graph' in df.columns and 'top_node_idx' in df.columns:\n",
        "    df['top_node_agent'] = [\n",
        "        label_from_tuple((gid, idx)) if pd.notna(gid) and pd.notna(idx) else None\n",
        "        for gid, idx in zip(df['top_node_graph'], df['top_node_idx'])\n",
        "    ]\n",
        "\n",
        "if 'top_edge_u' in df.columns and 'top_edge_v' in df.columns:\n",
        "    def edge_labels(uv):\n",
        "        try:\n",
        "            (gid_u, iu), (gid_v, iv) = uv\n",
        "            if gid_u != gid_v:  # most should match; if not, fall back\n",
        "                return None\n",
        "            G = load_graph(gid_u)\n",
        "            return f\"{node_label(G, iu)}→{node_label(G, iv)}\"\n",
        "        except Exception:\n",
        "            return None\n",
        "    df['top_edge_agents'] = [\n",
        "        edge_labels((u, v)) if (isinstance(u, tuple) and isinstance(v, tuple)) else None\n",
        "        for u, v in zip(df['top_edge_u'], df['top_edge_v'])\n",
        "    ]\n",
        "\n",
        "# (Optional) persist enriched CSV\n",
        "df.to_csv(\"culprit_node_edge_summary_enriched.csv\", index=False)\n",
        "print(\"Saved: culprit_node_edge_summary_enriched.csv\")\n"
      ],
      "metadata": {
        "id": "OpPqGZbjLwk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Distribution by node/edge index (position)"
      ],
      "metadata": {
        "id": "YLmg0W0wUa8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, ast\n",
        "from collections import Counter\n",
        "\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "\n",
        "def parse_tuple(s):\n",
        "    try: return ast.literal_eval(s)\n",
        "    except: return None\n",
        "\n",
        "# Parse top_node: (\"graph_id\", idx)\n",
        "tn = df['top_node'].apply(parse_tuple)\n",
        "df['top_node_idx'] = tn.apply(lambda x: x[1] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "\n",
        "# Parse edge: (\"graph_id\", iu) / (\"graph_id\", iv)\n",
        "tu = df['top_edge_u'].apply(parse_tuple)\n",
        "tv = df['top_edge_v'].apply(parse_tuple)\n",
        "df['top_edge_idx_u'] = tu.apply(lambda x: x[1] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "df['top_edge_idx_v'] = tv.apply(lambda x: x[1] if isinstance(x, tuple) and len(x)==2 else None)\n",
        "\n",
        "# Node-index frequency\n",
        "node_idx_counts = (df['top_node_idx']\n",
        "                   .dropna().astype(int).value_counts().sort_index())\n",
        "print(\"\\nCulprit node index frequency:\")\n",
        "print(node_idx_counts)\n",
        "\n",
        "# Edge-index pair frequency\n",
        "edge_idx_pairs = (df[['top_edge_idx_u','top_edge_idx_v']]\n",
        "                  .dropna().astype(int))\n",
        "edge_pair_counts = (edge_idx_pairs.value_counts().rename_axis(['u_idx','v_idx'])\n",
        "                    .reset_index(name='count').sort_values('count', ascending=False))\n",
        "print(\"\\nCulprit edge (u_idx→v_idx) frequency (top 10):\")\n",
        "print(edge_pair_counts.head(10))\n",
        "\n",
        "# Save to CSVs if you want\n",
        "node_idx_counts.to_csv(\"dist_culprit_node_index.csv\")\n",
        "edge_pair_counts.to_csv(\"dist_culprit_edge_index_pairs.csv\", index=False)\n",
        "print(\"\\nWrote: dist_culprit_node_index.csv, dist_culprit_edge_index_pairs.csv\")\n"
      ],
      "metadata": {
        "id": "SojuJSOYUbI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2) Distributions of responsibility strength"
      ],
      "metadata": {
        "id": "KbTZ2h9CU46v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "\n",
        "# Summary stats\n",
        "print(\"\\nNode share summary:\")\n",
        "print(df['top_node_share'].describe())\n",
        "print(\"\\nEdge share summary:\")\n",
        "print(df['top_edge_share'].describe())\n",
        "\n",
        "# Simple bins (counts per bin)\n",
        "node_bins = pd.cut(df['top_node_share'], bins=[0,0.25,0.5,0.75,1.0], include_lowest=True).value_counts().sort_index()\n",
        "edge_bins = pd.cut(df['top_edge_share'], bins=[0,0.25,0.5,0.75,1.0], include_lowest=True).value_counts().sort_index()\n",
        "print(\"\\nNode share bins:\\n\", node_bins)\n",
        "print(\"\\nEdge share bins:\\n\", edge_bins)\n",
        "\n",
        "# Threshold prevalence\n",
        "frac_node_ge_06 = (df['top_node_share']>=0.60).mean()\n",
        "frac_edge_ge_08 = (df['top_edge_share']>=0.80).mean()\n",
        "print(f\"\\nFrac graphs with top_node_share ≥0.60: {frac_node_ge_06:.3f}\")\n",
        "print(f\"Frac graphs with top_edge_share ≥0.80: {frac_edge_ge_08:.3f}\")\n"
      ],
      "metadata": {
        "id": "lk6DrnSnU5CR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3) Distribution by reason (features behind the attacks)"
      ],
      "metadata": {
        "id": "g1SARjm0U5FK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "\n",
        "# explode top_feats into rows\n",
        "tf = (df.assign(feature=df['top_feats'].fillna('')\n",
        "        .str.split(';'))\n",
        "        .explode('feature'))\n",
        "tf['feature'] = tf['feature'].str.strip()\n",
        "tf = tf[tf['feature']!='']\n",
        "\n",
        "feat_counts = tf['feature'].value_counts()\n",
        "print(\"\\nMost frequent local top features (across attacks):\")\n",
        "print(feat_counts.head(30))\n",
        "\n",
        "# Optionally bucket features into categories (node vs edge, compute vs text/latency)\n",
        "def bucket(f):\n",
        "    f=f.lower()\n",
        "    if f.startswith('node_'): return 'node'\n",
        "    if f.startswith('edge_'): return 'edge'\n",
        "    return 'other'\n",
        "tf['bucket'] = tf['feature'].map(bucket)\n",
        "print(\"\\nCounts by bucket:\")\n",
        "print(tf['bucket'].value_counts())\n"
      ],
      "metadata": {
        "id": "t65BY_xtU5IE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#How to patch (playbook):"
      ],
      "metadata": {
        "id": "Pba-zYa1U5Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Edge-first patch (cheapest & most specific)\n",
        "\n",
        "# When a graph has a culprit edge (e.g., 1→2) and it’s backed by outlier edge features:\n",
        "\n",
        "# Prompt caps: clamp edge_prompt_tokens__mean to the train P90 (or 21–22 in your PDP knee).\n",
        "\n",
        "# Width caps: clamp edge_vis_width__max to train P90.\n",
        "\n",
        "# Latency budget: reject or degrade traffic when edge_timing_ms__max > train P95 (or route around).\n",
        "\n",
        "# Burst smoothing: for *_win__std features, apply batching/debounce to reduce variance.\n",
        "\n",
        "# 2) Node patch (when node dominates)\n",
        "\n",
        "# For the culprit node:\n",
        "\n",
        "# Util variance smoothing: reduce node_us_mean__std (buffering, CPU scheduler niceness, request jitter).\n",
        "\n",
        "# CPU contention: throttle or reschedule to reduce node_cs_mean__min (sustained cs), move load off that node.\n",
        "\n",
        "# GPU power/util caps: cap node_power_w_mean__max (or job power limit), enforce max GPU util.\n",
        "\n",
        "# 3) Decision gates (to avoid false cuts)\n",
        "\n",
        "# Act only if both are true:\n",
        "\n",
        "# Share: top_node_share ≥ 0.60 or top_edge_share ≥ 0.80 (0.40 if you want recall), and\n",
        "\n",
        "# Outlier: implicated feature > train P90/P95 (or z-score > 2)."
      ],
      "metadata": {
        "id": "sUFwI_EQaGY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd\n",
        "\n",
        "# 1) Compute train quantiles for guard rails\n",
        "Q = Xtr_A.quantile([0.9, 0.95], numeric_only=True).T\n",
        "Q.columns = [\"P90\",\"P95\"]\n",
        "\n",
        "# 2) One-graph patch simulator\n",
        "def simulate_patch(model, x_row_df, actions):\n",
        "    \"\"\"\n",
        "    actions: list of (feature_name, to_value) to clamp.\n",
        "    Returns: p_before, p_after, deltas table\n",
        "    \"\"\"\n",
        "    p0 = float(model.predict_proba(x_row_df)[0,1])\n",
        "    x_cf = x_row_df.copy()\n",
        "    rows=[]\n",
        "    for feat, to_val in actions:\n",
        "        if feat in x_cf.columns:\n",
        "            x_cf.loc[:, feat] = to_val\n",
        "            rows.append((feat, float(to_val)))\n",
        "    p1 = float(model.predict_proba(x_cf)[0,1])\n",
        "    delta = p1 - p0\n",
        "    return p0, p1, delta, pd.DataFrame(rows, columns=[\"feature\",\"set_to\"])\n",
        "\n",
        "# 3) Pick top positive local features for the current graph\n",
        "def top_local_positive(top_local_df, k=5):\n",
        "    return (top_local_df[top_local_df[\"shap_value\"]>0]\n",
        "            .sort_values(\"shap_value\", ascending=False)\n",
        "            .head(k)[[\"feature\",\"value\"]])\n",
        "\n",
        "# 4) Build an action list guided by quantiles (edge-first, then node)\n",
        "def propose_actions_for_graph(top_local_df, Q, train_medians):\n",
        "    feats = top_local_positive(top_local_df, k=6)\n",
        "    actions = []\n",
        "    for f, val in feats.itertuples(index=False):\n",
        "        # choose a realistic clamp (prefer P90, fall back to median)\n",
        "        if f in Q.index:\n",
        "            to_val = float(Q.loc[f, \"P90\"])\n",
        "        else:\n",
        "            to_val = float(train_medians.get(f, np.nan))\n",
        "        if np.isfinite(to_val):\n",
        "            actions.append((f, to_val))\n",
        "    return actions\n",
        "\n",
        "# 5) Example on your current explained graph (x_row, top_local_df exist)\n",
        "actions = propose_actions_for_graph(top_local_df, Q, train_medians_A)\n",
        "p0, p1, dP, action_table = simulate_patch(hgb_A, x_row, actions)\n",
        "print(f\"P(attack) {p0:.3f} → {p1:.3f} (Δ={dP:+.3f})\")\n",
        "display(action_table)\n"
      ],
      "metadata": {
        "id": "JS55OaDiaGb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Targeted edge-only vs node-only tests"
      ],
      "metadata": {
        "id": "_iNXVVu2azAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EDGE_KEYS = [\"edge_prompt_tokens__mean\",\"edge_vis_width__max\",\"edge_timing_ms__max\",\n",
        "             \"edge_cs_mean_win__std\",\"edge_us_mean_win__std\",\"edge_completion_tokens__sum\"]\n",
        "NODE_KEYS = [\"node_power_w_mean__max\",\"node_cs_mean__min\",\"node_us_mean__max\",\"node_us_mean__std\",\n",
        "             \"node_gpu_util_mean__mean\",\"node_gpu_util_mean__max\"]\n",
        "\n",
        "def propose_subset(top_local_df, keys, Q, train_medians):\n",
        "    feats = top_local_positive(top_local_df, k=8)\n",
        "    out=[]\n",
        "    for f,_ in feats.itertuples(index=False):\n",
        "        if f in keys:\n",
        "            to_val = float(Q.loc[f,\"P90\"]) if f in Q.index else float(train_medians.get(f, np.nan))\n",
        "            if np.isfinite(to_val): out.append((f,to_val))\n",
        "    return out\n",
        "\n",
        "edge_actions = propose_subset(top_local_df, EDGE_KEYS, Q, train_medians_A)\n",
        "node_actions = propose_subset(top_local_df, NODE_KEYS, Q, train_medians_A)\n",
        "\n",
        "print(\"Edge actions:\", edge_actions)\n",
        "print(\"Node actions:\", node_actions)\n",
        "\n",
        "p0, p1e, dPe, _ = simulate_patch(hgb_A, x_row, edge_actions)\n",
        "_,  p1n, dPn, _ = simulate_patch(hgb_A, x_row, node_actions)\n",
        "print(f\"Edge-only ΔP={dPe:+.3f} | Node-only ΔP={dPn:+.3f} | Baseline={p0:.3f}\")\n"
      ],
      "metadata": {
        "id": "hprCj3tGaGe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch patch recommender (produce a CSV)"
      ],
      "metadata": {
        "id": "RBiUhqveaGhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd, ast, numpy as np\n",
        "\n",
        "summary = pd.read_csv(\"culprit_node_edge_summary.csv\")\n",
        "# requires columns: file, p_attr, top_node_share, top_edge_share, top_feats (semicolon list)\n",
        "\n",
        "def parse_feats(s):\n",
        "    try:\n",
        "        # already semicolon-separated string of feature names\n",
        "        return [t.strip() for t in str(s).split(\";\") if t.strip()]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "def pick_actions_from_list(feat_list, Q, train_medians, max_k=4):\n",
        "    actions=[]\n",
        "    for f in feat_list[:max_k]:\n",
        "        if f in Q.index:\n",
        "            actions.append((f, float(Q.loc[f, \"P90\"])))\n",
        "        elif f in train_medians.index:\n",
        "            actions.append((f, float(train_medians[f])))\n",
        "    return actions\n",
        "\n",
        "rows=[]\n",
        "for _,r in summary.iterrows():\n",
        "    feats = parse_feats(r.get(\"top_feats\",\"\"))\n",
        "    actions_edge = [f for f in feats if f.startswith(\"edge_\")]\n",
        "    actions_node = [f for f in feats if f.startswith(\"node_\")]\n",
        "    a_edge = pick_actions_from_list(actions_edge, Q, Xtr_A.median(numeric_only=True), max_k=3)\n",
        "    a_node = pick_actions_from_list(actions_node, Q, Xtr_A.median(numeric_only=True), max_k=3)\n",
        "    rows.append({\n",
        "        \"file\": r[\"file\"],\n",
        "        \"p_attr\": r[\"p_attr\"],\n",
        "        \"top_node_share\": r[\"top_node_share\"],\n",
        "        \"top_edge_share\": r[\"top_edge_share\"],\n",
        "        \"edge_actions\": a_edge,\n",
        "        \"node_actions\": a_node\n",
        "    })\n",
        "\n",
        "patch_plan = pd.DataFrame(rows)\n",
        "patch_plan.to_csv(\"patch_plan.csv\", index=False)\n",
        "print(\"Wrote patch_plan.csv (per-graph proposed clamps)\")\n"
      ],
      "metadata": {
        "id": "ObYmqCJgaGjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "############################ Early detection ######################################"
      ],
      "metadata": {
        "id": "xb9tEJ-hcVG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION (Train once on full TRAIN, test on TEST prefixes)\n",
        "# Compatible with your notebook: reuses `merged`, `y`, and (optional) `load_gpickle`\n",
        "# =====================================================================================\n",
        "import math, warnings\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]\n",
        "TARGET_SPEC = 0.95\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 5           # outer split count\n",
        "TEST_FOLD = 0          # take the first split as test\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------- REQUIRED GLOBALS FROM YOUR NOTEBOOK ----------\n",
        "def _need(name: str):\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing `{name}` in this session (define it earlier in the notebook).\")\n",
        "_need(\"merged\"); _need(\"y\")\n",
        "\n",
        "# Column that holds file paths\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "if file_col_m is None:\n",
        "    raise RuntimeError(\"`merged` must contain a 'file' or 'path' (or 'filepath'/'graph_path') column pointing to .gpickle files.\")\n",
        "paths = [Path(str(p)) for p in merged[file_col_m].tolist()]\n",
        "\n",
        "# Graph loader (use your loader if present)\n",
        "if \"load_gpickle\" in globals():\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "# --------- PICK A FEATURE FUNCTION FROM YOUR SESSION OR FALL BACK ----------\n",
        "# We try a list of common names you might have used. The first one found is used.\n",
        "_CANDIDATE_FEATURE_FUNCS = [\n",
        "    \"tiny_topo_features\", \"smart_topo_features\", \"topo_features\",\n",
        "    \"features_for_graph\", \"make_features\", \"graph_features\",\n",
        "    \"featurize_graph\", \"featureize_graph\"\n",
        "]\n",
        "_selected = None\n",
        "for _fname in _CANDIDATE_FEATURE_FUNCS:\n",
        "    if _fname in globals() and callable(globals()[_fname]):\n",
        "        _selected = globals()[_fname]\n",
        "        break\n",
        "\n",
        "def _fallback_features(G: nx.Graph) -> pd.Series:\n",
        "    \"\"\"Robust, small-graph-safe global features (used only if none found in your session).\"\"\"\n",
        "    try:\n",
        "        is_multi = isinstance(G, (nx.MultiGraph, nx.MultiDiGraph))\n",
        "        H = G  # keep as is (don’t convert structure)\n",
        "        n = H.number_of_nodes(); m = H.number_of_edges()\n",
        "        feats = dict(\n",
        "            n_nodes=n,\n",
        "            n_edges=m,\n",
        "            density=(nx.density(H) if n > 1 else 0.0),\n",
        "            avg_deg=(float(np.mean([d for _, d in H.degree()])) if n > 0 else 0.0),\n",
        "            transitivity=(nx.transitivity(H) if n > 2 and not H.is_directed() else 0.0),\n",
        "        )\n",
        "        # assortativity can blow up on tiny graphs—guard it\n",
        "        try:\n",
        "            feats[\"assortativity\"] = (nx.degree_assortativity_coefficient(H) if n > 3 else 0.0)\n",
        "        except Exception:\n",
        "            feats[\"assortativity\"] = 0.0\n",
        "        # components\n",
        "        try:\n",
        "            if isinstance(H, (nx.DiGraph, nx.MultiDiGraph)):\n",
        "                comps = list(nx.weakly_connected_components(H))\n",
        "            else:\n",
        "                comps = list(nx.connected_components(H))\n",
        "            feats[\"n_components\"] = len(comps)\n",
        "            feats[\"largest_cc\"] = max((len(c) for c in comps), default=0)\n",
        "        except Exception:\n",
        "            feats[\"n_components\"] = 0\n",
        "            feats[\"largest_cc\"] = 0\n",
        "        return pd.Series(feats, dtype=\"float64\")\n",
        "    except Exception:\n",
        "        # As last resort, return a minimal safe vector\n",
        "        return pd.Series(dict(n_nodes=H.number_of_nodes(), n_edges=H.number_of_edges(), density=0.0), dtype=\"float64\")\n",
        "\n",
        "if _selected is None:\n",
        "    print(\"[INFO] No known feature function found in session; using built-in fallback.\")\n",
        "    features_for_graph = _fallback_features\n",
        "else:\n",
        "    print(f\"[INFO] Using your feature function: `{_selected.__name__}`\")\n",
        "    features_for_graph = _selected\n",
        "\n",
        "# -------- Temporal ordering and prefix creation --------\n",
        "_TS_KEYS = (\"end_ts\", \"ts\", \"timestamp\", \"time\", \"t\")\n",
        "\n",
        "def _edge_time(d):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return np.inf\n",
        "\n",
        "def _to_multi(G):\n",
        "    return G if isinstance(G, nx.MultiDiGraph) else nx.MultiDiGraph(G)\n",
        "\n",
        "def get_prefix_by_nodes(G_full: nx.MultiDiGraph, keep_frac: float):\n",
        "    \"\"\"\n",
        "    Keep the FIRST `keep_frac` fraction of nodes by their earliest timestamp of incident edges.\n",
        "    Nodes with no timestamp appear last (so they are included only at larger keep_fracs).\n",
        "    \"\"\"\n",
        "    assert 0.0 < keep_frac <= 1.0\n",
        "    G_full = _to_multi(G_full)\n",
        "    if G_full.number_of_nodes() == 0:\n",
        "        return G_full.__class__(), 0.0\n",
        "\n",
        "    node_times = {n: np.inf for n in G_full.nodes()}\n",
        "    for u, v, d in G_full.edges(data=True):\n",
        "        t = _edge_time(d)\n",
        "        if np.isfinite(t):\n",
        "            if t < node_times[u]: node_times[u] = t\n",
        "            if t < node_times[v]: node_times[v] = t\n",
        "\n",
        "    sorted_nodes = [n for n, t in sorted(node_times.items(), key=lambda it: it[1])]\n",
        "    n = len(sorted_nodes)\n",
        "    keep_k = max(1, int(round(n * keep_frac)))\n",
        "    nodes_to_keep = sorted_nodes[:keep_k]\n",
        "    H = G_full.subgraph(nodes_to_keep).copy()\n",
        "    return H, len(nodes_to_keep) / n\n",
        "\n",
        "# -------- Feature extraction for partial graphs --------\n",
        "def small_graph_stats(H: nx.Graph) -> dict:\n",
        "    n = H.number_of_nodes(); m = H.number_of_edges()\n",
        "    if n == 0:\n",
        "        return dict(n_nodes=0, n_edges=0, density=0.0, avg_deg=0.0, n_components=0, largest_cc=0)\n",
        "    degs = [d for _, d in H.degree()]\n",
        "    try:\n",
        "        comps = (list(nx.weakly_connected_components(H))\n",
        "                 if isinstance(H, (nx.DiGraph, nx.MultiDiGraph))\n",
        "                 else list(nx.connected_components(H)))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    return dict(\n",
        "        n_nodes=n, n_edges=m,\n",
        "        density=(nx.density(H) if n > 1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_components=len(comps),\n",
        "        largest_cc=max((len(c) for c in comps), default=0)\n",
        "    )\n",
        "\n",
        "def get_feature_row(G_partial, keep_frac):\n",
        "    base = features_for_graph(G_partial)\n",
        "    if not isinstance(base, pd.Series):\n",
        "        base = pd.Series(base)\n",
        "    # Always add robust extras + the keep fraction (helps prevent collapse)\n",
        "    extras = small_graph_stats(G_partial)\n",
        "    extras[\"keep_frac\"] = float(keep_frac)\n",
        "    for k, v in extras.items():\n",
        "        base[k] = v\n",
        "    # ensure numeric\n",
        "    base = base.apply(pd.to_numeric, errors=\"coerce\")\n",
        "    return base\n",
        "\n",
        "# -------- Model setup --------\n",
        "def _mk_hgb():\n",
        "    return Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "def _mk_lr():\n",
        "    return Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"sc\", StandardScaler(with_mean=True, with_std=True)),\n",
        "        (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))\n",
        "    ])\n",
        "\n",
        "def _mk_rf():\n",
        "    return Pipeline([\n",
        "        (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"clf\", RandomForestClassifier(\n",
        "            n_estimators=400, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE\n",
        "        ))\n",
        "    ])\n",
        "\n",
        "MODEL_FACTORIES = {\"HGB\": _mk_hgb, \"LogReg\": _mk_lr, \"RF\": _mk_rf}\n",
        "\n",
        "# ====================== Main Logic ======================\n",
        "# 1) One outer fold for train/test\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "splits = list(cv.split(np.arange(len(y)), y))\n",
        "tr_idx, te_idx = splits[TEST_FOLD]\n",
        "\n",
        "# 2) Train ONCE on FULL graphs (TRAIN)\n",
        "print(\"Step 1: Training models on full graphs from the training set...\")\n",
        "train_rows = []\n",
        "y_train_full = y[tr_idx]\n",
        "for i in tr_idx:\n",
        "    try:\n",
        "        G_full = _loader(paths[i])\n",
        "        train_rows.append(get_feature_row(G_full, keep_frac=1.0))\n",
        "    except Exception as e:\n",
        "        print(f\"  [WARN] Skipping train file {paths[i]}: {e}\")\n",
        "\n",
        "X_train_full = pd.DataFrame(train_rows)\n",
        "if X_train_full.empty:\n",
        "    raise RuntimeError(\"No training rows were created. Check that your paths point to valid .gpickle graphs.\")\n",
        "\n",
        "# Fix schema based on training columns\n",
        "TRAIN_SCHEMA = X_train_full.columns.tolist()\n",
        "\n",
        "# 3) Fit models\n",
        "models = {name: fac().fit(X_train_full, y_train_full) for name, fac in MODEL_FACTORIES.items()}\n",
        "print(f\"Trained {len(models)} models on {len(X_train_full)} full graphs with {len(TRAIN_SCHEMA)} features.\")\n",
        "\n",
        "# 4) Set θ on TRAIN (Target specificity = TARGET_SPEC)\n",
        "print(f\"Step 2: Calibrating thresholds for specificity ≈ {TARGET_SPEC:.2f} on TRAIN...\")\n",
        "thetas = {}\n",
        "for name, model in models.items():\n",
        "    p_train = model.predict_proba(X_train_full)[:, 1]\n",
        "    p_neg = p_train[y_train_full == 0]\n",
        "    if len(p_neg) == 0:\n",
        "        raise RuntimeError(\"No negatives in training set for threshold selection.\")\n",
        "    thetas[name] = float(np.quantile(p_neg, TARGET_SPEC))\n",
        "    print(f\"  - {name}: θ = {thetas[name]:.3f}\")\n",
        "\n",
        "# 5) Evaluate prefixes on TEST\n",
        "print(\"Step 3: Evaluating test prefixes...\")\n",
        "records = []\n",
        "for keep_frac in KEEP_FRACS:\n",
        "    part_rows = []\n",
        "    y_test_prefix = []\n",
        "    for i in te_idx:\n",
        "        try:\n",
        "            G_full = _loader(paths[i])\n",
        "            G_pref, actual_keep = get_prefix_by_nodes(G_full, keep_frac)\n",
        "            s = get_feature_row(G_pref, keep_frac=actual_keep).reindex(TRAIN_SCHEMA)\n",
        "            part_rows.append(s)\n",
        "            y_test_prefix.append(y[i])\n",
        "        except Exception as e:\n",
        "            print(f\"  [WARN] Skipping test file {paths[i]} at keep_frac={keep_frac:.2f}: {e}\")\n",
        "    if not part_rows:\n",
        "        continue\n",
        "\n",
        "    X_test = pd.DataFrame(part_rows)\n",
        "    y_test = np.array(y_test_prefix)\n",
        "\n",
        "    # Per model metrics\n",
        "    for name, model in models.items():\n",
        "        p = model.predict_proba(X_test)[:, 1]\n",
        "        yhat = (p >= thetas[name]).astype(int)\n",
        "\n",
        "        is_attack = (y_test == 1)\n",
        "        is_normal = (y_test == 0)\n",
        "\n",
        "        det = (yhat[is_attack] == 1).mean() if is_attack.any() else np.nan  # TPR/Recall on attacks\n",
        "        fpr = (yhat[is_normal] == 1).mean() if is_normal.any() else np.nan  # FPR on normals\n",
        "\n",
        "        records.append(dict(keep_frac=keep_frac, model=name, det_rate=det, fpr=fpr))\n",
        "\n",
        "# 6) Summary table\n",
        "summary_df = pd.DataFrame(records)\n",
        "summary_pivot = summary_df.pivot_table(index=\"keep_frac\", columns=\"model\", values=[\"det_rate\", \"fpr\"])\n",
        "with pd.option_context('display.max_rows', 200, 'display.width', 160):\n",
        "    print(\"\\n================== Early Detection Performance ==================\")\n",
        "    print(summary_pivot.round(3))\n",
        "\n",
        "# (Optional) Quick sanity print for θ stability & prefix dispersion\n",
        "print(\"\\n[θ per model]\", {k: round(v, 3) for k, v in thetas.items()})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aa_OMgS8hdk-",
        "outputId": "9c4a7a80-b7ab-495e-dcb2-43188562d346"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Using your feature function: `features_for_graph`\n",
            "Step 1: Training models on full graphs from the training set...\n",
            "Trained 3 models on 616 full graphs with 17 features.\n",
            "Step 2: Calibrating thresholds for specificity ≈ 0.95 on TRAIN...\n",
            "  - HGB: θ = 0.516\n",
            "  - LogReg: θ = 0.506\n",
            "  - RF: θ = 0.506\n",
            "Step 3: Evaluating test prefixes...\n",
            "\n",
            "================== Early Detection Performance ==================\n",
            "          det_rate                  fpr              \n",
            "model          HGB LogReg     RF    HGB LogReg     RF\n",
            "keep_frac                                            \n",
            "0.1000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.2000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.3000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.4000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.5000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.6000      1.0000 0.0130 0.0000 1.0000 0.0670 0.0000\n",
            "0.7000      1.0000 0.0130 0.0000 1.0000 0.0670 0.0000\n",
            "0.8000      1.0000 0.0130 0.0000 1.0000 0.0670 0.0000\n",
            "0.9000      1.0000 0.0130 0.0000 1.0000 0.0800 0.0000\n",
            "1.0000      1.0000 0.9750 0.9750 1.0000 0.9330 0.9200\n",
            "\n",
            "[θ per model] {'HGB': 0.516, 'LogReg': 0.506, 'RF': 0.506}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION (FINAL: Train once, test on prefixes with a robust feature set)\n",
        "# =====================================================================================\n",
        "import math, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS = [0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 1.00]\n",
        "TARGET_SPEC = 0.95\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS = 5\n",
        "TEST_FOLD = 0\n",
        "# --------------------------------------------\n",
        "\n",
        "# --------- REQUIRED GLOBALS FROM YOUR NOTEBOOK ----------\n",
        "def _need(name: str):\n",
        "    if name not in globals():\n",
        "        raise RuntimeError(f\"Missing `{name}` in this session.\")\n",
        "_need(\"merged\"); _need(\"y\")\n",
        "\n",
        "file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), None)\n",
        "if file_col_m is None: raise RuntimeError(\"`merged` must have a 'file' or 'path' column.\")\n",
        "paths = [Path(str(p)) for p in merged[file_col_m].tolist()]\n",
        "\n",
        "try:\n",
        "    _loader = load_gpickle\n",
        "except NameError:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "# -------- Temporal ordering and prefix creation --------\n",
        "_TS_KEYS = (\"end_ts\", \"ts\", \"timestamp\", \"time\", \"t\")\n",
        "def _edge_time(d):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return np.inf\n",
        "\n",
        "def _to_multi(G):\n",
        "    return G if isinstance(G, nx.MultiDiGraph) else nx.MultiDiGraph(G)\n",
        "\n",
        "def get_prefix_by_nodes(G_full: nx.MultiDiGraph, keep_frac: float):\n",
        "    assert 0.0 <= keep_frac <= 1.0\n",
        "    G_full = _to_multi(G_full)\n",
        "    if G_full.number_of_nodes() == 0 or keep_frac == 0.0:\n",
        "        return G_full.__class__(), 0.0\n",
        "\n",
        "    node_times = {n: np.inf for n in G_full.nodes()}\n",
        "    for u, v, d in G_full.edges(data=True):\n",
        "        t = _edge_time(d)\n",
        "        if np.isfinite(t):\n",
        "            node_times[u] = min(node_times.get(u, np.inf), t)\n",
        "            node_times[v] = min(node_times.get(v, np.inf), t)\n",
        "\n",
        "    sorted_nodes = [n for n, t in sorted(node_times.items(), key=lambda item: item[1])]\n",
        "    n = len(sorted_nodes)\n",
        "    keep_k = max(1, int(round(n * keep_frac))) if n > 0 else 0\n",
        "    nodes_to_keep = sorted_nodes[:keep_k]\n",
        "\n",
        "    H = G_full.subgraph(nodes_to_keep).copy()\n",
        "    return H, len(nodes_to_keep) / n if n > 0 else 0.0\n",
        "\n",
        "# -------- NEW: Robust Feature Extractor for Early Detection --------\n",
        "def features_for_early_detection(G: nx.Graph) -> pd.Series:\n",
        "    \"\"\"Computes a small, stable set of features that are always well-defined.\"\"\"\n",
        "    n = G.number_of_nodes()\n",
        "    m = G.number_of_edges()\n",
        "\n",
        "    if n == 0:\n",
        "        return pd.Series(dict(n_nodes=0, n_edges=0, density=0.0, avg_deg=0.0, n_components=0, largest_cc_frac=0.0), dtype=\"float64\")\n",
        "\n",
        "    degs = [d for _, d in G.degree()]\n",
        "\n",
        "    try:\n",
        "        if G.is_directed():\n",
        "            comps = list(nx.weakly_connected_components(G))\n",
        "        else:\n",
        "            comps = list(nx.connected_components(G))\n",
        "        n_comp = len(comps)\n",
        "        largest_cc = max((len(c) for c in comps), default=0)\n",
        "    except Exception:\n",
        "        n_comp, largest_cc = 1, n\n",
        "\n",
        "    feats = dict(\n",
        "        n_nodes=float(n),\n",
        "        n_edges=float(m),\n",
        "        density=nx.density(G) if n > 1 else 0.0,\n",
        "        avg_deg=float(np.mean(degs)) if degs else 0.0,\n",
        "        n_components=float(n_comp),\n",
        "        largest_cc_frac=float(largest_cc / n) if n > 0 else 0.0,\n",
        "    )\n",
        "    return pd.Series(feats, dtype=\"float64\")\n",
        "\n",
        "def get_feature_row(G_partial, keep_frac):\n",
        "    feats = features_for_early_detection(G_partial)\n",
        "    feats[\"keep_frac\"] = float(keep_frac)\n",
        "    return feats\n",
        "\n",
        "# -------- Model setup with safer pipelines --------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"vt\", VarianceThreshold(0.0)), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=400, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "\n",
        "MODEL_FACTORIES = {\"HGB\": _mk_hgb, \"LogReg\": _mk_lr, \"RF\": _mk_rf}\n",
        "\n",
        "# ====================== Main Logic ======================\n",
        "cv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "splits = list(cv.split(y, y))\n",
        "tr_idx, te_idx = splits[TEST_FOLD]\n",
        "\n",
        "print(\"Step 1: Training models on full graphs from the training set...\")\n",
        "train_rows = [get_feature_row(_loader(paths[i]), 1.0) for i in tr_idx if paths[i].exists()]\n",
        "X_train_full = pd.DataFrame(train_rows)\n",
        "y_train_full = y[tr_idx][:len(X_train_full)] # Align y to successfully loaded graphs\n",
        "\n",
        "TRAIN_SCHEMA = X_train_full.columns.tolist()\n",
        "models = {name: fac().fit(X_train_full, y_train_full) for name, fac in MODEL_FACTORIES.items()}\n",
        "print(f\"Trained {len(models)} models on {len(X_train_full)} full graphs.\")\n",
        "\n",
        "print(f\"Step 2: Calibrating thresholds for Specificity ≈ {TARGET_SPEC} on TRAIN...\")\n",
        "thetas = {}\n",
        "for name, model in models.items():\n",
        "    p_train_neg = model.predict_proba(X_train_full[y_train_full == 0])[:, 1]\n",
        "    thetas[name] = float(np.quantile(p_train_neg, TARGET_SPEC))\n",
        "    print(f\"  - {name}: θ = {thetas[name]:.3f}\")\n",
        "\n",
        "print(\"Step 3: Evaluating models on test graph prefixes...\")\n",
        "results = []\n",
        "for keep_frac in KEEP_FRACS:\n",
        "    test_rows, y_test_prefix = [], []\n",
        "    for i in te_idx:\n",
        "        if not paths[i].exists(): continue\n",
        "        G_full = _loader(paths[i])\n",
        "        G_prefix, actual_keep = get_prefix_by_nodes(G_full, keep_frac)\n",
        "        s = get_feature_row(G_prefix, actual_keep).reindex(TRAIN_SCHEMA)\n",
        "        test_rows.append(s)\n",
        "        y_test_prefix.append(y[i])\n",
        "\n",
        "    if not test_rows: continue\n",
        "\n",
        "    X_test_prefix = pd.DataFrame(test_rows)\n",
        "    y_test_prefix = np.array(y_test_prefix)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        p_test = model.predict_proba(X_test_prefix)[:, 1]\n",
        "        y_pred = (p_test >= thetas[name]).astype(int)\n",
        "\n",
        "        is_attack, is_normal = (y_test_prefix == 1), (y_test_prefix == 0)\n",
        "        det_rate = (y_pred[is_attack] == 1).mean() if is_attack.any() else np.nan\n",
        "        fpr = (y_pred[is_normal] == 1).mean() if is_normal.any() else np.nan\n",
        "\n",
        "        results.append({\"keep_frac\": keep_frac, \"model\": name, \"det_rate\": det_rate, \"fpr\": fpr})\n",
        "\n",
        "# ====================== Final Results ======================\n",
        "summary_df = pd.DataFrame(results)\n",
        "summary_pivot = summary_df.pivot_table(index=\"keep_frac\", columns=\"model\", values=[\"det_rate\", \"fpr\"])\n",
        "print(\"\\n================== Early Detection Performance ==================\")\n",
        "with pd.option_context('display.max_rows', 200, 'display.width', 160):\n",
        "    print(summary_pivot.round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-QG43BCkdzV",
        "outputId": "cc11587e-f1e9-462a-da5a-ba91d3e0172a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1: Training models on full graphs from the training set...\n",
            "Trained 3 models on 616 full graphs.\n",
            "Step 2: Calibrating thresholds for Specificity ≈ 0.95 on TRAIN...\n",
            "  - HGB: θ = 0.516\n",
            "  - LogReg: θ = 0.505\n",
            "  - RF: θ = 0.506\n",
            "Step 3: Evaluating models on test graph prefixes...\n",
            "\n",
            "================== Early Detection Performance ==================\n",
            "          det_rate                  fpr              \n",
            "model          HGB LogReg     RF    HGB LogReg     RF\n",
            "keep_frac                                            \n",
            "0.1000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.2000      1.0000 0.0000 0.0000 1.0000 0.0000 0.0000\n",
            "0.3000      1.0000 0.9870 0.0000 1.0000 0.9870 0.0000\n",
            "0.4000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "0.5000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "0.6000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "0.7000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "0.8000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "0.9000      1.0000 0.9870 0.0000 1.0000 1.0000 0.0000\n",
            "1.0000      1.0000 0.9750 0.9750 1.0000 0.9330 0.9200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — Prefix-augmented training + per-prefix θ calibration (tie-safe)\n",
        "# =====================================================================================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS_EVAL = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]  # eval curve\n",
        "KEEP_FRACS_TRAIN = [0.15,0.30,0.50,0.75,1.00]                           # train augmentation\n",
        "TARGET_SPEC  = 0.95\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "CALIB_FRAC   = 0.30\n",
        "# --------------------------------------------\n",
        "\n",
        "# ---------- Graph discovery ----------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ---------- Prefix builder ----------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time(d):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return np.inf\n",
        "\n",
        "def prefix_by_nodes(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "    node_t = {n: np.inf for n in G_full.nodes()}\n",
        "    for u,v,d in G_full.edges(data=True):\n",
        "        t = _edge_time(d)\n",
        "        if np.isfinite(t):\n",
        "            if t < node_t[u]: node_t[u] = t\n",
        "            if t < node_t[v]: node_t[v] = t\n",
        "    # Deterministic tie-breaker on node id\n",
        "    order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "    k = max(1, int(round(len(order)*max(0.0,min(1.0,keep_frac)))))\n",
        "    return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "# ---------- Prefix-robust features ----------\n",
        "def _extras(H: nx.Graph) -> dict:\n",
        "    n = H.number_of_nodes(); m = H.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(\n",
        "            n_nodes=0, n_edges=0, density=0.0, avg_deg=0.0,\n",
        "            n_components=0, largest_cc=0, largest_cc_frac=0.0,\n",
        "            edges_per_node=0.0, is_directed=0.0\n",
        "        )\n",
        "    degs = [d for _,d in H.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(H)) if isinstance(H,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(H))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n_nodes=n,\n",
        "        n_edges=m,\n",
        "        density=(nx.density(H) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_components=len(comps),\n",
        "        largest_cc=largest,\n",
        "        largest_cc_frac=(largest / n if n>0 else 0.0),\n",
        "        edges_per_node=(m / n if n>0 else 0.0),\n",
        "        is_directed=float(H.is_directed()) if hasattr(H,\"is_directed\") else 0.0\n",
        "    )\n",
        "\n",
        "def feature_row(G, keep_frac):\n",
        "    s = pd.Series(_extras(G), dtype=\"float64\")\n",
        "    s[\"keep_frac\"]=float(keep_frac)\n",
        "    s[\"epn_x_keep\"] = s[\"edges_per_node\"] * s[\"keep_frac\"]\n",
        "    return s\n",
        "\n",
        "# ---------- Models ----------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=500, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ---------- Split (outer) ----------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "\n",
        "# Split TRAIN into fit vs calibration\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=CALIB_FRAC, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Build PREFIX-AUGMENTED TRAIN set ----------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_nodes(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row(H, actual))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows)\n",
        "y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models on augmented TRAIN\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ---------- Tie-safe quantile helper ----------\n",
        "def quantile_higher(arr: np.ndarray, q: float) -> float:\n",
        "    \"\"\"Quantile with 'higher' interpolation to ensure >= q fraction are <= θ.\"\"\"\n",
        "    if arr.size == 0: return 0.5\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    arr.sort()\n",
        "    k = int(np.ceil(q * arr.size)) - 1\n",
        "    k = max(0, min(k, arr.size-1))\n",
        "    return float(arr[k])\n",
        "\n",
        "# ---------- CALIBRATE θ PER PREFIX on TRAIN-CALIB NEGATIVES ----------\n",
        "theta_map = {name:{} for name in MODELS}   # θ[name][keep_frac] = threshold\n",
        "calib_fpr_report = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    neg_scores = {name: [] for name in MODELS}\n",
        "    for i in tr_cal_idx:\n",
        "        if y[i] != 0:  # only negatives\n",
        "            continue\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        s = feature_row(H, actual).reindex(SCHEMA)\n",
        "        X1 = pd.DataFrame([s])\n",
        "        for name, model in MODELS.items():\n",
        "            neg_scores[name].append(model.predict_proba(X1)[:,1][0])\n",
        "    for name in MODELS:\n",
        "        arr = np.array(neg_scores[name], dtype=float)\n",
        "        if arr.size == 0:\n",
        "            # fallback: negatives from augmented fit set (approximate)\n",
        "            arr = MODELS[name].predict_proba(X_fit[y_fit==0])[:,1]\n",
        "        th = quantile_higher(arr, TARGET_SPEC)   # tie-safe\n",
        "        theta_map[name][keep] = th\n",
        "        # Achieved spec/FPR on calibration negatives (ties count as NEGATIVE)\n",
        "        spec = (arr <= th).mean() if arr.size else np.nan\n",
        "        calib_fpr_report[name][keep] = float(1.0 - spec)\n",
        "\n",
        "print(\"[CALIB] Approx FPR on TRAIN-CALIB negatives at θ(k):\")\n",
        "for name in MODELS:\n",
        "    demo = {k: round(calib_fpr_report[name][k],3) for k in list(calib_fpr_report[name].keys())[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ---------- Evaluate on TEST PREFIXES with θ(keep_frac) ----------\n",
        "records=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    rows=[]; y_test=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        rows.append(feature_row(H, actual).reindex(SCHEMA))\n",
        "        y_test.append(y[i])\n",
        "    if not rows:\n",
        "        continue\n",
        "    X_test = pd.DataFrame(rows); y_test=np.array(y_test)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(X_test)[:,1]\n",
        "        th = theta_map[name][keep]\n",
        "        yhat = (p > th).astype(int)                 # tie → NEGATIVE\n",
        "        is_attack = (y_test==1); is_normal=(y_test==0)\n",
        "        det = (yhat[is_attack]==1).mean() if is_attack.any() else np.nan\n",
        "        fpr = (yhat[is_normal]==1).mean() if is_normal.any() else np.nan\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr))\n",
        "\n",
        "res = pd.DataFrame(records).pivot_table(index=\"keep_frac\", columns=\"model\", values=[\"det_rate\",\"fpr\"])\n",
        "print(\"\\n==== Early Detection (DetRate / FPR vs keep_frac) — Train-Augmented + Tie-safe θ ====\")\n",
        "with pd.option_context('display.max_rows', 200, 'display.width', 160):\n",
        "    print(res.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3-nIlcvlb4c",
        "outputId": "9817f45f-c310-47f4-bd6f-351613675c7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 770 graphs from merged(y). Class balance: [373 397]\n",
            "[TRAIN] Fit with 2155 prefix samples (from 431 graphs). Features: 11\n",
            "[CALIB] Approx FPR on TRAIN-CALIB negatives at θ(k):\n",
            "  HGB: {0.1: 0.0, 0.2: 0.0, 0.3: 0.0, 0.4: 0.0} ...\n",
            "  LogReg: {0.1: 0.0, 0.2: 0.0, 0.3: 0.0, 0.4: 0.0} ...\n",
            "  RF: {0.1: 0.0, 0.2: 0.022, 0.3: 0.034, 0.4: 0.011} ...\n",
            "\n",
            "==== Early Detection (DetRate / FPR vs keep_frac) — Train-Augmented + Tie-safe θ ====\n",
            "          det_rate                  fpr              \n",
            "model          HGB LogReg     RF    HGB LogReg     RF\n",
            "keep_frac                                            \n",
            "0.1000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.2000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.3000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.4000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.5000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.6000      0.0130 0.0000 0.0130 0.0800 0.0130 0.0800\n",
            "0.7000      0.0000 0.0000 0.0000 0.0000 0.0130 0.0000\n",
            "0.8000      0.0130 0.0000 0.0000 0.0670 0.0130 0.0000\n",
            "0.9000      0.0130 0.0000 0.0000 0.0800 0.0000 0.0000\n",
            "1.0000      0.0000 0.0000 0.0000 0.0130 0.0000 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION (robust): prefix-augmented training + per-prefix tie-safe θ,\n",
        "# with reliable prefixing (timestamp fallback) and relative-to-full features.\n",
        "# =====================================================================================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS_EVAL  = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]  # eval curve\n",
        "KEEP_FRACS_TRAIN = [0.15,0.30,0.50,0.75,1.00]                           # train augmentation\n",
        "TARGET_SPEC  = 0.95\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "CALIB_FRAC   = 0.30\n",
        "# --------------------------------------------\n",
        "\n",
        "# ---------- Graph discovery ----------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ---------- Prefix builder with timestamp *fallback* ----------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    # Use provided timestamp if parseable; else fall back to edge insertion order index\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)  # fallback\n",
        "\n",
        "def prefix_by_nodes(G_full: nx.Graph, keep_frac: float):\n",
        "    # Convert to MultiDiGraph for uniform handling\n",
        "    if not isinstance(G_full, nx.MultiDiGraph):\n",
        "        G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "\n",
        "    # Build edge-ordered times (timestamp or insertion index)\n",
        "    node_t = {n: np.inf for n in G_full.nodes()}\n",
        "    for idx, (u,v,d) in enumerate(G_full.edges(data=True)):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if t < node_t[u]: node_t[u] = t\n",
        "        if t < node_t[v]: node_t[v] = t\n",
        "\n",
        "    # Deterministic tie-breaker on node id\n",
        "    order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "    k = max(1, int(round(len(order)*max(0.0,min(1.0,keep_frac)))))\n",
        "    return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "# ---------- Full-graph stats + prefix features (absolute + relative) ----------\n",
        "def _stats_basic(G: nx.Graph):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0)\n",
        "    )\n",
        "\n",
        "def feature_row_prefix_vs_full(H: nx.Graph, keep_frac: float, S_full: dict) -> pd.Series:\n",
        "    S_p = _stats_basic(H)\n",
        "    # Absolute\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], is_directed=float(H.is_directed()) if hasattr(H,\"is_directed\") else 0.0,\n",
        "        keep_frac=float(keep_frac),\n",
        "    )\n",
        "    # Relative to full (guard divide-by-zero)\n",
        "    def _safe_div(a,b): return float(a)/float(b) if (b is not None and b!=0) else 0.0\n",
        "    out.update({\n",
        "        \"frac_nodes_of_full\": _safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        \"frac_edges_of_full\": _safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        \"density_rel\":        _safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        \"epn_rel\":            _safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        \"largest_cc_rel\":     _safe_div(S_p[\"largest\"], S_full[\"largest\"]),\n",
        "        \"largest_cc_frac_rel\":_safe_div(S_p[\"largest_frac\"], S_full[\"largest_frac\"]) if S_full[\"largest_frac\"]>0 else 0.0,\n",
        "        \"epn_x_keep\":         S_p[\"epn\"]*keep_frac,\n",
        "    })\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---------- Models ----------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=600, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ---------- Split ----------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=CALIB_FRAC, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Precompute full-graph stats (once) ----------\n",
        "full_stats = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    Gfull = _loader(paths[i])\n",
        "    if not isinstance(Gfull, nx.MultiDiGraph):\n",
        "        Gfull = nx.MultiDiGraph(Gfull)\n",
        "    full_stats[i] = _stats_basic(Gfull)\n",
        "\n",
        "# ---------- Build PREFIX-AUGMENTED TRAIN set (absolute + relative features) ----------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full = full_stats[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_nodes(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full(H, actual, S_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ---------- Tie-safe quantile ----------\n",
        "def quantile_higher(arr: np.ndarray, q: float) -> float:\n",
        "    if arr.size == 0: return 0.5\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    arr.sort()\n",
        "    k = int(np.ceil(q * arr.size)) - 1\n",
        "    k = max(0, min(k, arr.size-1))\n",
        "    return float(arr[k])\n",
        "\n",
        "# ---------- Calibrate θ(k) on TRAIN-CALIB negatives ----------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "calib_auc  = {name:{} for name in MODELS}\n",
        "calib_fpr  = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    scores_neg = {name: [] for name in MODELS}\n",
        "    scores_all = {name: [] for name in MODELS}\n",
        "    labels_all = []\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        x = feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA)\n",
        "        X1 = pd.DataFrame([x])\n",
        "        for name, model in MODELS.items():\n",
        "            p = model.predict_proba(X1)[:,1][0]\n",
        "            scores_all[name].append(p)\n",
        "            if y[i]==0: scores_neg[name].append(p)\n",
        "        labels_all.append(y[i])\n",
        "    labels_all = np.array(labels_all)\n",
        "\n",
        "    for name in MODELS:\n",
        "        arr_neg = np.array(scores_neg[name], float)\n",
        "        th = quantile_higher(arr_neg, TARGET_SPEC) if arr_neg.size else 0.5\n",
        "        theta_map[name][keep] = th\n",
        "        spec = (arr_neg <= th).mean() if arr_neg.size else np.nan\n",
        "        calib_fpr[name][keep] = float(1.0 - spec)\n",
        "        # AUC on calib (sanity)\n",
        "        arr_all = np.array(scores_all[name], float)\n",
        "        try:\n",
        "            calib_auc[name][keep] = float(roc_auc_score(labels_all, arr_all))\n",
        "        except Exception:\n",
        "            calib_auc[name][keep] = np.nan\n",
        "\n",
        "print(\"[CALIB] FPR on TRAIN-CALIB negatives @ θ(k) and AUC on calib:\")\n",
        "for name in MODELS:\n",
        "    demo_fpr = {k: round(calib_fpr[name][k],3) for k in list(calib_fpr[name].keys())[:4]}\n",
        "    demo_auc = {k: round(calib_auc[name][k],3) for k in list(calib_auc[name].keys())[:4]}\n",
        "    print(f\"  {name}: FPR {demo_fpr} | AUC {demo_auc} ...\")\n",
        "\n",
        "# ---------- Evaluate on TEST prefixes ----------\n",
        "records=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    rows=[]; y_test=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        rows.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "        y_test.append(y[i])\n",
        "    if not rows: continue\n",
        "    X_test = pd.DataFrame(rows); y_test = np.array(y_test)\n",
        "\n",
        "    # quick AUC (threshold-free) to verify separability per keep\n",
        "    aucs={}\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(X_test)[:,1]\n",
        "        try:\n",
        "            aucs[name] = float(roc_auc_score(y_test, p))\n",
        "        except Exception:\n",
        "            aucs[name] = np.nan\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(X_test)[:,1]\n",
        "        th = theta_map[name][keep]\n",
        "        yhat = (p > th).astype(int)   # tie → NEG\n",
        "        is_attack = (y_test==1); is_normal=(y_test==0)\n",
        "        det = (yhat[is_attack]==1).mean() if is_attack.any() else np.nan\n",
        "        fpr = (yhat[is_normal]==1).mean() if is_normal.any() else np.nan\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=aucs[name]))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "pivot_det = res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\")\n",
        "pivot_fpr = res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\")\n",
        "pivot_auc = res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\")\n",
        "print(\"\\n==== Early Detection — DetRate vs keep_frac ====\")\n",
        "print(pivot_det.round(3))\n",
        "print(\"\\n==== Early Detection — FPR vs keep_frac ====\")\n",
        "print(pivot_fpr.round(3))\n",
        "print(\"\\n==== Early Detection — AUC (threshold-free) vs keep_frac ====\")\n",
        "print(pivot_auc.round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeAuBuHQqRQv",
        "outputId": "c0cda787-9f09-40d4-963d-2d63546cae7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 770 graphs from merged(y). Class balance: [373 397]\n",
            "[TRAIN] Fit with 2155 prefix samples (from 431 graphs). Features: 17\n",
            "[CALIB] FPR on TRAIN-CALIB negatives @ θ(k) and AUC on calib:\n",
            "  HGB: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.0, 0.4: 0.0} | AUC {0.1: 0.506, 0.2: 0.506, 0.3: 0.517, 0.4: 0.517} ...\n",
            "  LogReg: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.011, 0.4: 0.011} | AUC {0.1: 0.517, 0.2: 0.517, 0.3: 0.511, 0.4: 0.511} ...\n",
            "  RF: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.011, 0.4: 0.011} | AUC {0.1: 0.517, 0.2: 0.465, 0.3: 0.561, 0.4: 0.52} ...\n",
            "\n",
            "==== Early Detection — DetRate vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.9750 0.0000\n",
            "0.2000    0.0000  0.9750 0.0000\n",
            "0.3000    0.0000  0.0000 0.0000\n",
            "0.4000    0.0000  0.0000 0.0000\n",
            "0.5000    0.0000  0.0000 0.9750\n",
            "0.6000    0.0130  0.0000 0.0000\n",
            "0.7000    0.0000  0.0000 0.0000\n",
            "0.8000    0.0130  0.0000 0.0000\n",
            "0.9000    0.0130  0.0000 0.0000\n",
            "1.0000    0.0000  0.0000 0.0000\n",
            "\n",
            "==== Early Detection — FPR vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.9330 0.0000\n",
            "0.2000    0.0000  0.9330 0.0000\n",
            "0.3000    0.0000  0.0130 0.0000\n",
            "0.4000    0.0000  0.0130 0.0000\n",
            "0.5000    0.0000  0.0130 0.9200\n",
            "0.6000    0.0670  0.0130 0.0130\n",
            "0.7000    0.0000  0.0130 0.0000\n",
            "0.8000    0.0670  0.0130 0.0000\n",
            "0.9000    0.0800  0.0000 0.0000\n",
            "1.0000    0.0130  0.0000 0.0000\n",
            "\n",
            "==== Early Detection — AUC (threshold-free) vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.4940  0.5140 0.5200\n",
            "0.2000    0.4940  0.5140 0.5200\n",
            "0.3000    0.5270  0.5140 0.5270\n",
            "0.4000    0.5270  0.5140 0.5270\n",
            "0.5000    0.5270  0.5140 0.5270\n",
            "0.6000    0.4740  0.5140 0.5140\n",
            "0.7000    0.5270  0.5140 0.5270\n",
            "0.8000    0.4740  0.5140 0.5270\n",
            "0.9000    0.4610  0.5270 0.5270\n",
            "1.0000    0.5140  0.5270 0.5270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4CI8rjduGzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Q4kjIiEuG5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po1GuZXfuHBU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XWV2Bv56uHJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e4245hfkuHO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — prefix-augmented training + per-prefix tie-safe θ\n",
        "# with richer, prefix-sensitive features (shape + growth), timestamp fallback\n",
        "# =====================================================================================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS_EVAL  = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]   # eval curve\n",
        "KEEP_FRACS_TRAIN = [0.12,0.25,0.40,0.65,1.00]                            # train augmentation (diverse)\n",
        "TARGET_SPEC  = 0.95   # target specificity on NEGATIVES (so target FPR ≈ 0.05)\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "CALIB_FRAC   = 0.30\n",
        "# --------------------------------------------\n",
        "\n",
        "# ---------- Graph discovery ----------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ---------- Prefix builder (timestamp fallback) ----------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)  # insertion index fallback\n",
        "\n",
        "def prefix_by_nodes(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "    node_t = {n: np.inf for n in G_full.nodes()}\n",
        "    for idx, (u,v,d) in enumerate(G_full.edges(data=True)):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if t < node_t[u]: node_t[u] = t\n",
        "        if t < node_t[v]: node_t[v] = t\n",
        "    order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "    k = max(1, int(round(len(order)*max(0.0,min(1.0,keep_frac)))))\n",
        "    return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "# ---------- Base stats ----------\n",
        "def _basic_stats(G: nx.Graph):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "    )\n",
        "\n",
        "# ---------- EXTRA shape/growth features ----------\n",
        "def _safe_frac(num, den):\n",
        "    return float(num)/float(den) if den and den!=0 else 0.0\n",
        "\n",
        "def _shape_features(G: nx.Graph):\n",
        "    n = G.number_of_nodes()\n",
        "    if n==0:\n",
        "        return dict(frac_deg1=0.0, frac_deg_ge3=0.0, avg_clust=0.0, trans=0.0, reciprocity=0.0, assort=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    frac_deg1 = _safe_frac(sum(1 for d in degs if d==1), n)\n",
        "    frac_deg_ge3 = _safe_frac(sum(1 for d in degs if d>=3), n)\n",
        "    # clustering (on simple undirected view)\n",
        "    try:\n",
        "        Gu = G.to_undirected() if hasattr(G, \"to_undirected\") else nx.Graph(G)\n",
        "        Gu_simple = nx.Graph(Gu)  # remove multiedges\n",
        "        avg_clust = nx.average_clustering(Gu_simple) if Gu_simple.number_of_nodes() > 1 else 0.0\n",
        "        trans = nx.transitivity(Gu_simple) if Gu_simple.number_of_nodes() > 2 else 0.0\n",
        "    except Exception:\n",
        "        avg_clust, trans = 0.0, 0.0\n",
        "    # reciprocity for directed\n",
        "    try:\n",
        "        reciprocity = nx.reciprocity(G) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else 0.0\n",
        "        if reciprocity is None: reciprocity = 0.0\n",
        "    except Exception:\n",
        "        reciprocity = 0.0\n",
        "    # assortativity (guarded)\n",
        "    try:\n",
        "        assort = nx.degree_assortativity_coefficient(Gu_simple) if Gu_simple.number_of_nodes()>3 else 0.0\n",
        "        if not np.isfinite(assort): assort = 0.0\n",
        "    except Exception:\n",
        "        assort = 0.0\n",
        "    return dict(frac_deg1=frac_deg1, frac_deg_ge3=frac_deg_ge3, avg_clust=avg_clust, trans=trans, reciprocity=float(reciprocity), assort=float(assort))\n",
        "\n",
        "def feature_row_prefix_vs_full(H: nx.Graph, keep_frac: float, S_full: dict) -> pd.Series:\n",
        "    S_p  = _basic_stats(H)\n",
        "    shp  = _shape_features(H)\n",
        "    # Absolute\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], is_directed=float(H.is_directed()) if hasattr(H,\"is_directed\") else 0.0,\n",
        "        keep_frac=float(keep_frac),\n",
        "        **shp\n",
        "    )\n",
        "    # Relative to full\n",
        "    out.update({\n",
        "        \"frac_nodes_of_full\": _safe_frac(S_p[\"n\"], S_full[\"n\"]),\n",
        "        \"frac_edges_of_full\": _safe_frac(S_p[\"m\"], S_full[\"m\"]),\n",
        "        \"density_rel\":        _safe_frac(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        \"epn_rel\":            _safe_frac(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        \"largest_cc_rel\":     _safe_frac(S_p[\"largest\"], S_full[\"largest\"]),\n",
        "        \"largest_cc_frac_rel\":_safe_frac(S_p[\"largest_frac\"], S_full[\"largest_frac\"]) if S_full[\"largest_frac\"]>0 else 0.0,\n",
        "        # “aheadness”: how much growth outruns time\n",
        "        \"nodes_ahead\":        _safe_frac(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        \"edges_ahead\":        _safe_frac(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        \"epn_x_keep\":         S_p[\"epn\"]*keep_frac,\n",
        "    })\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---------- Models ----------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=700, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ---------- Split ----------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=CALIB_FRAC, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Precompute full-graph stats ----------\n",
        "def _basic_full(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    return _basic_stats(G)\n",
        "\n",
        "full_stats = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_stats[i] = _basic_full(_loader(paths[i]))\n",
        "\n",
        "# ---------- Build PREFIX-AUGMENTED TRAIN set ----------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full = full_stats[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_nodes(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full(H, actual, S_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ---------- Tie-safe quantile ----------\n",
        "def quantile_higher(arr: np.ndarray, q: float) -> float:\n",
        "    if arr.size == 0: return 0.5\n",
        "    arr = np.asarray(arr, dtype=float)\n",
        "    arr.sort()\n",
        "    k = int(np.ceil(q * arr.size)) - 1\n",
        "    k = max(0, min(k, arr.size-1))\n",
        "    return float(arr[k])\n",
        "\n",
        "# ---------- Calibrate θ(k) on TRAIN-CALIB negatives ----------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "calib_auc  = {name:{} for name in MODELS}\n",
        "calib_fpr  = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    scores_neg = {name: [] for name in MODELS}\n",
        "    scores_all = {name: [] for name in MODELS}\n",
        "    labels_all = []\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        x = feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA)\n",
        "        X1 = pd.DataFrame([x])\n",
        "        for name, model in MODELS.items():\n",
        "            p = model.predict_proba(X1)[:,1][0]\n",
        "            scores_all[name].append(p)\n",
        "            if y[i]==0: scores_neg[name].append(p)\n",
        "        labels_all.append(y[i])\n",
        "    labels_all = np.array(labels_all)\n",
        "\n",
        "    for name in MODELS:\n",
        "        arr_neg = np.array(scores_neg[name], float)\n",
        "        th = quantile_higher(arr_neg, TARGET_SPEC) if arr_neg.size else 0.5\n",
        "        theta_map[name][keep] = th\n",
        "        spec = (arr_neg <= th).mean() if arr_neg.size else np.nan\n",
        "        calib_fpr[name][keep] = float(1.0 - spec)\n",
        "        arr_all = np.array(scores_all[name], float)\n",
        "        try:\n",
        "            calib_auc[name][keep] = float(roc_auc_score(labels_all, arr_all))\n",
        "        except Exception:\n",
        "            calib_auc[name][keep] = np.nan\n",
        "\n",
        "print(\"[CALIB] FPR on TRAIN-CALIB negatives @ θ(k) and AUC on calib:\")\n",
        "for name in MODELS:\n",
        "    demo_fpr = {k: round(calib_fpr[name][k],3) for k in list(calib_fpr[name].keys())[:4]}\n",
        "    demo_auc = {k: round(calib_auc[name][k],3) for k in list(calib_auc[name].keys())[:4]}\n",
        "    print(f\"  {name}: FPR {demo_fpr} | AUC {demo_auc} ...\")\n",
        "\n",
        "# ---------- Evaluate on TEST prefixes ----------\n",
        "records=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    rows=[]; y_test=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        rows.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "        y_test.append(y[i])\n",
        "    if not rows: continue\n",
        "    X_test = pd.DataFrame(rows); y_test = np.array(y_test)\n",
        "\n",
        "    aucs={}\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(X_test)[:,1]\n",
        "        try:\n",
        "            aucs[name] = float(roc_auc_score(y_test, p))\n",
        "        except Exception:\n",
        "            aucs[name] = np.nan\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(X_test)[:,1]\n",
        "        th = theta_map[name][keep]\n",
        "        yhat = (p > th).astype(int)   # tie → NEG\n",
        "        is_attack = (y_test==1); is_normal=(y_test==0)\n",
        "        det = (yhat[is_attack]==1).mean() if is_attack.any() else np.nan\n",
        "        fpr = (yhat[is_normal]==1).mean() if is_normal.any() else np.nan\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=aucs[name]))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate vs keep_frac ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR vs keep_frac ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (threshold-free) vs keep_frac ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUHwbpoRrh3k",
        "outputId": "5f8721ae-c78f-42fc-f4d4-84b27ccc397b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 770 graphs from merged(y). Class balance: [373 397]\n",
            "[TRAIN] Fit with 2155 prefix samples (from 431 graphs). Features: 25\n",
            "[CALIB] FPR on TRAIN-CALIB negatives @ θ(k) and AUC on calib:\n",
            "  HGB: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.0, 0.4: 0.0} | AUC {0.1: 0.517, 0.2: 0.517, 0.3: 0.522, 0.4: 0.522} ...\n",
            "  LogReg: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.0, 0.4: 0.0} | AUC {0.1: 0.517, 0.2: 0.517, 0.3: 0.522, 0.4: 0.522} ...\n",
            "  RF: FPR {0.1: 0.0, 0.2: 0.0, 0.3: 0.045, 0.4: 0.011} | AUC {0.1: 0.489, 0.2: 0.449, 0.3: 0.53, 0.4: 0.519} ...\n",
            "\n",
            "==== Early Detection — DetRate vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.0000 0.0000\n",
            "0.2000    0.0000  0.0000 0.0000\n",
            "0.3000    0.0000  0.0000 0.0000\n",
            "0.4000    0.0000  0.0000 0.0000\n",
            "0.5000    0.0000  0.0000 0.0000\n",
            "0.6000    0.0130  0.0000 0.0000\n",
            "0.7000    0.0130  0.0000 0.0000\n",
            "0.8000    0.0130  0.0000 0.9750\n",
            "0.9000    0.0130  0.0000 0.9750\n",
            "1.0000    0.0000  0.0000 0.0000\n",
            "\n",
            "==== Early Detection — FPR vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.0130 0.0000\n",
            "0.2000    0.0000  0.0130 0.0000\n",
            "0.3000    0.0000  0.0130 0.0000\n",
            "0.4000    0.0000  0.0130 0.0000\n",
            "0.5000    0.0000  0.0130 0.0000\n",
            "0.6000    0.0670  0.0130 0.0000\n",
            "0.7000    0.0800  0.0130 0.0130\n",
            "0.8000    0.0800  0.0130 0.9330\n",
            "0.9000    0.0800  0.0130 0.9200\n",
            "1.0000    0.0130  0.0130 0.0000\n",
            "\n",
            "==== Early Detection — AUC (threshold-free) vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.5270  0.5140 0.5270\n",
            "0.2000    0.5270  0.5140 0.5270\n",
            "0.3000    0.5270  0.5140 0.5270\n",
            "0.4000    0.5270  0.5140 0.5270\n",
            "0.5000    0.5270  0.5140 0.5270\n",
            "0.6000    0.4740  0.5140 0.5270\n",
            "0.7000    0.4610  0.5140 0.5140\n",
            "0.8000    0.4610  0.5140 0.5140\n",
            "0.9000    0.4610  0.5140 0.5270\n",
            "1.0000    0.5140  0.5140 0.5270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== DROP-IN: robust calibration (auto-orient + ROC threshold) and test eval =====\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "ALPHA = 1.0 - TARGET_SPEC   # target FPR cap\n",
        "\n",
        "def _safe_auc(y_true, s):\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true, s))\n",
        "    except Exception:\n",
        "        return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Choose threshold maximizing TPR subject to FPR <= alpha.\n",
        "    Ties broken by lower FPR; if none satisfy, pick threshold with smallest FPR.\n",
        "    Returns (th, fpr, tpr, auc, flipped)\n",
        "    \"\"\"\n",
        "    y_true = np.asarray(y_true, int)\n",
        "    s = np.asarray(s, float)\n",
        "    # ROC uses descending thresholds; we’ll evaluate all candidates\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    # Keep candidates with FPR <= alpha (plus tiny epsilon for float stability)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])                  # max TPR under cap\n",
        "        # break ties by lower FPR\n",
        "        best_idxs = np.flatnonzero((tpr[ok] == tpr[ok][idx]))\n",
        "        pick = best_idxs[np.argmin(fpr[ok][best_idxs])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        # fallback: pick minimal FPR (then max TPR among ties)\n",
        "        minf = np.min(fpr)\n",
        "        cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "# --------- CALIBRATION on TRAIN-CALIB (per keep, per model) ----------\n",
        "theta_map = {name:{} for name in MODELS}    # threshold per keep\n",
        "flip_map  = {name:{} for name in MODELS}    # whether we flipped scores\n",
        "calib_rep = {name:{} for name in MODELS}    # dict of dicts with AUC/FPR/TPR\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    # collect calibration features/scores\n",
        "    xs=[]; ys=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "        ys.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(ys)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        raw = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, raw)\n",
        "        flipped = False\n",
        "        s = raw.copy()\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s\n",
        "            flipped = True\n",
        "            auc = 1.0 - auc  # symmetric\n",
        "\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(\"[CALIB] per keep (AUC, FPR@θ<=α, TPR@θ<=α) — after auto-orientation:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# --------- TEST EVALUATION (per keep, per model) ----------\n",
        "records=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]:\n",
        "            p = 1.0 - p\n",
        "        th = theta_map[name][keep]\n",
        "        yhat = (p > th).astype(int)  # tie -> negative\n",
        "        is_pos = (yt==1); is_neg=(yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, p)\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate vs keep_frac (ROC-optimized @ FPR≤α) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR vs keep_frac (should be ≤ α≈{:.2f}) ====\".format(ALPHA))\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (after auto-orientation) vs keep_frac ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcMZMr2AssHk",
        "outputId": "647cb327-8d7e-48de-a7d4-3681bea6b6ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CALIB] per keep (AUC, FPR@θ<=α, TPR@θ<=α) — after auto-orientation:\n",
            "  HGB: {0.1: (0.517, 0.0, 0.0), 0.2: (0.517, 0.0, 0.0), 0.3: (0.522, 0.0, 0.0), 0.4: (0.522, 0.0, 0.0)} ...\n",
            "  LogReg: {0.1: (0.517, 0.0, 0.0), 0.2: (0.517, 0.0, 0.0), 0.3: (0.522, 0.0, 0.0), 0.4: (0.522, 0.0, 0.0)} ...\n",
            "  RF: {0.1: (0.517, 0.0, 0.0), 0.2: (0.517, 0.0, 0.0), 0.3: (0.522, 0.0, 0.0), 0.4: (0.522, 0.0, 0.0)} ...\n",
            "\n",
            "==== Early Detection — DetRate vs keep_frac (ROC-optimized @ FPR≤α) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.0000 0.0000\n",
            "0.2000    0.0000  0.0000 0.0000\n",
            "0.3000    0.0000  0.0000 0.0000\n",
            "0.4000    0.0000  0.0000 0.0000\n",
            "0.5000    0.0000  0.0000 0.0000\n",
            "0.6000    0.0000  0.0000 0.0000\n",
            "0.7000    0.0000  0.0000 0.0000\n",
            "0.8000    0.0000  0.0000 0.0000\n",
            "0.9000    0.0000  0.0000 0.0000\n",
            "1.0000    0.0000  0.0000 0.0000\n",
            "\n",
            "==== Early Detection — FPR vs keep_frac (should be ≤ α≈0.05) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0000  0.0000 0.0000\n",
            "0.2000    0.0000  0.0000 0.0000\n",
            "0.3000    0.0000  0.0000 0.0000\n",
            "0.4000    0.0000  0.0000 0.0000\n",
            "0.5000    0.0000  0.0000 0.0000\n",
            "0.6000    0.0000  0.0000 0.0000\n",
            "0.7000    0.0000  0.0000 0.0000\n",
            "0.8000    0.0000  0.0000 0.0000\n",
            "0.9000    0.0000  0.0000 0.0000\n",
            "1.0000    0.0000  0.0000 0.0000\n",
            "\n",
            "==== Early Detection — AUC (after auto-orientation) vs keep_frac ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.5270  0.5140 0.5270\n",
            "0.2000    0.5270  0.5140 0.5270\n",
            "0.3000    0.5270  0.5140 0.5270\n",
            "0.4000    0.5270  0.5140 0.5270\n",
            "0.5000    0.5270  0.5140 0.5270\n",
            "0.6000    0.5260  0.5140 0.5270\n",
            "0.7000    0.5390  0.5140 0.5140\n",
            "0.8000    0.5390  0.5140 0.5140\n",
            "0.9000    0.5390  0.5140 0.5270\n",
            "1.0000    0.5140  0.5140 0.5270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# EARLY-DETECTION TRIAGE CELL\n",
        "# ============================\n",
        "import numpy as np, pandas as pd, warnings\n",
        "from sklearn.metrics import roc_auc_score\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "def _safe_auc(y_true, s):\n",
        "    try:\n",
        "        return float(roc_auc_score(y_true, s))\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "# 0) quick label↔file sanity\n",
        "print(\"[SANITY] y length vs paths:\", len(y), len(paths))\n",
        "u, c = np.unique(y, return_counts=True)\n",
        "print(\"[SANITY] class balance:\", dict(zip(u, c)))\n",
        "\n",
        "# 1) timestamp coverage across a sample\n",
        "def _edge_has_ts(d):\n",
        "    for k in (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\"):\n",
        "        if k in d and d[k] is not None:\n",
        "            try:\n",
        "                float(d[k]); return True\n",
        "            except: pass\n",
        "    return False\n",
        "\n",
        "rng = np.random.default_rng(42)\n",
        "sample_ids = rng.choice(len(paths), size=min(100, len(paths)), replace=False)\n",
        "cov = []\n",
        "for i in sample_ids:\n",
        "    G = _loader(paths[i])\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0:\n",
        "        cov.append(np.nan)\n",
        "    else:\n",
        "        hits = 0\n",
        "        for _,_,d in G.edges(data=True):\n",
        "            if _edge_has_ts(d): hits += 1\n",
        "        cov.append(hits / m)\n",
        "cov = np.array([x for x in cov if np.isfinite(x)])\n",
        "print(f\"[TS-COVERAGE] mean={cov.mean():.3f}, median={np.median(cov):.3f}, pct>=0.5={(cov>=0.5).mean():.3f}, n={len(cov)}\")\n",
        "\n",
        "# 2) build one table of features for TEST graphs at each keep\n",
        "#    (re-use te_idx from earlier split)\n",
        "perkeep_feature_auc = {k: {} for k in KEEP_FRACS_EVAL}\n",
        "perkeep_perm_auc = {}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    rows = []; labels = []\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "        H, actual = prefix_by_nodes(Gfull, keep)\n",
        "        rows.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "        labels.append(y[i])\n",
        "    X = pd.DataFrame(rows).astype(float)\n",
        "    Y = np.array(labels, dtype=int)\n",
        "\n",
        "    # univariate AUC for each feature\n",
        "    aucs = {}\n",
        "    for col in X.columns:\n",
        "        if X[col].nunique() <= 1:\n",
        "            aucs[col] = np.nan\n",
        "            continue\n",
        "        aucs[col] = _safe_auc(Y, X[col].values)\n",
        "    perkeep_feature_auc[keep] = aucs\n",
        "\n",
        "    # permutation test: model-free benchmark\n",
        "    Y_perm = Y.copy()\n",
        "    rng.shuffle(Y_perm)\n",
        "    perkeep_perm_auc[keep] = _safe_auc(Y_perm, X.mean(axis=1).values)  # any score proxy; mean suffices for check\n",
        "\n",
        "# 3) print top features by AUC for a few keeps\n",
        "def _topk(d, k=8):\n",
        "    items = [(f, v) for f, v in d.items() if np.isfinite(v)]\n",
        "    items.sort(key=lambda x: abs(x[1]-0.5), reverse=True)\n",
        "    return items[:k]\n",
        "\n",
        "for keep in [0.2, 0.4, 0.6, 0.8, 1.0]:\n",
        "    if keep not in perkeep_feature_auc: continue\n",
        "    top = _topk(perkeep_feature_auc[keep], 8)\n",
        "    print(f\"\\n[UNIV AUC @ keep={keep:.1f}] (feature → AUC)\")\n",
        "    for f, v in top:\n",
        "        print(f\"  {f:24s}  {v:.3f}\")\n",
        "    print(f\"[PERM AUC baseline] {perkeep_perm_auc.get(keep):.3f}\")\n",
        "\n",
        "# 4) optional: score histograms for a couple keeps using one trained model\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    for keep in [0.3, 0.7]:\n",
        "        rows=[]; Y=[]\n",
        "        for i in te_idx:\n",
        "            Gfull = _loader(paths[i]); S_full = full_stats[i]\n",
        "            H, actual = prefix_by_nodes(Gfull, keep)\n",
        "            rows.append(feature_row_prefix_vs_full(H, actual, S_full).reindex(SCHEMA))\n",
        "            Y.append(y[i])\n",
        "        X = pd.DataFrame(rows).astype(float)\n",
        "        # pick a model (HGB) for visualization\n",
        "        s = MODELS[\"HGB\"].predict_proba(X)[:,1]\n",
        "        plt.figure()\n",
        "        plt.hist(s[np.array(Y)==0], bins=30, alpha=0.6, label=\"benign\")\n",
        "        plt.hist(s[np.array(Y)==1], bins=30, alpha=0.6, label=\"attack\")\n",
        "        plt.title(f\"HGB score distribution @ keep={keep}\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "except Exception as e:\n",
        "    print(\"[WARN] plotting failed:\", e)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JTHEq2TksslV",
        "outputId": "2f04c2f5-557f-405c-b910-308d3ce1c2af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SANITY] y length vs paths: 770 770\n",
            "[SANITY] class balance: {np.int64(0): np.int64(373), np.int64(1): np.int64(397)}\n",
            "[TS-COVERAGE] mean=1.000, median=1.000, pct>=0.5=1.000, n=100\n",
            "\n",
            "[UNIV AUC @ keep=0.2] (feature → AUC)\n",
            "  largest_cc_rel            0.526\n",
            "  keep_frac                 0.515\n",
            "  frac_nodes_of_full        0.515\n",
            "  edges_ahead               0.486\n",
            "  n_nodes                   0.506\n",
            "  n_edges                   0.506\n",
            "  density                   0.506\n",
            "  avg_deg                   0.506\n",
            "[PERM AUC baseline] 0.514\n",
            "\n",
            "[UNIV AUC @ keep=0.4] (feature → AUC)\n",
            "  density_rel               0.540\n",
            "  epn_rel                   0.539\n",
            "  frac_edges_of_full        0.526\n",
            "  largest_cc_rel            0.526\n",
            "  epn_x_keep                0.526\n",
            "  keep_frac                 0.515\n",
            "  frac_nodes_of_full        0.515\n",
            "  edges_ahead               0.486\n",
            "[PERM AUC baseline] 0.488\n",
            "\n",
            "[UNIV AUC @ keep=0.6] (feature → AUC)\n",
            "  density_rel               0.540\n",
            "  avg_deg                   0.485\n",
            "  edges_per_node            0.485\n",
            "  keep_frac                 0.485\n",
            "  frac_nodes_of_full        0.485\n",
            "  epn_x_keep                0.485\n",
            "  n_nodes                   0.486\n",
            "  n_edges                   0.486\n",
            "[PERM AUC baseline] 0.512\n",
            "\n",
            "[UNIV AUC @ keep=0.8] (feature → AUC)\n",
            "  density                   0.460\n",
            "  avg_clust                 0.460\n",
            "  trans                     0.460\n",
            "  assort                    0.473\n",
            "  avg_deg                   0.485\n",
            "  edges_per_node            0.485\n",
            "  keep_frac                 0.485\n",
            "  frac_nodes_of_full        0.485\n",
            "[PERM AUC baseline] 0.486\n",
            "\n",
            "[UNIV AUC @ keep=1.0] (feature → AUC)\n",
            "  density                   0.460\n",
            "  avg_clust                 0.472\n",
            "  trans                     0.472\n",
            "  avg_deg                   0.472\n",
            "  edges_per_node            0.472\n",
            "  epn_x_keep                0.472\n",
            "  assort                    0.526\n",
            "  largest_cc                0.486\n",
            "[PERM AUC baseline] 0.512\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — edge-based prefixes + role/centralization + burstiness features\n",
        "# Prefix-augmented training, ROC-based θ per keep (FPR≤α), score auto-orientation\n",
        "# =====================================================================================\n",
        "import os, glob, warnings, math\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS_EVAL  = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]\n",
        "KEEP_FRACS_TRAIN = [0.12,0.25,0.40,0.65,1.00]\n",
        "TARGET_SPEC  = 0.95     # α = 1 - TARGET_SPEC (FPR cap)\n",
        "ALPHA        = 1.0 - TARGET_SPEC\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "CALIB_FRAC   = 0.30\n",
        "TOPK         = 5        # for role concentration features\n",
        "# --------------------------------------------\n",
        "\n",
        "# ---------- Discovery (use your merged/y if present) ----------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ---------- Edge-time with fallback ----------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)  # fallback: insertion order\n",
        "\n",
        "# ---------- EDGE-BASED prefix (earliest edges -> induced nodes) ----------\n",
        "def prefix_by_edges(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    m = G_full.number_of_edges()\n",
        "    if m == 0: return G_full.__class__(), 0.0\n",
        "    # sort edges by time (tie-break by endpoints for determinism)\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    edges_sorted = sorted(\n",
        "        [(u, v, k, _edge_time_or_idx(d, i)) for i,(u,v,k,d) in enumerate(edges)],\n",
        "        key=lambda x: (x[3], str(x[0]), str(x[1]), str(x[2]))\n",
        "    )\n",
        "    k = max(1, int(round(m * max(0.0, min(1.0, keep_frac)))))\n",
        "    keep_e = set((u,v,k_) for (u,v,k_,_) in edges_sorted[:k])\n",
        "    # build induced subgraph on nodes touched by kept edges\n",
        "    nodes_keep = set()\n",
        "    for (u,v,k_) in keep_e:\n",
        "        nodes_keep.add(u); nodes_keep.add(v)\n",
        "    H = G_full.subgraph(nodes_keep).copy()\n",
        "    # but ensure only early edges are present (cut edges too)\n",
        "    H2 = G_full.__class__()\n",
        "    H2.add_nodes_from(H.nodes(data=True))\n",
        "    for (u,v,k_,d) in edges:\n",
        "        if (u,v,k_) in keep_e:\n",
        "            H2.add_edge(u,v,key=k_,**d)\n",
        "    return H2, k/m\n",
        "\n",
        "# ---------- Utilities ----------\n",
        "def _safe_div(a,b): return float(a)/float(b) if b not in (None,0) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0: return 0.0\n",
        "    if np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x))\n",
        "    n = x.size\n",
        "    cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "def _centralization(scores, topk=TOPK):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float)\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    top = np.sort(vals)[-min(topk, vals.size):][::-1]\n",
        "    return float(top.sum()), _safe_div(top.sum(), ssum)\n",
        "\n",
        "# ---------- Base + Role/Burst Features ----------\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0,\n",
        "                    deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _role_features(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=sum([]), pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    # Undirected simple graph for PageRank stability on small prefixes\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:\n",
        "        pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except Exception:\n",
        "        pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr, TOPK)\n",
        "    # HITS on directed view if available\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hsimple = nx.DiGraph()\n",
        "            Hsimple.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges():\n",
        "                Hsimple.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hsimple, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "    hub_top, hub_top_ratio   = _centralization(hubs, TOPK)\n",
        "    auth_top, auth_top_ratio = _centralization(auths, TOPK)\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    \"\"\"Within-prefix burstiness: fraction of edges in the last 20% of prefix time window.\"\"\"\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    # Gather times\n",
        "    ts = []\n",
        "    for idx, (_,_,d) in enumerate(G.edges(data=True)):\n",
        "        ts.append(_edge_time_or_idx(d, idx))\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    tmin, tmax = ts[0], ts[-1]\n",
        "    span = max(1e-9, (tmax - tmin))\n",
        "    t_cut = tmin + 0.8*span\n",
        "    frac_last = (ts >= t_cut).mean()\n",
        "    return dict(prefix_burst=float(frac_last), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full_edge(H: nx.Graph, keep_frac: float, S_full: dict, F_full_role: dict) -> pd.Series:\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features(H)\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "\n",
        "    # Absolute features\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"]\n",
        "    )\n",
        "\n",
        "    # Relative-to-full (role + structure)\n",
        "    out.update({\n",
        "        \"frac_nodes_of_full\": _safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        \"frac_edges_of_full\": _safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        \"density_rel\":        _safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        \"epn_rel\":            _safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        \"deg_gini_rel\":       _safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        \"pr_top_ratio_rel\":   _safe_div(R_p[\"pr_top_ratio\"], F_full_role[\"pr_top_ratio\"]) if F_full_role[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        \"hub_top_ratio_rel\":  _safe_div(R_p[\"hub_top_ratio\"], F_full_role[\"hub_top_ratio\"]) if F_full_role[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        \"auth_top_ratio_rel\": _safe_div(R_p[\"auth_top_ratio\"], F_full_role[\"auth_top_ratio\"]) if F_full_role[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        # growth “aheadness”\n",
        "        \"nodes_ahead\":        _safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        \"edges_ahead\":        _safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        \"epn_x_keep\":         S_p[\"epn\"]*keep_frac,\n",
        "    })\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---------- Models ----------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ---------- Split ----------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=CALIB_FRAC, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Precompute full-graph stats + full roles ----------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    S = _basic_stats(G)\n",
        "    R = _role_features(G)  # role on full\n",
        "    return S, R\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ---------- Build PREFIX-AUGMENTED TRAIN set (EDGE prefixes) ----------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_edges(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full_edge(H, actual, S_full, R_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ---------- helpers ----------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        best_idxs = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = best_idxs[np.argmin(fpr[ok][best_idxs])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "# ---------- Calibrate per keep (auto-orient + ROC θ) ----------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; ys=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_edges(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full_edge(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        ys.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(ys)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s)\n",
        "        flipped = False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s\n",
        "            flipped = True\n",
        "            auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(\"[CALIB] per keep (AUC, FPR@θ<=α, TPR@θ<=α):\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ---------- Evaluate on TEST (per keep) ----------\n",
        "records=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_edges(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full_edge(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]:\n",
        "            p = 1.0 - p\n",
        "        th = theta_map[name][keep]\n",
        "        yhat = (p > th).astype(int)  # tie -> negative\n",
        "        is_pos = (yt==1); is_neg=(yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, p)\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate (ROC-optimized @ FPR≤α) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR (should be ≤ α≈{:.2f}) ====\".format(ALPHA))\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvPpTSRPuJ32",
        "outputId": "8f393c56-5bfd-4f72-fc0b-9f0c017abcbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 774 graphs from glob. Class balance: [376 398]\n",
            "[TRAIN] Fit with 2165 prefix samples (from 433 graphs). Features: 29\n",
            "[CALIB] per keep (AUC, FPR@θ<=α, TPR@θ<=α):\n",
            "  HGB: {0.1: (0.5, 0.011, 0.01), 0.2: (0.52, 0.044, 0.062), 0.3: (0.521, 0.0, 0.0), 0.4: (0.773, 0.044, 0.615)} ...\n",
            "  LogReg: {0.1: (0.532, 0.0, 0.0), 0.2: (0.536, 0.0, 0.0), 0.3: (0.523, 0.044, 0.031), 0.4: (0.828, 0.011, 0.604)} ...\n",
            "  RF: {0.1: (0.528, 0.0, 0.0), 0.2: (0.528, 0.0, 0.0), 0.3: (0.517, 0.0, 0.0), 0.4: (0.787, 0.033, 0.573)} ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== ECT + strict-FPR + monotone postprocess =====================\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# Build a tidy DataFrame from your 'records' (already exists as 'res' in your output)\n",
        "df = res.copy()\n",
        "\n",
        "ALPHA = 1 - TARGET_SPEC  # your cap\n",
        "\n",
        "def earliest_confirmation(df, target_det=0.6, alpha=ALPHA):\n",
        "    ect = {}\n",
        "    for model in df[\"model\"].unique():\n",
        "        d = df[df[\"model\"]==model].sort_values(\"keep_frac\")\n",
        "        ok = (d[\"det_rate\"] >= target_det) & (d[\"fpr\"] <= alpha)\n",
        "        ect[model] = float(d.loc[ok, \"keep_frac\"].min()) if ok.any() else np.nan\n",
        "    return ect\n",
        "\n",
        "for t in [0.5, 0.6, 0.7]:\n",
        "    ect = earliest_confirmation(df, target_det=t, alpha=ALPHA)\n",
        "    print(f\"[ECT] target_det={t:.1f}, alpha≈{ALPHA:.2f} -> {ect}\")\n",
        "\n",
        "# ---- Strict FPR re-check (warn if any keeps violate the cap) ----\n",
        "viol = df.groupby(\"model\").apply(lambda g: dict(g.loc[g[\"fpr\"]>ALPHA, [\"keep_frac\",\"fpr\"]].values))\n",
        "print(\"\\n[STRICT-FPR CHECK] Keeps with FPR>α (if any):\")\n",
        "for m, items in viol.items():\n",
        "    print(f\"  {m}: {items if items else '{}'}\")\n",
        "\n",
        "# ---- Optional: Monotone predictions (once positive, always positive) ----\n",
        "# If you kept per-keep predictions, you can enforce monotonicity per sample.\n",
        "# Here is a template to apply AFTER computing per-keep yhat (not just aggregated rates):\n",
        "\n",
        "# Suppose you have a dict: PRED[model][keep] = np.array of 0/1 predictions per test sample\n",
        "# You can enforce monotonicity per sample like this:\n",
        "# for model in PRED:\n",
        "#     keeps = sorted(PRED[model].keys())\n",
        "#     M = np.vstack([PRED[model][k] for k in keeps])  # shape [K, N]\n",
        "#     M_mon = np.maximum.accumulate(M, axis=0)        # once 1, stay 1\n",
        "#     # recompute det_rate/fpr from M_mon for reporting\n",
        "#     # (you'll need y_test per keep; in your loop you already had it as 'yt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FDcGyB3ZuJ-J",
        "outputId": "1f5f53cb-74e6-4feb-fc55-4f20c037278f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ECT] target_det=0.5, alpha≈0.05 -> {'HGB': 0.6, 'LogReg': 0.4, 'RF': 0.6}\n",
            "[ECT] target_det=0.6, alpha≈0.05 -> {'HGB': 1.0, 'LogReg': nan, 'RF': 0.6}\n",
            "[ECT] target_det=0.7, alpha≈0.05 -> {'HGB': nan, 'LogReg': nan, 'RF': nan}\n",
            "\n",
            "[STRICT-FPR CHECK] Keeps with FPR>α (if any):\n",
            "  HGB: {np.float64(0.3): np.float64(0.06666666666666667), np.float64(0.4): np.float64(0.05333333333333334), np.float64(0.7): np.float64(0.06666666666666667), np.float64(0.8): np.float64(0.06666666666666667), np.float64(0.9): np.float64(0.06666666666666667)}\n",
            "  LogReg: {np.float64(0.5): np.float64(0.10666666666666667), np.float64(0.6): np.float64(0.05333333333333334), np.float64(0.7): np.float64(0.06666666666666667)}\n",
            "  RF: {np.float64(0.5): np.float64(0.08)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#improve"
      ],
      "metadata": {
        "id": "onhsnlizv7xQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Make the curve behave: α-margin calibration + monotone yhat\n",
        "# + dynamic TOP-K centralization for role features\n",
        "# ===========================================================\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# -------- Params you can tune --------\n",
        "ALPHA        = 1.0 - TARGET_SPEC          # your public FPR cap\n",
        "ALPHA_MARGIN = 0.01                       # train stricter by δ so TEST stays ≤ α\n",
        "ALPHA_TRAIN  = max(0.0, ALPHA - ALPHA_MARGIN)\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02  # dynamic TOP-K ~2% of nodes\n",
        "# -------------------------------------\n",
        "\n",
        "# --- Rebind role features to use dynamic TOP-K (≈ 2% of |V|) ---\n",
        "def _centralization(scores, topk):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float)\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), float(top.sum()/ssum if ssum>0 else 0.0)\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    # Undirected simple for PR\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr, topk=None)\n",
        "\n",
        "    # HITS directed if available\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hsimple = nx.DiGraph(); Hsimple.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hsimple.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hsimple, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs,  topk=None)\n",
        "    auth_top, auth_top_ratio = _centralization(auths, topk=None)\n",
        "\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "# Monkey-patch your feature_row to use dynamic roles (keep the same schema keys)\n",
        "def feature_row_prefix_vs_full_edge_DYNAMIC(H, keep_frac, S_full, F_full_role):\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features_DYNAMIC(H)            # <- dynamic TOP-K here\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        # relative-to-full same as before\n",
        "        frac_nodes_of_full=_safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full=_safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel=_safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel=_safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel=_safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel=_safe_div(R_p[\"pr_top_ratio\"], F_full_role[\"pr_top_ratio\"]) if F_full_role[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel=_safe_div(R_p[\"hub_top_ratio\"], F_full_role[\"hub_top_ratio\"]) if F_full_role[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel=_safe_div(R_p[\"auth_top_ratio\"], F_full_role[\"auth_top_ratio\"]) if F_full_role[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead=_safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead=_safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep=S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---- Helper: robust AUC and threshold selection with α_train ----\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        cands = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = cands[np.argmin(fpr[ok][cands])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "# ----------- CALIBRATION (per keep) with margin + auto-orientation -----------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    # Build calib frame with dynamic roles\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_edges(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full_edge_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)  # stricter cap during calibration\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ------------- TEST: collect per-keep scores, then monotone yhat -------------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}  # same y per keep\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_edges(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full_edge_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# Convert to monotone decisions per sample: once 1, always 1\n",
        "records=[]\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    # thresholds per keep\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    # score matrix KxN\n",
        "    P = np.vstack([SCORES[name][k] for k in keeps_sorted])  # shape [K, N]\n",
        "    # hard decisions by keep\n",
        "    YH = (P > ths[:,None]).astype(int)\n",
        "    # monotone enforce\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "    # metrics per keep\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]\n",
        "        yhat = YH_mon[kk]\n",
        "        is_pos = (yt==1); is_neg = (yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, P[kk])\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res2 = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate (monotone, α-margin calibrated) ====\")\n",
        "print(res2.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR (monotone) — should mostly be ≤ α≈{:.2f} ====\".format(ALPHA))\n",
        "print(res2.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (scores unchanged by monotone step) ====\")\n",
        "print(res2.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_N2voZSYv73C",
        "outputId": "38bd6e88-90a6-4d98-d574-cf9007b02c49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.1: (0.537, 0.034, 0.062), 0.2: (0.541, 0.034, 0.052), 0.3: (0.519, 0.0, 0.0), 0.4: (0.746, 0.034, 0.521)} ...\n",
            "  LogReg: {0.1: (0.511, 0.0, 0.0), 0.2: (0.521, 0.0, 0.0), 0.3: (0.535, 0.034, 0.01), 0.4: (0.798, 0.022, 0.49)} ...\n",
            "  RF: {0.1: (0.5, 0.0, 0.0), 0.2: (0.511, 0.0, 0.0), 0.3: (0.5, 0.0, 0.0), 0.4: (0.72, 0.034, 0.312)} ...\n",
            "\n",
            "==== Early Detection — DetRate (monotone, α-margin calibrated) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0510  0.0000 0.0000\n",
            "0.2000    0.0510  0.0000 0.0000\n",
            "0.3000    0.0510  0.0000 0.0000\n",
            "0.4000    0.5950  0.5570 0.1650\n",
            "0.5000    0.6080  0.6080 0.4430\n",
            "0.6000    0.6460  0.6710 0.5820\n",
            "0.7000    0.6580  0.6710 0.5950\n",
            "0.8000    0.6580  0.6710 0.5950\n",
            "0.9000    0.6710  0.6710 0.6710\n",
            "1.0000    0.6710  0.6710 0.6710\n",
            "\n",
            "==== Early Detection — FPR (monotone) — should mostly be ≤ α≈0.05 ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.0130  0.0000 0.0000\n",
            "0.2000    0.0130  0.0000 0.0000\n",
            "0.3000    0.0130  0.0130 0.0000\n",
            "0.4000    0.0670  0.0400 0.0000\n",
            "0.5000    0.1200  0.1200 0.0800\n",
            "0.6000    0.1200  0.1730 0.0930\n",
            "0.7000    0.1200  0.1730 0.0930\n",
            "0.8000    0.1200  0.1730 0.0930\n",
            "0.9000    0.1200  0.1730 0.1330\n",
            "1.0000    0.1200  0.1730 0.1470\n",
            "\n",
            "==== Early Detection — AUC (scores unchanged by monotone step) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1000    0.5310  0.5320 0.4610\n",
            "0.2000    0.5340  0.5620 0.5140\n",
            "0.3000    0.5270  0.4630 0.5260\n",
            "0.4000    0.7110  0.7950 0.7760\n",
            "0.5000    0.4670  0.7320 0.6310\n",
            "0.6000    0.7890  0.7140 0.7630\n",
            "0.7000    0.7590  0.8080 0.7130\n",
            "0.8000    0.7570  0.8190 0.7920\n",
            "0.9000    0.8050  0.8060 0.7920\n",
            "1.0000    0.8040  0.8100 0.7960\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# How many nodes are kept/cut at keep=0.40 (edge-based)?\n",
        "# =========================\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "KEEP_TO_INSPECT = 0.40  # change to any keep you want (e.g., 0.10, 0.60, etc.)\n",
        "\n",
        "rows = []\n",
        "for i in te_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    n_full = Gfull.number_of_nodes()\n",
        "    m_full = Gfull.number_of_edges()\n",
        "    H, actual_edge_keep = prefix_by_edges(Gfull, KEEP_TO_INSPECT)\n",
        "    n_pref = H.number_of_nodes()\n",
        "    m_pref = H.number_of_edges()\n",
        "    rows.append(dict(\n",
        "        file=Path(paths[i]).name,\n",
        "        n_full=n_full, n_prefix=n_pref, nodes_cut=max(n_full - n_pref, 0),\n",
        "        frac_nodes_kept=(n_pref / n_full) if n_full>0 else np.nan,\n",
        "        m_full=m_full, m_prefix=m_pref, frac_edges_kept=actual_edge_keep\n",
        "    ))\n",
        "\n",
        "df_nodes = pd.DataFrame(rows).sort_values(\"frac_nodes_kept\")\n",
        "print(f\"[INFO] TEST graphs: {len(df_nodes)} | keep(edges) requested={KEEP_TO_INSPECT:.2f}\")\n",
        "print(\"\\nHead (lowest frac_nodes_kept first):\")\n",
        "print(df_nodes.head(10).to_string(index=False))\n",
        "\n",
        "# Summary stats\n",
        "def q(x,p):\n",
        "    x = np.asarray(x, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    return np.quantile(x, p) if x.size else np.nan\n",
        "\n",
        "print(\"\\n--- Summary over TEST ---\")\n",
        "print(f\"Edges kept (actual): mean={df_nodes['frac_edges_kept'].mean():.3f}, median={df_nodes['frac_edges_kept'].median():.3f}\")\n",
        "print(f\"Nodes kept fraction : mean={df_nodes['frac_nodes_kept'].mean():.3f}, median={df_nodes['frac_nodes_kept'].median():.3f}, \"\n",
        "      f\"p10={q(df_nodes['frac_nodes_kept'],0.10):.3f}, p90={q(df_nodes['frac_nodes_kept'],0.90):.3f}\")\n",
        "print(f\"Nodes cut (count)   : mean={df_nodes['nodes_cut'].mean():.1f}, median={df_nodes['nodes_cut'].median():.0f}\")\n",
        "\n",
        "# If you want to inspect a specific graph with extreme behavior:\n",
        "worst = df_nodes.iloc[0]\n",
        "best  = df_nodes.iloc[-1]\n",
        "print(\"\\nExamples:\")\n",
        "print(f\"  LOW keep case : file={worst['file']}  n_full={int(worst['n_full'])}  n_prefix={int(worst['n_prefix'])}  \"\n",
        "      f\"frac_nodes_kept={worst['frac_nodes_kept']:.3f}  frac_edges_kept={worst['frac_edges_kept']:.3f}\")\n",
        "print(f\"  HIGH keep case: file={best['file']}   n_full={int(best['n_full'])}  n_prefix={int(best['n_prefix'])}  \"\n",
        "      f\"frac_nodes_kept={best['frac_nodes_kept']:.3f}   frac_edges_kept={best['frac_edges_kept']:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjkC6h3jv782",
        "outputId": "ddcfb627-6203-4c92-effc-ca79f648e835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] TEST graphs: 154 | keep(edges) requested=0.40\n",
            "\n",
            "Head (lowest frac_nodes_kept first):\n",
            "                                    file  n_full  n_prefix  nodes_cut  frac_nodes_kept  m_full  m_prefix  frac_edges_kept\n",
            "                   message_graph.gpickle    1986       797       1189           0.4013    5190      2076           0.4000\n",
            "24c4ba3a0bf6ca58b0efa61faf98f190.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "5020485fb74375ed5652e6693dcd1d70.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "26a6a66f7e8a9acbedaa8463051a18ba.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "85fec0fb2d868db3e08612eb3d46d665.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "9005eb6d522f5818e20f18a771f9ef82.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "992f5baa3714da869b27b3be1c1ee82b.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "78fd5dac4f6f00ef00887272b6438f36.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "d772421a5e256b523c8b23e98748818c.gpickle       6         6          0           1.0000      20         8           0.4000\n",
            "e46e001c5a1366f8d530f9d3541fd6f7.gpickle       5         5          0           1.0000      13         5           0.3846\n",
            "\n",
            "--- Summary over TEST ---\n",
            "Edges kept (actual): mean=0.386, median=0.385\n",
            "Nodes kept fraction : mean=0.996, median=1.000, p10=1.000, p90=1.000\n",
            "Nodes cut (count)   : mean=7.7, median=0\n",
            "\n",
            "Examples:\n",
            "  LOW keep case : file=message_graph.gpickle  n_full=1986  n_prefix=797  frac_nodes_kept=0.401  frac_edges_kept=0.400\n",
            "  HIGH keep case: file=ffcdc391449183086b16730d68ece0e1.gpickle   n_full=5  n_prefix=5  frac_nodes_kept=1.000   frac_edges_kept=0.385\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Nodes kept vs edges kept for many keep_fracs\n",
        "# =========================\n",
        "import numpy as np, pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "KEEPS = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]\n",
        "\n",
        "rows = []\n",
        "for keep in KEEPS:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        n_full = Gfull.number_of_nodes(); m_full = Gfull.number_of_edges()\n",
        "        H, actual_edge_keep = prefix_by_edges(Gfull, keep)\n",
        "        n_pref = H.number_of_nodes(); m_pref = H.number_of_edges()\n",
        "        rows.append(dict(\n",
        "            keep=keep, file=Path(paths[i]).name,\n",
        "            n_full=n_full, n_prefix=n_pref, nodes_cut=max(n_full - n_pref, 0),\n",
        "            frac_nodes_kept=(n_pref / n_full) if n_full>0 else np.nan,\n",
        "            m_full=m_full, m_prefix=m_pref, frac_edges_kept=actual_edge_keep\n",
        "        ))\n",
        "\n",
        "df_all = pd.DataFrame(rows)\n",
        "\n",
        "# Per-keep summary\n",
        "summ = df_all.groupby(\"keep\").agg(\n",
        "    graphs=(\"file\",\"count\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    nodes_kept_p10=(\"frac_nodes_kept\", lambda x: np.nanquantile(x, 0.10)),\n",
        "    nodes_kept_p90=(\"frac_nodes_kept\", lambda x: np.nanquantile(x, 0.90)),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(3)\n",
        "\n",
        "# Correlation (how tightly node coverage tracks edge coverage) per keep\n",
        "cors = []\n",
        "for k, g in df_all.groupby(\"keep\"):\n",
        "    x = g[\"frac_edges_kept\"].to_numpy()\n",
        "    y = g[\"frac_nodes_kept\"].to_numpy()\n",
        "    mask = np.isfinite(x) & np.isfinite(y)\n",
        "    rho = spearmanr(x[mask], y[mask]).correlation if mask.any() else np.nan\n",
        "    cors.append(dict(keep=k, spearman_rho=round(float(rho),3) if np.isfinite(rho) else np.nan))\n",
        "cors = pd.DataFrame(cors)\n",
        "\n",
        "print(\"\\n=== Nodes vs Edges Kept: per-keep summary ===\")\n",
        "print(summ)\n",
        "print(\"\\n=== Spearman correlation(frac_edges_kept, frac_nodes_kept) by keep ===\")\n",
        "print(cors.sort_values(\"keep\").to_string(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXS1yNsKv8C2",
        "outputId": "63694a3b-bb08-4ebb-9a1e-c483fab1ead5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Nodes vs Edges Kept: per-keep summary ===\n",
            "        graphs  edges_kept_mean  edges_kept_median  nodes_kept_mean  \\\n",
            "keep                                                                  \n",
            "0.1000     154           0.0780             0.0770           0.4030   \n",
            "0.2000     154           0.2290             0.2310           0.7970   \n",
            "0.3000     154           0.3080             0.3080           0.9950   \n",
            "0.4000     154           0.3860             0.3850           0.9960   \n",
            "0.5000     154           0.4640             0.4620           0.9970   \n",
            "0.6000     154           0.6140             0.6150           0.9970   \n",
            "0.7000     154           0.6920             0.6920           0.9980   \n",
            "0.8000     154           0.7710             0.7690           0.9990   \n",
            "0.9000     154           0.9220             0.9230           0.9990   \n",
            "1.0000     154           1.0000             1.0000           1.0000   \n",
            "\n",
            "        nodes_kept_median  nodes_kept_p10  nodes_kept_p90  nodes_cut_median  \n",
            "keep                                                                         \n",
            "0.1000             0.4000          0.4000          0.4000            3.0000  \n",
            "0.2000             0.8000          0.8000          0.8000            1.0000  \n",
            "0.3000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.4000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.5000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.6000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.7000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.8000             1.0000          1.0000          1.0000            0.0000  \n",
            "0.9000             1.0000          1.0000          1.0000            0.0000  \n",
            "1.0000             1.0000          1.0000          1.0000            0.0000  \n",
            "\n",
            "=== Spearman correlation(frac_edges_kept, frac_nodes_kept) by keep ===\n",
            "  keep  spearman_rho\n",
            "0.1000        0.7480\n",
            "0.2000       -0.4900\n",
            "0.3000        0.3480\n",
            "0.4000       -0.3430\n",
            "0.5000       -0.3450\n",
            "0.6000        0.3430\n",
            "0.7000       -0.3480\n",
            "0.8000       -0.3430\n",
            "0.9000        0.3480\n",
            "1.0000           NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###############improve"
      ],
      "metadata": {
        "id": "PWffzfZNv8Ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — Node-governed (time-guarded) prefixes + role centralization + burstiness\n",
        "# Prefix-augmented training, α-margin ROC calibration, monotone decisions\n",
        "# =====================================================================================\n",
        "import os, glob, warnings, math\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option(\"display.width\", 160)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "KEEP_FRACS_EVAL  = [0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90,1.00]  # for evaluation\n",
        "KEEP_FRACS_TRAIN = [0.12,0.25,0.40,0.65,1.00]                           # prefix augmentation for TRAIN\n",
        "TARGET_SPEC  = 0.95                                                     # α = 1 - TARGET_SPEC\n",
        "ALPHA        = 1.0 - TARGET_SPEC\n",
        "ALPHA_MARGIN = 0.01                                                     # train with stricter α\n",
        "ALPHA_TRAIN  = max(0.0, ALPHA - ALPHA_MARGIN)\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "\n",
        "# Dynamic TOP-K centralization\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02\n",
        "\n",
        "# ------------------ Discovery ------------------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    # Prefer merged/y if provided by earlier notebook cells, else glob\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader (use your load_gpickle if defined)\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ------------------ Time helpers ------------------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)\n",
        "\n",
        "# ------------------ Node-governed (time-guarded) prefix ------------------\n",
        "def prefix_by_nodes_guarded(G_full: nx.Graph, keep_frac: float):\n",
        "    \"\"\"Keep earliest K% nodes (by first incident edge time) and only edges among them\n",
        "       whose time ≤ the first-appearance time of the next (K+1)-th node.\"\"\"\n",
        "    if not isinstance(G_full, nx.MultiDiGraph):\n",
        "        G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes() == 0:\n",
        "        return G_full.__class__(), 0.0\n",
        "\n",
        "    # First-appearance per node\n",
        "    node_first = {n: np.inf for n in G_full.nodes()}\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if t < node_first[u]: node_first[u] = t\n",
        "        if t < node_first[v]: node_first[v] = t\n",
        "\n",
        "    nodes_sorted = sorted(node_first.items(), key=lambda kv: (kv[1], str(kv[0])))\n",
        "    N = len(nodes_sorted)\n",
        "    K = max(1, int(round(max(0.0, min(1.0, keep_frac)) * N)))\n",
        "\n",
        "    kept_nodes = {n for n,_ in nodes_sorted[:K]}\n",
        "    t_guard = nodes_sorted[K][1] if K < N else np.inf\n",
        "\n",
        "    H = G_full.__class__()\n",
        "    H.add_nodes_from((n, G_full.nodes[n]) for n in kept_nodes)\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        if (u in kept_nodes) and (v in kept_nodes):\n",
        "            t = _edge_time_or_idx(d, idx)\n",
        "            if t <= t_guard:\n",
        "                H.add_edge(u, v, key=k, **d)\n",
        "\n",
        "    return H, K / float(N)\n",
        "\n",
        "# ------------------ Feature helpers ------------------\n",
        "def _safe_div(a,b): return float(a)/float(b) if (b not in (None,0)) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0 or np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x))\n",
        "    n = x.size\n",
        "    cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0, deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _centralization(scores):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float)\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), float(top.sum()/ssum if ssum>0 else 0.0)\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    # Undirected for PR\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr)\n",
        "\n",
        "    # HITS on directed view if available\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hd = nx.DiGraph(); Hd.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hd.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hd, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs)\n",
        "    auth_top, auth_top_ratio = _centralization(auths)\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    \"\"\"Within-prefix burstiness: fraction of edges in last 20% of prefix time window.\"\"\"\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    ts = []\n",
        "    for idx, (_,_,d) in enumerate(G.edges(data=True)):\n",
        "        ts.append(_edge_time_or_idx(d, idx))\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    tmin, tmax = ts[0], ts[-1]\n",
        "    span = max(1e-9, (tmax - tmin))\n",
        "    t_cut = tmin + 0.8*span\n",
        "    frac_last = (ts >= t_cut).mean()\n",
        "    return dict(prefix_burst=float(frac_last), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full_DYNAMIC(H: nx.Graph, keep_frac: float, S_full: dict, R_full: dict) -> pd.Series:\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features_DYNAMIC(H)\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        # relative to full\n",
        "        frac_nodes_of_full=_safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full=_safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel=_safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel=_safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel=_safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel=_safe_div(R_p[\"pr_top_ratio\"], R_full[\"pr_top_ratio\"]) if R_full[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel=_safe_div(R_p[\"hub_top_ratio\"], R_full[\"hub_top_ratio\"]) if R_full[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel=_safe_div(R_p[\"auth_top_ratio\"], R_full[\"auth_top_ratio\"]) if R_full[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead=_safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead=_safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep=S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ------------------ Models ------------------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ------------------ Split ------------------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=0.30, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ------------------ Precompute full-graph packs ------------------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    return _basic_stats(G), _role_features_DYNAMIC(G)\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ------------------ Build TRAIN (prefix-augmented, node-guarded) ------------------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ------------------ Calibration (per keep) with α_train & auto-orientation ------------------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        cands = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = cands[np.argmin(fpr[ok][cands])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ------------------ TEST: collect per-keep scores, then monotone decisions ------------------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# Monotone: once 1, always 1 as keep grows\n",
        "records=[]\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    P = np.vstack([SCORES[name][k] for k in keeps_sorted])   # [K, N]\n",
        "    YH = (P > ths[:,None]).astype(int)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]\n",
        "        yhat = YH_mon[kk]\n",
        "        is_pos = (yt==1); is_neg=(yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, P[kk])\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate (monotone, α-margin calibrated) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR (monotone) — target α≈{:.2f} ====\".format(ALPHA))\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (threshold-free) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n",
        "\n",
        "# ------------------ Sanity: nodes vs edges kept on TEST (node-governed) ------------------\n",
        "rows=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual_nodes_keep = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        n_full = Gfull.number_of_nodes(); n_pref = H.number_of_nodes()\n",
        "        m_full = Gfull.number_of_edges();  m_pref = H.number_of_edges()\n",
        "        rows.append(dict(keep=keep,\n",
        "                         frac_nodes_kept=(n_pref/n_full) if n_full>0 else np.nan,\n",
        "                         frac_edges_kept=(m_pref/m_full) if m_full>0 else np.nan,\n",
        "                         nodes_cut=max(n_full - n_pref, 0)))\n",
        "df_chk = pd.DataFrame(rows)\n",
        "summ = df_chk.groupby(\"keep\").agg(\n",
        "    graphs=(\"nodes_cut\",\"count\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(3)\n",
        "print(\"\\n==== Node-governed prefixes — coverage sanity on TEST ====\")\n",
        "print(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5rm5Fi1yg_H",
        "outputId": "c4ba34db-014e-4949-c525-9da2ee9acc5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 774 graphs from glob. Class balance: [376 398]\n",
            "[TRAIN] Fit with 2165 prefix samples (from 433 graphs). Features: 29\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.1: (0.555, 0.0, 0.0), 0.2: (0.555, 0.0, 0.0), 0.3: (0.555, 0.0, 0.0), 0.4: (0.555, 0.0, 0.0)} ...\n",
            "  LogReg: {0.1: (0.541, 0.022, 0.01), 0.2: (0.552, 0.011, 0.01), 0.3: (0.529, 0.0, 0.0), 0.4: (0.529, 0.0, 0.0)} ...\n",
            "  RF: {0.1: (0.528, 0.0, 0.0), 0.2: (0.528, 0.0, 0.0), 0.3: (0.528, 0.0, 0.0), 0.4: (0.528, 0.0, 0.0)} ...\n",
            "\n",
            "==== Early Detection — DetRate (monotone, α-margin calibrated) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1        0.000    0.00  0.000\n",
            "0.2        0.000    0.00  0.000\n",
            "0.3        0.000    0.00  0.000\n",
            "0.4        0.000    0.00  0.000\n",
            "0.5        0.000    0.00  0.000\n",
            "0.6        0.000    0.00  0.000\n",
            "0.7        0.000    0.00  0.000\n",
            "0.8        0.000    0.00  0.000\n",
            "0.9        0.000    0.00  0.000\n",
            "1.0        0.638    0.65  0.538\n",
            "\n",
            "==== Early Detection — FPR (monotone) — target α≈0.05 ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1        0.000   0.013  0.000\n",
            "0.2        0.000   0.013  0.000\n",
            "0.3        0.000   0.013  0.000\n",
            "0.4        0.000   0.013  0.000\n",
            "0.5        0.000   0.013  0.000\n",
            "0.6        0.000   0.013  0.000\n",
            "0.7        0.000   0.013  0.000\n",
            "0.8        0.000   0.013  0.000\n",
            "0.9        0.000   0.013  0.000\n",
            "1.0        0.067   0.120  0.013\n",
            "\n",
            "==== Early Detection — AUC (threshold-free) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.1        0.520   0.515  0.514\n",
            "0.2        0.520   0.528  0.514\n",
            "0.3        0.532   0.519  0.514\n",
            "0.4        0.533   0.519  0.514\n",
            "0.5        0.533   0.518  0.514\n",
            "0.6        0.533   0.507  0.501\n",
            "0.7        0.534   0.512  0.501\n",
            "0.8        0.533   0.512  0.514\n",
            "0.9        0.532   0.512  0.501\n",
            "1.0        0.845   0.809  0.780\n",
            "\n",
            "==== Node-governed prefixes — coverage sanity on TEST ====\n",
            "      graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                  \n",
            "0.1      155            0.198                0.2            0.001              0.000               4.0\n",
            "0.2      155            0.199                0.2            0.003              0.000               4.0\n",
            "0.3      155            0.397                0.4            0.079              0.077               3.0\n",
            "0.4      155            0.398                0.4            0.080              0.077               3.0\n",
            "0.5      155            0.404                0.4            0.083              0.077               3.0\n",
            "0.6      155            0.602                0.6            0.159              0.154               2.0\n",
            "0.7      155            0.795                0.8            0.235              0.231               1.0\n",
            "0.8      155            0.801                0.8            0.237              0.231               1.0\n",
            "0.9      155            0.802                0.8            0.239              0.231               1.0\n",
            "1.0      155            1.000                1.0            1.000              1.000               0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate at achievable node fractions (cleanest)"
      ],
      "metadata": {
        "id": "ui7qtmRsyhFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — Node-governed (time-guarded) prefixes @ achievable node fractions\n",
        "# Roles (dynamic TOP-K), burstiness; prefix-augmented training; α-margin ROC calibration;\n",
        "# monotone decisions over keeps. 100% drop-in, no external dependencies required.\n",
        "# =====================================================================================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option(\"display.width\", 160)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "# Use achievable node fractions to avoid rounding plateaus on small graphs\n",
        "KEEP_FRACS_EVAL  = [0.20, 0.40, 0.60, 0.80, 1.00]   # evaluation grid\n",
        "KEEP_FRACS_TRAIN = [0.20, 0.40, 0.60, 1.00]         # prefix augmentation for TRAIN\n",
        "\n",
        "TARGET_SPEC  = 0.95                                  # α = 1 - TARGET_SPEC\n",
        "ALPHA        = 1.0 - TARGET_SPEC\n",
        "ALPHA_MARGIN = 0.01                                  # train stricter to keep TEST ≤ α\n",
        "ALPHA_TRAIN  = max(0.0, ALPHA - ALPHA_MARGIN)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "\n",
        "# Dynamic TOP-K centralization params\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02\n",
        "\n",
        "# ------------------ Discovery ------------------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    # Prefer merged/y if provided, else glob for .gpickle/.gpkl\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# loader (use your load_gpickle if defined)\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ------------------ Time helpers ------------------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)\n",
        "\n",
        "# ------------------ Node-governed (time-guarded) prefix ------------------\n",
        "def prefix_by_nodes_guarded(G_full: nx.Graph, keep_frac: float):\n",
        "    \"\"\"\n",
        "    Keep earliest K% of nodes by first-incident-edge time.\n",
        "    Include only edges among kept nodes whose time ≤ the (K+1)-th node's first-appearance time.\n",
        "    Deterministic rounding to nearest achievable fraction via K = round(keep*N).\n",
        "    \"\"\"\n",
        "    if not isinstance(G_full, nx.MultiDiGraph):\n",
        "        G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes() == 0:\n",
        "        return G_full.__class__(), 0.0\n",
        "\n",
        "    # First-appearance per node\n",
        "    node_first = {n: np.inf for n in G_full.nodes()}\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if t < node_first[u]: node_first[u] = t\n",
        "        if t < node_first[v]: node_first[v] = t\n",
        "\n",
        "    nodes_sorted = sorted(node_first.items(), key=lambda kv: (kv[1], str(kv[0])))\n",
        "    N = len(nodes_sorted)\n",
        "    # Deterministic achievable K on node grid\n",
        "    K = int(round(np.clip(keep_frac, 0.0, 1.0) * N))\n",
        "    K = min(max(1, K), N)\n",
        "\n",
        "    kept_nodes = {n for n,_ in nodes_sorted[:K]}\n",
        "    t_guard = nodes_sorted[K][1] if K < N else np.inf\n",
        "\n",
        "    H = G_full.__class__()\n",
        "    H.add_nodes_from((n, G_full.nodes[n]) for n in kept_nodes)\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        if (u in kept_nodes) and (v in kept_nodes):\n",
        "            t = _edge_time_or_idx(d, idx)\n",
        "            if t <= t_guard:\n",
        "                H.add_edge(u, v, key=k, **d)\n",
        "\n",
        "    return H, K / float(N)\n",
        "\n",
        "# ------------------ Feature helpers ------------------\n",
        "def _safe_div(a,b): return float(a)/float(b) if (b not in (None,0)) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0 or np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x))\n",
        "    n = x.size\n",
        "    cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0, deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _centralization(scores):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float)\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), float(top.sum()/ssum if ssum>0 else 0.0)\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    # Undirected for PR\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr)\n",
        "\n",
        "    # HITS (directed) if available\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hd = nx.DiGraph(); Hd.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hd.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hd, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs)\n",
        "    auth_top, auth_top_ratio = _centralization(auths)\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    \"\"\"Within-prefix burstiness: fraction of edges in last 20% of prefix time window.\"\"\"\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    ts = []\n",
        "    for idx, (_,_,d) in enumerate(G.edges(data=True)):\n",
        "        ts.append(_edge_time_or_idx(d, idx))\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    tmin, tmax = ts[0], ts[-1]\n",
        "    span = max(1e-9, (tmax - tmin))\n",
        "    t_cut = tmin + 0.8*span\n",
        "    frac_last = (ts >= t_cut).mean()\n",
        "    return dict(prefix_burst=float(frac_last), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full_DYNAMIC(H: nx.Graph, keep_frac: float, S_full: dict, R_full: dict) -> pd.Series:\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features_DYNAMIC(H)\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        # relative to full\n",
        "        frac_nodes_of_full=_safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full=_safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel=_safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel=_safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel=_safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel=_safe_div(R_p[\"pr_top_ratio\"], R_full[\"pr_top_ratio\"]) if R_full[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel=_safe_div(R_p[\"hub_top_ratio\"], R_full[\"hub_top_ratio\"]) if R_full[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel=_safe_div(R_p[\"auth_top_ratio\"], R_full[\"auth_top_ratio\"]) if R_full[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead=_safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead=_safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep=S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ------------------ Models ------------------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ------------------ Split ------------------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=0.30, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ------------------ Precompute full-graph packs ------------------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    return _basic_stats(G), _role_features_DYNAMIC(G)\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ------------------ Build TRAIN (prefix-augmented, node-guarded) ------------------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ------------------ Calibration (per keep) with α_train & auto-orientation ------------------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        cands = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = cands[np.argmin(fpr[ok][cands])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:3]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ------------------ TEST: collect per-keep scores, then monotone decisions ------------------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# Monotone: once 1, always 1 as keep grows\n",
        "records=[]\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    P = np.vstack([SCORES[name][k] for k in keeps_sorted])   # [K, N]\n",
        "    YH = (P > ths[:,None]).astype(int)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]\n",
        "        yhat = YH_mon[kk]\n",
        "        is_pos = (yt==1); is_neg=(yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, P[kk])\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate (monotone, α-margin calibrated) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR (monotone) — target α≈{:.2f} ====\".format(ALPHA))\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (threshold-free) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n",
        "\n",
        "# ------------------ Sanity: nodes vs edges kept on TEST (node-governed) ------------------\n",
        "rows=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual_nodes_keep = prefix_by_nodes_guarded(Gfull, keep)\n",
        "        n_full = Gfull.number_of_nodes(); n_pref = H.number_of_nodes()\n",
        "        m_full = Gfull.number_of_edges();  m_pref = H.number_of_edges()\n",
        "        rows.append(dict(keep=keep,\n",
        "                         frac_nodes_kept=(n_pref/n_full) if n_full>0 else np.nan,\n",
        "                         frac_edges_kept=(m_pref/m_full) if m_full>0 else np.nan,\n",
        "                         nodes_cut=max(n_full - n_pref, 0)))\n",
        "df_chk = pd.DataFrame(rows)\n",
        "summ = df_chk.groupby(\"keep\").agg(\n",
        "    graphs=(\"nodes_cut\",\"count\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(3)\n",
        "print(\"\\n==== Node-governed prefixes — coverage sanity on TEST ====\")\n",
        "print(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUF3zhEyhK1",
        "outputId": "eaadd669-bafe-414a-e241-accbbe9cc37b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 770 graphs from merged(y). Class balance: [373 397]\n",
            "[TRAIN] Fit with 1724 prefix samples (from 431 graphs). Features: 29\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.2: (0.502, 0.022, 0.031), 0.4: (0.502, 0.022, 0.031), 0.6: (0.502, 0.022, 0.031)} ...\n",
            "  LogReg: {0.2: (0.511, 0.0, 0.0), 0.4: (0.511, 0.0, 0.0), 0.6: (0.511, 0.0, 0.0)} ...\n",
            "  RF: {0.2: (0.522, 0.0, 0.0), 0.4: (0.522, 0.0, 0.0), 0.6: (0.522, 0.0, 0.0)} ...\n",
            "\n",
            "==== Early Detection — DetRate (monotone, α-margin calibrated) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2000    0.0000  0.0000 0.0000\n",
            "0.4000    0.0000  0.0000 0.0000\n",
            "0.6000    0.0000  0.0000 0.0000\n",
            "0.8000    0.0000  0.0000 0.0000\n",
            "1.0000    0.4560  0.5820 0.0000\n",
            "\n",
            "==== Early Detection — FPR (monotone) — target α≈0.05 ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2000    0.0130  0.0000 0.0000\n",
            "0.4000    0.0130  0.0000 0.0000\n",
            "0.6000    0.0130  0.0000 0.0000\n",
            "0.8000    0.0130  0.0000 0.0000\n",
            "1.0000    0.0270  0.0270 0.0000\n",
            "\n",
            "==== Early Detection — AUC (threshold-free) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2000    0.4980  0.5140 0.5270\n",
            "0.4000    0.4980  0.5140 0.5270\n",
            "0.6000    0.4980  0.5260 0.5270\n",
            "0.8000    0.5040  0.5260 0.5140\n",
            "1.0000    0.8130  0.7950 0.8460\n",
            "\n",
            "==== Node-governed prefixes — coverage sanity on TEST ====\n",
            "        graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                    \n",
            "0.2000     154           0.1990             0.2000           0.0010             0.0000            4.0000\n",
            "0.4000     154           0.3980             0.4000           0.0780             0.0770            3.0000\n",
            "0.6000     154           0.6020             0.6000           0.1560             0.1540            2.0000\n",
            "0.8000     154           0.8010             0.8000           0.2330             0.2310            1.0000\n",
            "1.0000     154           1.0000             1.0000           1.0000             1.0000            0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================================================\n",
        "# EARLY DETECTION — Achievable node fractions + (fix starving) NODE-INDUCED prefixes\n",
        "# Modes: 'node_induced' (default), 'node_guarded', or 'edge'\n",
        "# Roles (dynamic TOP-K), burstiness; prefix-aug training; α-margin ROC calibration; monotone yhat\n",
        "# =====================================================================================\n",
        "import os, glob, warnings\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "np.set_printoptions(suppress=True)\n",
        "pd.set_option(\"display.width\", 160)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "PREFIX_MODE   = \"node_induced\"             # 'node_induced' | 'node_guarded' | 'edge'\n",
        "KEEP_FRACS_EVAL  = [0.20, 0.40, 0.60, 0.80, 1.00]\n",
        "KEEP_FRACS_TRAIN = [0.20, 0.40, 0.60, 1.00]\n",
        "\n",
        "TARGET_SPEC  = 0.95\n",
        "ALPHA        = 1.0 - TARGET_SPEC\n",
        "ALPHA_MARGIN = 0.01\n",
        "ALPHA_TRAIN  = max(0.0, ALPHA - ALPHA_MARGIN)\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "\n",
        "# Dynamic TOP-K for role centralization\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02\n",
        "\n",
        "# ------------------ Discovery ------------------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"merged(col)\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"merged(path)\"\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ------------------ Time helpers ------------------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)\n",
        "\n",
        "# ------------------ Prefix builders ------------------\n",
        "def _first_node_times(G_full):\n",
        "    node_first = {n: np.inf for n in G_full.nodes()}\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if t < node_first[u]: node_first[u] = t\n",
        "        if t < node_first[v]: node_first[v] = t\n",
        "    return sorted(node_first.items(), key=lambda kv: (kv[1], str(kv[0]))), edges\n",
        "\n",
        "def prefix_by_nodes_guarded(G_full: nx.Graph, keep_frac: float):\n",
        "    \"\"\"Earliest K% nodes; include only edges among them whose time <= (K+1)-th node's time.\"\"\"\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes() == 0: return G_full.__class__(), 0.0\n",
        "    nodes_sorted, edges = _first_node_times(G_full)\n",
        "    N = len(nodes_sorted)\n",
        "    K = int(round(np.clip(keep_frac, 0.0, 1.0) * N)); K = min(max(1, K), N)\n",
        "    kept = {n for n,_ in nodes_sorted[:K]}\n",
        "    t_guard = nodes_sorted[K][1] if K < N else np.inf\n",
        "    H = G_full.__class__(); H.add_nodes_from((n, G_full.nodes[n]) for n in kept)\n",
        "    for idx,(u,v,k,d) in enumerate(edges):\n",
        "        if u in kept and v in kept and _edge_time_or_idx(d, idx) <= t_guard:\n",
        "            H.add_edge(u, v, key=k, **d)\n",
        "    return H, K/float(N)\n",
        "\n",
        "def prefix_by_nodes_induced(G_full: nx.Graph, keep_frac: float):\n",
        "    \"\"\"Earliest K% nodes; include ALL edges among them (induced subgraph).\"\"\"\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes() == 0: return G_full.__class__(), 0.0\n",
        "    nodes_sorted, _ = _first_node_times(G_full)\n",
        "    N = len(nodes_sorted)\n",
        "    K = int(round(np.clip(keep_frac, 0.0, 1.0) * N)); K = min(max(1, K), N)\n",
        "    kept = [n for n,_ in nodes_sorted[:K]]\n",
        "    H = G_full.subgraph(kept).copy()\n",
        "    return H, K/float(N)\n",
        "\n",
        "def prefix_by_edges(G_full: nx.Graph, keep_frac: float):\n",
        "    \"\"\"Earliest K% edges; include only the kept edges; induced nodes.\"\"\"\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    m = G_full.number_of_edges()\n",
        "    if m == 0: return G_full.__class__(), 0.0\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    edges_sorted = sorted(\n",
        "        [(u, v, k, _edge_time_or_idx(d, i)) for i,(u,v,k,d) in enumerate(edges)],\n",
        "        key=lambda x: (x[3], str(x[0]), str(x[1]), str(x[2]))\n",
        "    )\n",
        "    k = max(1, int(round(np.clip(keep_frac,0.0,1.0) * m)))\n",
        "    keep_e = set((u,v,k_) for (u,v,k_,_) in edges_sorted[:k])\n",
        "    nodes_keep = set()\n",
        "    for (u,v,k_) in keep_e:\n",
        "        nodes_keep.add(u); nodes_keep.add(v)\n",
        "    H = G_full.__class__(); H.add_nodes_from((n, G_full.nodes[n]) for n in nodes_keep)\n",
        "    for (u,v,k_,d) in edges:\n",
        "        if (u,v,k_) in keep_e: H.add_edge(u,v,key=k_,**d)\n",
        "    return H, k/m\n",
        "\n",
        "def build_prefix(G, keep):\n",
        "    if PREFIX_MODE == \"node_induced\":\n",
        "        return prefix_by_nodes_induced(G, keep)\n",
        "    elif PREFIX_MODE == \"node_guarded\":\n",
        "        return prefix_by_nodes_guarded(G, keep)\n",
        "    elif PREFIX_MODE == \"edge\":\n",
        "        return prefix_by_edges(G, keep)\n",
        "    else:\n",
        "        raise ValueError(\"PREFIX_MODE must be 'node_induced', 'node_guarded', or 'edge'.\")\n",
        "\n",
        "# ------------------ Feature helpers ------------------\n",
        "def _safe_div(a,b): return float(a)/float(b) if (b not in (None,0)) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0 or np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x))\n",
        "    n = x.size\n",
        "    cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0, deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _centralization(scores):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float)\n",
        "    vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), float(top.sum()/ssum if ssum>0 else 0.0)\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr)\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hd = nx.DiGraph(); Hd.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hd.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hd, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs)\n",
        "    auth_top, auth_top_ratio = _centralization(auths)\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    ts = []\n",
        "    for idx, (_,_,d) in enumerate(G.edges(data=True)):\n",
        "        ts.append(_edge_time_or_idx(d, idx))\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    tmin, tmax = ts[0], ts[-1]\n",
        "    span = max(1e-9, (tmax - tmin))\n",
        "    t_cut = tmin + 0.8*span\n",
        "    frac_last = (ts >= t_cut).mean()\n",
        "    return dict(prefix_burst=float(frac_last), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full_DYNAMIC(H: nx.Graph, keep_frac: float, S_full: dict, R_full: dict) -> pd.Series:\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features_DYNAMIC(H)\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        frac_nodes_of_full = _safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full = _safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel        = _safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel            = _safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel       = _safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel   = _safe_div(R_p[\"pr_top_ratio\"], R_full[\"pr_top_ratio\"]) if R_full[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel  = _safe_div(R_p[\"hub_top_ratio\"], R_full[\"hub_top_ratio\"]) if R_full[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel = _safe_div(R_p[\"auth_top_ratio\"], R_full[\"auth_top_ratio\"]) if R_full[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead        = _safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead        = _safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep         = S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ------------------ Models ------------------\n",
        "def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ------------------ Split ------------------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=0.30, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ------------------ Precompute full-graph packs ------------------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    return _basic_stats(G), _role_features_DYNAMIC(G)\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ------------------ TRAIN (prefix-augmented) ------------------\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i])\n",
        "    S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = build_prefix(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "\n",
        "for name in MODELS:\n",
        "    MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ------------------ Calibration (per keep) with α_train & auto-orientation ------------------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        cands = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = cands[np.argmin(fpr[ok][cands])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = build_prefix(Gfull, keep)\n",
        "        xs.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:3]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ------------------ TEST: collect per-keep scores, then monotone ------------------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = build_prefix(Gfull, keep)\n",
        "        xt.append(feature_row_prefix_vs_full_DYNAMIC(H, actual, S_full, R_full).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "records=[]\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    P = np.vstack([SCORES[name][k] for k in keeps_sorted])   # [K, N]\n",
        "    YH = (P > ths[:,None]).astype(int)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]\n",
        "        yhat = YH_mon[kk]\n",
        "        is_pos = (yt==1); is_neg=(yt==0)\n",
        "        det = (yhat[is_pos]==1).mean() if is_pos.any() else np.nan\n",
        "        fpr = (yhat[is_neg]==1).mean() if is_neg.any() else np.nan\n",
        "        auc = _safe_auc(yt, P[kk])\n",
        "        records.append(dict(keep_frac=keep, model=name, det_rate=det, fpr=fpr, auc=auc))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "print(\"\\n==== Early Detection — DetRate (monotone, α-margin calibrated) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"det_rate\").round(3))\n",
        "print(\"\\n==== Early Detection — FPR (monotone) — target α≈{:.2f} ====\".format(ALPHA))\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"fpr\").round(3))\n",
        "print(\"\\n==== Early Detection — AUC (threshold-free) ====\")\n",
        "print(res.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"auc\").round(3))\n",
        "\n",
        "# ------------------ Sanity: nodes vs edges kept on TEST ------------------\n",
        "rows=[]\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual = build_prefix(Gfull, keep)\n",
        "        n_full = Gfull.number_of_nodes(); n_pref = H.number_of_nodes()\n",
        "        m_full = Gfull.number_of_edges();  m_pref = H.number_of_edges()\n",
        "        rows.append(dict(keep=keep,\n",
        "                         frac_nodes_kept=(n_pref/n_full) if n_full>0 else np.nan,\n",
        "                         frac_edges_kept=(m_pref/m_full) if m_full>0 else np.nan,\n",
        "                         nodes_cut=max(n_full - n_pref, 0)))\n",
        "df_chk = pd.DataFrame(rows)\n",
        "summ = df_chk.groupby(\"keep\").agg(\n",
        "    graphs=(\"nodes_cut\",\"count\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(3)\n",
        "print(\"\\n==== Prefix coverage sanity on TEST (mode='{}') ====\".format(PREFIX_MODE))\n",
        "print(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8nUgUtJyhQ_",
        "outputId": "4f6fbcd0-026d-488f-9a2c-13ced586102b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 774 graphs from glob. Class balance: [376 398]\n",
            "[TRAIN] Fit with 1732 prefix samples (from 433 graphs). Features: 29\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.2: (0.516, 0.0, 0.0), 0.4: (0.857, 0.022, 0.667), 0.6: (0.844, 0.022, 0.646)} ...\n",
            "  LogReg: {0.2: (0.542, 0.011, 0.042), 0.4: (0.85, 0.022, 0.667), 0.6: (0.856, 0.033, 0.688)} ...\n",
            "  RF: {0.2: (0.528, 0.0, 0.0), 0.4: (0.814, 0.011, 0.542), 0.6: (0.782, 0.022, 0.604)} ...\n",
            "\n",
            "==== Early Detection — DetRate (monotone, α-margin calibrated) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.000   0.000  0.000\n",
            "0.4        0.638   0.625  0.475\n",
            "0.6        0.638   0.650  0.612\n",
            "0.8        0.638   0.662  0.662\n",
            "1.0        0.638   0.662  0.662\n",
            "\n",
            "==== Early Detection — FPR (monotone) — target α≈0.05 ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.000   0.000  0.000\n",
            "0.4        0.093   0.067  0.000\n",
            "0.6        0.093   0.133  0.027\n",
            "0.8        0.093   0.133  0.093\n",
            "1.0        0.107   0.133  0.120\n",
            "\n",
            "==== Early Detection — AUC (threshold-free) ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.543   0.542  0.514\n",
            "0.4        0.787   0.785  0.760\n",
            "0.6        0.796   0.801  0.800\n",
            "0.8        0.772   0.809  0.800\n",
            "1.0        0.782   0.809  0.786\n",
            "\n",
            "==== Prefix coverage sanity on TEST (mode='node_induced') ====\n",
            "      graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                  \n",
            "0.2      155            0.199                0.2            0.003              0.000               4.0\n",
            "0.4      155            0.398                0.4            0.156              0.154               3.0\n",
            "0.6      155            0.602                0.6            0.467              0.462               2.0\n",
            "0.8      155            0.801                0.8            0.696              0.692               1.0\n",
            "1.0      155            1.000                1.0            1.000              1.000               0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Extra metrics per keep: accuracy, precision, recall, specificity, F1\n",
        "# Uses your calibrated thresholds + monotone decisions (YH_mon)\n",
        "# ===========================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def _safe_div(a, b):\n",
        "    return (a / b) if b not in (0, None) else 0.0\n",
        "\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "records_metrics = []\n",
        "\n",
        "for name in MODELS:\n",
        "    # thresholds per keep\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    # K x N score matrix\n",
        "    P = np.vstack([SCORES[name][k] for k in keeps_sorted])  # shape [K, N]\n",
        "    # hard decisions by keep (strict > θ; ties negative)\n",
        "    YH = (P > ths[:, None]).astype(int)\n",
        "    # enforce monotonicity over keeps (once 1, always 1)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt   = YTEST[keep].astype(int)\n",
        "        yhat = YH_mon[kk]\n",
        "\n",
        "        tp = int(np.sum((yhat == 1) & (yt == 1)))\n",
        "        tn = int(np.sum((yhat == 0) & (yt == 0)))\n",
        "        fp = int(np.sum((yhat == 1) & (yt == 0)))\n",
        "        fn = int(np.sum((yhat == 0) & (yt == 1)))\n",
        "\n",
        "        acc  = _safe_div(tp + tn, tp + tn + fp + fn)\n",
        "        prec = _safe_div(tp, tp + fp)\n",
        "        rec  = _safe_div(tp, tp + fn)  # aka TPR / sensitivity / detection rate\n",
        "        spec = _safe_div(tn, tn + fp)  # specificity = 1 - FPR\n",
        "        f1   = _safe_div(2 * prec * rec, (prec + rec)) if (prec + rec) > 0 else 0.0\n",
        "\n",
        "        records_metrics.append(dict(\n",
        "            keep_frac=keep, model=name,\n",
        "            tp=tp, fp=fp, tn=tn, fn=fn,\n",
        "            accuracy=acc, precision=prec, recall=rec, specificity=spec, f1=f1\n",
        "        ))\n",
        "\n",
        "dfm = pd.DataFrame(records_metrics)\n",
        "\n",
        "print(\"\\n==== Accuracy by keep ====\")\n",
        "print(dfm.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"accuracy\").round(3))\n",
        "\n",
        "print(\"\\n==== Precision by keep ====\")\n",
        "print(dfm.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"precision\").round(3))\n",
        "\n",
        "print(\"\\n==== Recall (Detection Rate) by keep ====\")\n",
        "print(dfm.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"recall\").round(3))\n",
        "\n",
        "print(\"\\n==== Specificity (1 - FPR) by keep ====\")\n",
        "print(dfm.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"specificity\").round(3))\n",
        "\n",
        "print(\"\\n==== F1 by keep ====\")\n",
        "print(dfm.pivot_table(index=\"keep_frac\", columns=\"model\", values=\"f1\").round(3))\n",
        "\n",
        "# (Optional) show confusion-matrix counts too\n",
        "print(\"\\n==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\")\n",
        "print(dfm.sort_values([\"keep_frac\",\"model\"]).head(15))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVkMI3gZyhXT",
        "outputId": "0ad4af75-fe6a-4eff-efda-c2f57648bd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==== Accuracy by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.484   0.484  0.484\n",
            "0.4        0.768   0.774  0.729\n",
            "0.6        0.768   0.755  0.787\n",
            "0.8        0.768   0.761  0.781\n",
            "1.0        0.761   0.761  0.768\n",
            "\n",
            "==== Precision by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.000   0.000  0.000\n",
            "0.4        0.879   0.909  1.000\n",
            "0.6        0.879   0.839  0.961\n",
            "0.8        0.879   0.841  0.883\n",
            "1.0        0.864   0.841  0.855\n",
            "\n",
            "==== Recall (Detection Rate) by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.000   0.000  0.000\n",
            "0.4        0.638   0.625  0.475\n",
            "0.6        0.638   0.650  0.612\n",
            "0.8        0.638   0.662  0.662\n",
            "1.0        0.638   0.662  0.662\n",
            "\n",
            "==== Specificity (1 - FPR) by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        1.000   1.000  1.000\n",
            "0.4        0.907   0.933  1.000\n",
            "0.6        0.907   0.867  0.973\n",
            "0.8        0.907   0.867  0.907\n",
            "1.0        0.893   0.867  0.880\n",
            "\n",
            "==== F1 by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.000   0.000  0.000\n",
            "0.4        0.739   0.741  0.644\n",
            "0.6        0.739   0.732  0.748\n",
            "0.8        0.739   0.741  0.757\n",
            "1.0        0.734   0.741  0.746\n",
            "\n",
            "==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\n",
            "    keep_frac   model  tp  fp  tn  fn  accuracy  precision  recall  specificity        f1\n",
            "0         0.2     HGB   0   0  75  80  0.483871   0.000000  0.0000     1.000000  0.000000\n",
            "5         0.2  LogReg   0   0  75  80  0.483871   0.000000  0.0000     1.000000  0.000000\n",
            "10        0.2      RF   0   0  75  80  0.483871   0.000000  0.0000     1.000000  0.000000\n",
            "1         0.4     HGB  51   7  68  29  0.767742   0.879310  0.6375     0.906667  0.739130\n",
            "6         0.4  LogReg  50   5  70  30  0.774194   0.909091  0.6250     0.933333  0.740741\n",
            "11        0.4      RF  38   0  75  42  0.729032   1.000000  0.4750     1.000000  0.644068\n",
            "2         0.6     HGB  51   7  68  29  0.767742   0.879310  0.6375     0.906667  0.739130\n",
            "7         0.6  LogReg  52  10  65  28  0.754839   0.838710  0.6500     0.866667  0.732394\n",
            "12        0.6      RF  49   2  73  31  0.787097   0.960784  0.6125     0.973333  0.748092\n",
            "3         0.8     HGB  51   7  68  29  0.767742   0.879310  0.6375     0.906667  0.739130\n",
            "8         0.8  LogReg  53  10  65  27  0.761290   0.841270  0.6625     0.866667  0.741259\n",
            "13        0.8      RF  53   7  68  27  0.780645   0.883333  0.6625     0.906667  0.757143\n",
            "4         1.0     HGB  51   8  67  29  0.761290   0.864407  0.6375     0.893333  0.733813\n",
            "9         1.0  LogReg  53  10  65  27  0.761290   0.841270  0.6625     0.866667  0.741259\n",
            "14        1.0      RF  53   9  66  27  0.767742   0.854839  0.6625     0.880000  0.746479\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cfNLmvxlbWFw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f35VphjYg0r4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# GLUE + ATTRS + WRAPPERS  (DROP-IN)\n",
        "# ===========================\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "# ---- 1) Choose prefix mode (default: edge) ----\n",
        "MODE_PREFIX = \"edge\"   # or \"node\"\n",
        "\n",
        "# Minimal node-prefix if you don't already have one\n",
        "if MODE_PREFIX == \"node\" and \"prefix_by_nodes\" not in globals():\n",
        "    def prefix_by_nodes(G_full, keep_frac: float):\n",
        "        import numpy as np, networkx as nx\n",
        "        def _edge_time(d):\n",
        "            for k in (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\"):\n",
        "                if k in d and d[k] is not None:\n",
        "                    try: return float(d[k])\n",
        "                    except: pass\n",
        "            return np.inf\n",
        "        if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "        if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "        node_t = {n: np.inf for n in G_full.nodes()}\n",
        "        for u,v,d in G_full.edges(data=True):\n",
        "            t = _edge_time(d)\n",
        "            if np.isfinite(t):\n",
        "                if t < node_t[u]: node_t[u] = t\n",
        "                if t < node_t[v]: node_t[v] = t\n",
        "        order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "        k = max(1, int(round(len(order)*max(0.0, min(1.0, keep_frac)))))\n",
        "        return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "PREFIX_FN = prefix_by_edges if MODE_PREFIX == \"edge\" else prefix_by_nodes\n",
        "\n",
        "# ---- 2) Pick the feature builder that already exists in your notebook ----\n",
        "if \"feature_row_prefix_vs_full_edge_DYNAMIC\" in globals():\n",
        "    FEATURE_FN = feature_row_prefix_vs_full_edge_DYNAMIC\n",
        "elif \"feature_row_prefix_vs_full_edge\" in globals():\n",
        "    FEATURE_FN = feature_row_prefix_vs_full_edge\n",
        "else:\n",
        "    # Minimal fallback (shouldn't be used in your final runs)\n",
        "    def FEATURE_FN(H, keep_frac, S_full, R_full):\n",
        "        import numpy as np, pandas as pd, networkx as nx\n",
        "        n = H.number_of_nodes(); m = H.number_of_edges()\n",
        "        try:\n",
        "            comps = list(nx.weakly_connected_components(H)) if isinstance(H,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(H))\n",
        "        except Exception:\n",
        "            comps = []\n",
        "        largest = max((len(c) for c in comps), default=0)\n",
        "        out = dict(\n",
        "            n_nodes=float(n), n_edges=float(m),\n",
        "            density=(nx.density(H) if n>1 else 0.0),\n",
        "            avg_deg=(float(np.mean([d for _,d in H.degree()])) if n>0 else 0.0),\n",
        "            n_components=float(len(comps)),\n",
        "            largest_cc=float(largest), largest_cc_frac=(largest/n if n>0 else 0.0),\n",
        "            edges_per_node=(m/n if n>0 else 0.0),\n",
        "            keep_frac=float(keep_frac),\n",
        "        )\n",
        "        return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---- 3) Static ATTRS (optional; safe if df_num missing) ----\n",
        "if \"df_num\" in globals():\n",
        "    file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\")), \"file\")\n",
        "    if \"file\" not in df_num.columns:\n",
        "        raise RuntimeError(\"df_num must contain 'file' for alignment.\")\n",
        "    _aligned_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "\n",
        "    _num_cols = [c for c in _aligned_num.columns\n",
        "                 if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                 and pd.api.types.is_numeric_dtype(_aligned_num[c])]\n",
        "\n",
        "    Z = _aligned_num[_num_cols].replace([np.inf, -np.inf], np.nan)\n",
        "    med = Z.median(numeric_only=True); Z = Z.fillna(med)\n",
        "    q01 = Z.quantile(0.01, numeric_only=True); q99 = Z.quantile(0.99, numeric_only=True)\n",
        "    for c in Z.columns:\n",
        "        lo = q01.get(c, None); hi = q99.get(c, None)\n",
        "        if lo is not None and hi is not None and np.isfinite([lo,hi]).all():\n",
        "            Z[c] = Z[c].clip(lo, hi)\n",
        "    ATTRS_df = Z.astype(\"float64\").copy()\n",
        "    ATTRS_df.columns = [f\"attr__{c}\" for c in ATTRS_df.columns]\n",
        "else:\n",
        "    print(\"[WARN] df_num not found; proceeding without static ATTRS.\")\n",
        "    ATTRS_df = pd.DataFrame(index=range(len(paths)))  # empty, harmless\n",
        "\n",
        "def _attrs_row(i: int) -> pd.Series:\n",
        "    if ATTRS_df.shape[0] == 0:\n",
        "        return pd.Series(dtype=\"float64\")\n",
        "    return ATTRS_df.iloc[i]\n",
        "\n",
        "# ---- 4) Unified wrapper used everywhere ----\n",
        "def feature_row_with_ATTRS(H, keep_frac, S_full, R_full, i_index: int):\n",
        "    base = FEATURE_FN(H, keep_frac, S_full, R_full)   # uses dynamic roles if you have it\n",
        "    return pd.concat([base, _attrs_row(i_index)], axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_ge-XPpg0u7",
        "outputId": "e1d30da0-beaa-4a3f-8920-6bb7dc0d1217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[WARN] df_num not found; proceeding without static ATTRS.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN\n",
        "X_fit_rows=[]; y_fit=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = PREFIX_FN(Gfull, kf)\n",
        "        X_fit_rows.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i) )\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "for name in MODELS: MODELS[name].fit(X_fit, y_fit)\n",
        "\n",
        "# CALIB\n",
        "theta_map = {name:{} for name in MODELS}; flip_map = {name:{} for name in MODELS}; calib_rep={name:{} for name in MODELS}\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xs.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA) )\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5: s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th; flip_map[name][keep] = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "# TEST\n",
        "SCORES = {name:{} for name in MODELS}; YTEST={}\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xt.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA) )\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt); YTEST[keep]=yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n"
      ],
      "metadata": {
        "id": "g_VuhBk2bWJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# TRAIN + CALIBRATE + TEST + PRINT RESULTS\n",
        "# (works with the GLUE cell you just ran)\n",
        "# ============================================\n",
        "import numpy as np, pandas as pd\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# --------- Defaults if not already defined ---------\n",
        "KEEP_FRACS_TRAIN = globals().get(\"KEEP_FRACS_TRAIN\", [0.20, 0.40, 0.60, 1.00])\n",
        "KEEP_FRACS_EVAL  = globals().get(\"KEEP_FRACS_EVAL\",  [0.20, 0.40, 0.60, 0.80, 1.00])\n",
        "TARGET_SPEC      = globals().get(\"TARGET_SPEC\", 0.95)             # public specificity target\n",
        "ALPHA_MARGIN     = globals().get(\"ALPHA_MARGIN\", 0.01)            # be stricter on train\n",
        "ALPHA            = 1.0 - TARGET_SPEC\n",
        "ALPHA_TRAIN      = max(0.0, ALPHA - ALPHA_MARGIN)                 # stricter cap on calib\n",
        "\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        # maximize TPR with tie-break on lower FPR\n",
        "        idxs = np.flatnonzero(ok)\n",
        "        tpr_ok, fpr_ok = tpr[idxs], fpr[idxs]\n",
        "        best = idxs[np.argmax(tpr_ok)]\n",
        "        ties = idxs[np.where(tpr_ok == tpr[best])[0]]\n",
        "        pick = ties[np.argmin(fpr[ties])]\n",
        "        k = pick\n",
        "    else:\n",
        "        # no point hits the cap; choose min FPR then max TPR among those\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "# --------- TRAIN (prefix-augmented) ---------\n",
        "X_fit_rows, y_fit = [], []\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = PREFIX_FN(Gfull, kf)\n",
        "        X_fit_rows.append(feature_row_with_ATTRS(H, actual, S_full, R_full, i))\n",
        "        y_fit.append(y[i])\n",
        "X_fit = pd.DataFrame(X_fit_rows); y_fit = np.array(y_fit)\n",
        "SCHEMA = X_fit.columns.tolist()\n",
        "for name in MODELS: MODELS[name].fit(X_fit, y_fit)\n",
        "print(f\"[TRAIN] Fit with {len(X_fit)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# --------- CALIBRATION (per keep; auto-orient; stricter alpha) ---------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs, yc = [], []\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xs.append(feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped = False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; auc = 1.0 - auc; flipped = True\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th\n",
        "        flip_map[name][keep]  = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3))\n",
        "            for k in KEEP_FRACS_EVAL[:min(4,len(KEEP_FRACS_EVAL))]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# --------- TEST: collect per-keep scores ---------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}\n",
        "XTEST_ROWS = {}   # optional: keep per-keep DF if you want to inspect\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt, yt = [], []\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xt.append(feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    XTEST_ROWS[keep] = Xt\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# --------- Monotone decisions + metrics ---------\n",
        "def _conf_counts(y_true, y_hat):\n",
        "    tp = int(((y_hat==1)&(y_true==1)).sum())\n",
        "    fp = int(((y_hat==1)&(y_true==0)).sum())\n",
        "    tn = int(((y_hat==0)&(y_true==0)).sum())\n",
        "    fn = int(((y_hat==0)&(y_true==1)).sum())\n",
        "    return tp, fp, tn, fn\n",
        "\n",
        "records_full = []\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    P   = np.vstack([SCORES[name][k] for k in keeps_sorted])  # [K, N]\n",
        "    YH  = (P > ths[:,None]).astype(int)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)                # once 1 → always 1\n",
        "\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]\n",
        "        yhat = YH_mon[kk]\n",
        "        tp, fp, tn, fn = _conf_counts(yt, yhat)\n",
        "        acc = (tp+tn) / max(len(yt),1)\n",
        "        prec = (tp / max(tp+fp,1)) if (tp+fp)>0 else 0.0\n",
        "        rec  = (tp / max(tp+fn,1)) if (tp+fn)>0 else 0.0\n",
        "        spc  = (tn / max(tn+fp,1)) if (tn+fp)>0 else 0.0\n",
        "        f1   = (2*prec*rec / max(prec+rec,1e-12)) if (prec+rec)>0 else 0.0\n",
        "        auc  = _safe_auc(yt, P[kk])\n",
        "        fpr  = 1.0 - spc\n",
        "        records_full.append(dict(\n",
        "            keep_frac=float(keep), model=name, tp=tp, fp=fp, tn=tn, fn=fn,\n",
        "            accuracy=acc, precision=prec, recall=rec, specificity=spc, f1=f1,\n",
        "            auc=auc, fpr=fpr\n",
        "        ))\n",
        "\n",
        "res = pd.DataFrame(records_full)\n",
        "\n",
        "# --------- Pretty prints ---------\n",
        "def _ppivot(col):\n",
        "    tbl = res.pivot_table(index=\"keep_frac\", columns=\"model\", values=col).round(4)\n",
        "    print(f\"\\n==== {col.replace('_',' ').title()} by keep ====\")\n",
        "    print(tbl)\n",
        "    return tbl\n",
        "\n",
        "_ = _ppivot(\"auc\")\n",
        "_ = _ppivot(\"fpr\")\n",
        "_ = _ppivot(\"accuracy\")\n",
        "_ = _ppivot(\"precision\")\n",
        "_ = _ppivot(\"recall\")        # == Detection Rate\n",
        "_ = _ppivot(\"specificity\")\n",
        "_ = _ppivot(\"f1\")\n",
        "\n",
        "print(\"\\n==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\")\n",
        "cols = [\"keep_frac\",\"model\",\"tp\",\"fp\",\"tn\",\"fn\",\"accuracy\",\"precision\",\"recall\",\"specificity\",\"f1\"]\n",
        "print(res[cols].sort_values([\"keep_frac\",\"model\"]).head(15).to_string(index=False))\n",
        "\n",
        "# --------- Prefix coverage sanity (what the prefix actually keeps) ---------\n",
        "rows_cov = []\n",
        "for keep in keeps_sorted:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        n_full = Gfull.number_of_nodes(); m_full = Gfull.number_of_edges()\n",
        "        n_pref = H.number_of_nodes();     m_pref = H.number_of_edges()\n",
        "        rows_cov.append(dict(\n",
        "            keep=keep,\n",
        "            frac_nodes_kept=(n_pref/n_full if n_full>0 else np.nan),\n",
        "            frac_edges_kept=(m_pref/m_full if m_full>0 else np.nan),\n",
        "            nodes_cut=max(n_full - n_pref, 0)\n",
        "        ))\n",
        "df_cov = pd.DataFrame(rows_cov)\n",
        "summ = df_cov.groupby(\"keep\").agg(\n",
        "    graphs=(\"frac_nodes_kept\",\"count\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(4)\n",
        "print(f\"\\n==== Prefix coverage sanity on TEST (mode='{ 'edge' if PREFIX_FN==prefix_by_edges else 'node' }') ====\")\n",
        "print(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdQWk7Y8bWL-",
        "outputId": "8f6c331c-c4d1-4b16-d57e-718cb20d08b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] Fit with 1732 prefix samples (from 433 graphs). Features: 9\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.2: (0.528, 0.0, 0.0), 0.4: (0.517, 0.0, 0.0), 0.6: (0.517, 0.0, 0.0), 0.8: (0.517, 0.0, 0.0)} ...\n",
            "  LogReg: {0.2: (0.528, 0.0, 0.0), 0.4: (0.528, 0.0, 0.0), 0.6: (0.528, 0.0, 0.0), 0.8: (0.528, 0.0, 0.0)} ...\n",
            "  RF: {0.2: (0.528, 0.0, 0.0), 0.4: (0.517, 0.0, 0.0), 0.6: (0.517, 0.0, 0.0), 0.8: (0.517, 0.0, 0.0)} ...\n",
            "\n",
            "==== Auc by keep ====\n",
            "model         HGB  LogReg      RF\n",
            "keep_frac                        \n",
            "0.2        0.5145  0.5145  0.5145\n",
            "0.4        0.5012  0.5145  0.5012\n",
            "0.6        0.5012  0.5145  0.5012\n",
            "0.8        0.5012  0.5145  0.5012\n",
            "1.0        0.5145  0.5145  0.5145\n",
            "\n",
            "==== Fpr by keep ====\n",
            "model      HGB  LogReg   RF\n",
            "keep_frac                  \n",
            "0.2        0.0     0.0  0.0\n",
            "0.4        0.0     0.0  0.0\n",
            "0.6        0.0     0.0  0.0\n",
            "0.8        0.0     0.0  0.0\n",
            "1.0        0.0     0.0  0.0\n",
            "\n",
            "==== Accuracy by keep ====\n",
            "model         HGB  LogReg      RF\n",
            "keep_frac                        \n",
            "0.2        0.4839  0.4839  0.4839\n",
            "0.4        0.4839  0.4839  0.4839\n",
            "0.6        0.4839  0.4839  0.4839\n",
            "0.8        0.4839  0.4839  0.4839\n",
            "1.0        0.4839  0.4839  0.4839\n",
            "\n",
            "==== Precision by keep ====\n",
            "model      HGB  LogReg   RF\n",
            "keep_frac                  \n",
            "0.2        0.0     0.0  0.0\n",
            "0.4        0.0     0.0  0.0\n",
            "0.6        0.0     0.0  0.0\n",
            "0.8        0.0     0.0  0.0\n",
            "1.0        0.0     0.0  0.0\n",
            "\n",
            "==== Recall by keep ====\n",
            "model      HGB  LogReg   RF\n",
            "keep_frac                  \n",
            "0.2        0.0     0.0  0.0\n",
            "0.4        0.0     0.0  0.0\n",
            "0.6        0.0     0.0  0.0\n",
            "0.8        0.0     0.0  0.0\n",
            "1.0        0.0     0.0  0.0\n",
            "\n",
            "==== Specificity by keep ====\n",
            "model      HGB  LogReg   RF\n",
            "keep_frac                  \n",
            "0.2        1.0     1.0  1.0\n",
            "0.4        1.0     1.0  1.0\n",
            "0.6        1.0     1.0  1.0\n",
            "0.8        1.0     1.0  1.0\n",
            "1.0        1.0     1.0  1.0\n",
            "\n",
            "==== F1 by keep ====\n",
            "model      HGB  LogReg   RF\n",
            "keep_frac                  \n",
            "0.2        0.0     0.0  0.0\n",
            "0.4        0.0     0.0  0.0\n",
            "0.6        0.0     0.0  0.0\n",
            "0.8        0.0     0.0  0.0\n",
            "1.0        0.0     0.0  0.0\n",
            "\n",
            "==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\n",
            " keep_frac  model  tp  fp  tn  fn  accuracy  precision  recall  specificity  f1\n",
            "       0.2    HGB   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.2 LogReg   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.2     RF   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.4    HGB   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.4 LogReg   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.4     RF   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.6    HGB   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.6 LogReg   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.6     RF   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.8    HGB   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.8 LogReg   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       0.8     RF   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       1.0    HGB   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       1.0 LogReg   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "       1.0     RF   0   0  75  80  0.483871        0.0     0.0          1.0 0.0\n",
            "\n",
            "==== Prefix coverage sanity on TEST (mode='edge') ====\n",
            "      graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                  \n",
            "0.2      155           0.7932                0.8           0.2296             0.2308               1.0\n",
            "0.4      155           0.9923                1.0           0.3852             0.3846               0.0\n",
            "0.6      155           0.9949                1.0           0.6148             0.6154               0.0\n",
            "0.8      155           0.9975                1.0           0.7704             0.7692               0.0\n",
            "1.0      155           1.0000                1.0           1.0000             1.0000               0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpvHLhapjZ8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# PATCH: dynamic features + node-governed eval\n",
        "# ===============================================\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "from pathlib import Path\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "# ------------------ CONFIG (matches your \"expected\") ------------------\n",
        "KEEP_FRACS_TRAIN = [0.20, 0.40, 0.60, 1.00]\n",
        "KEEP_FRACS_EVAL  = [0.20, 0.40, 0.60, 0.80, 1.00]\n",
        "TARGET_SPEC      = 0.95\n",
        "ALPHA_MARGIN     = 0.01\n",
        "ALPHA            = 1.0 - TARGET_SPEC\n",
        "ALPHA_TRAIN      = max(0.0, ALPHA - ALPHA_MARGIN)\n",
        "RANDOM_STATE     = 42\n",
        "N_SPLITS         = 5\n",
        "TEST_FOLD        = 0\n",
        "\n",
        "# ------------------ UTILS ------------------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time(d):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return np.inf\n",
        "\n",
        "def _safe_div(a,b): return float(a)/float(b) if (b not in (None,0)) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float)\n",
        "    x = x[np.isfinite(x)]\n",
        "    if x.size == 0 or np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x))\n",
        "    n = x.size\n",
        "    cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "# ------------------ PREFIX: NODE-GOVERNED ------------------\n",
        "def prefix_by_nodes(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "    node_t = {n: np.inf for n in G_full.nodes()}\n",
        "    for u,v,d in G_full.edges(data=True):\n",
        "        t = _edge_time(d)\n",
        "        if np.isfinite(t):\n",
        "            if t < node_t[u]: node_t[u] = t\n",
        "            if t < node_t[v]: node_t[v] = t\n",
        "    order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "    k = max(1, int(round(len(order)*max(0.0,min(1.0,keep_frac)))))\n",
        "    return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "PREFIX_FN = prefix_by_nodes  # <<< force node-governed\n",
        "\n",
        "# ------------------ FEATURES: dynamic roles + burst + relatives ------------------\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02  # ~2% of nodes, bounded\n",
        "\n",
        "def _centralization(scores):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float); vals = vals[np.isfinite(vals)]\n",
        "    if vals.size == 0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), (float(top.sum()/ssum) if ssum>0 else 0.0)\n",
        "\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0, deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr)\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hsimple = nx.DiGraph(); Hsimple.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hsimple.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hsimple, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs)\n",
        "    auth_top, auth_top_ratio = _centralization(auths)\n",
        "    return dict(\n",
        "        pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "        hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "        auth_top=auth_top, auth_top_ratio=auth_top_ratio\n",
        "    )\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    m = G.number_of_edges()\n",
        "    if m == 0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    ts=[]\n",
        "    for idx, (_,_,d) in enumerate(G.edges(data=True)):\n",
        "        t=None\n",
        "        for k in _TS_KEYS:\n",
        "            if k in d and d[k] is not None:\n",
        "                try: t=float(d[k]); break\n",
        "                except: pass\n",
        "        if t is None: t=float(idx)\n",
        "        ts.append(t)\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    tmin, tmax = ts[0], ts[-1]\n",
        "    span = max(1e-9, (tmax - tmin))\n",
        "    t_cut = tmin + 0.8*span\n",
        "    frac_last = (ts >= t_cut).mean()\n",
        "    return dict(prefix_burst=float(frac_last), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full_node_DYNAMIC(H, keep_frac, S_full, R_full):\n",
        "    S_p   = _basic_stats(H)\n",
        "    R_p   = _role_features_DYNAMIC(H)\n",
        "    B_p   = _temporal_burst_in_prefix(H)\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"], keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        # relatives to FULL\n",
        "        frac_nodes_of_full=_safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full=_safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel=_safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel=_safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel=_safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel=_safe_div(R_p[\"pr_top_ratio\"], R_full[\"pr_top_ratio\"]) if R_full[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel=_safe_div(R_p[\"hub_top_ratio\"], R_full[\"hub_top_ratio\"]) if R_full[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel=_safe_div(R_p[\"auth_top_ratio\"], R_full[\"auth_top_ratio\"]) if R_full[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead=_safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead=_safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep=S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# Select the feature fn\n",
        "FEATURE_FN = feature_row_prefix_vs_full_node_DYNAMIC\n",
        "\n",
        "# ------------------ DISCOVERY/LOADER (use your globals if present) ------------------\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = merged\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            P = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            if \"y\" in globals() and len(y)==len(P):\n",
        "                return P, np.asarray(y).astype(int), \"merged(y)\"\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(P): return P, yy, \"merged(col)\"\n",
        "    hits = list(Path(\".\").rglob(\"*.gpickle\")) + list(Path(\".\").rglob(\"*.gpkl\"))\n",
        "    if not hits: raise RuntimeError(\"No graphs found (.gpickle/.gpkl).\")\n",
        "    P = sorted(set(hits))\n",
        "    return P, np.array([_infer_label_from_path(str(p)) for p in P], int), \"glob\"\n",
        "\n",
        "if \"paths\" not in globals() or \"y\" not in globals():\n",
        "    paths, y, src = _discover()\n",
        "    mask = np.array([p.exists() for p in paths], bool)\n",
        "    paths = [p for p,ok in zip(paths,mask) if ok]; y = np.asarray(y)[mask]\n",
        "else:\n",
        "    src = \"provided\"\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ------------------ SPLITS ------------------\n",
        "outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=0.30, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ------------------ FULL-PACK (full-graph stats + full roles) ------------------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    S = _basic_stats(G)\n",
        "    R = _role_features_DYNAMIC(G)\n",
        "    return S, R\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ------------------ MODELS (if not already built) ------------------\n",
        "if \"MODELS\" not in globals():\n",
        "    def _mk_hgb(): return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))])\n",
        "    def _mk_lr():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"sc\", StandardScaler()), (\"clf\", LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=RANDOM_STATE))])\n",
        "    def _mk_rf():  return Pipeline([(\"imp\", SimpleImputer(strategy=\"median\")), (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1, class_weight=\"balanced_subsample\", random_state=RANDOM_STATE))])\n",
        "    MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ------------------ TRAIN (prefix-augmented, NODE) ------------------\n",
        "X_train_rows=[]; y_train=[]\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = PREFIX_FN(Gfull, kf)\n",
        "        X_train_rows.append(FEATURE_FN(H, actual, S_full, R_full))\n",
        "        y_train.append(y[i])\n",
        "X_train = pd.DataFrame(X_train_rows); y_train = np.array(y_train)\n",
        "SCHEMA = X_train.columns.tolist()\n",
        "for name in MODELS: MODELS[name].fit(X_train, y_train)\n",
        "print(f\"[TRAIN] Fit with {len(X_train)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ------------------ CALIBRATION (per keep; auto-orient; α_train) ------------------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idxs = np.flatnonzero(ok)\n",
        "        best = idxs[np.argmax(tpr[idxs])]\n",
        "        ties = idxs[tpr[idxs]==tpr[best]]\n",
        "        pick = ties[np.argmin(fpr[ties])]\n",
        "        k = pick\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr==minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs, yc = [], []\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xs.append(pd.Series(FEATURE_FN(H, actual, S_full, R_full)).reindex(SCHEMA))\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; auc = 1.0 - auc; flipped=True\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th; flip_map[name][keep] = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ------------------ TEST: scores per keep ------------------\n",
        "SCORES = {name:{} for name in MODELS}\n",
        "YTEST  = {}\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt, yt = [], []\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full, R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xt.append(pd.Series(FEATURE_FN(H, actual, S_full, R_full)).reindex(SCHEMA))\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt)\n",
        "    YTEST[keep] = yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# ------------------ Monotone decisions + metrics ------------------\n",
        "def _conf_counts(y_true, y_hat):\n",
        "    tp = int(((y_hat==1)&(y_true==1)).sum())\n",
        "    fp = int(((y_hat==1)&(y_true==0)).sum())\n",
        "    tn = int(((y_hat==0)&(y_true==0)).sum())\n",
        "    fn = int(((y_hat==0)&(y_true==1)).sum())\n",
        "    return tp, fp, tn, fn\n",
        "\n",
        "records=[]\n",
        "keeps_sorted = sorted(KEEP_FRACS_EVAL)\n",
        "for name in MODELS:\n",
        "    ths = np.array([theta_map[name][k] for k in keeps_sorted], float)\n",
        "    P   = np.vstack([SCORES[name][k] for k in keeps_sorted])  # [K,N]\n",
        "    YH  = (P > ths[:,None]).astype(int)\n",
        "    YH_mon = np.maximum.accumulate(YH, axis=0)\n",
        "\n",
        "    for kk, keep in enumerate(keeps_sorted):\n",
        "        yt = YTEST[keep]; yhat = YH_mon[kk]\n",
        "        tp, fp, tn, fn = _conf_counts(yt, yhat)\n",
        "        acc = (tp+tn) / max(len(yt),1)\n",
        "        prec = (tp / max(tp+fp,1)) if (tp+fp)>0 else 0.0\n",
        "        rec  = (tp / max(tp+fn,1)) if (tp+fn)>0 else 0.0\n",
        "        spc  = (tn / max(tn+fp,1)) if (tn+fp)>0 else 0.0\n",
        "        f1   = (2*prec*rec / max(prec+rec,1e-12)) if (prec+rec)>0 else 0.0\n",
        "        auc  = _safe_auc(yt, P[kk])\n",
        "        fpr  = 1.0 - spc\n",
        "        records.append(dict(keep_frac=keep, model=name, accuracy=acc, precision=prec,\n",
        "                            recall=rec, specificity=spc, f1=f1, auc=auc, fpr=fpr,\n",
        "                            tp=tp, fp=fp, tn=tn, fn=fn))\n",
        "\n",
        "res = pd.DataFrame(records)\n",
        "\n",
        "def _ppivot(col):\n",
        "    tbl = res.pivot_table(index=\"keep_frac\", columns=\"model\", values=col).round(3)\n",
        "    print(f\"\\n==== {col.replace('_',' ').title()} by keep ====\"); print(tbl); return tbl\n",
        "\n",
        "_ = _ppivot(\"accuracy\")\n",
        "_ = _ppivot(\"precision\")\n",
        "_ = _ppivot(\"recall\")\n",
        "_ = _ppivot(\"specificity\")\n",
        "_ = _ppivot(\"f1\")\n",
        "_ = _ppivot(\"auc\")\n",
        "_ = _ppivot(\"fpr\")\n",
        "\n",
        "print(\"\\n==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\")\n",
        "cols = [\"keep_frac\",\"model\",\"tp\",\"fp\",\"tn\",\"fn\",\"accuracy\",\"precision\",\"recall\",\"specificity\",\"f1\"]\n",
        "print(res[cols].sort_values([\"keep_frac\",\"model\"]).head(15).to_string(index=False))\n",
        "\n",
        "# ------------------ Prefix coverage sanity (node-governed) ------------------\n",
        "rows_cov=[]\n",
        "for keep in keeps_sorted:\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i])\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        n_full = Gfull.number_of_nodes(); m_full = Gfull.number_of_edges()\n",
        "        n_pref = H.number_of_nodes();     m_pref = H.number_of_edges()\n",
        "        rows_cov.append(dict(\n",
        "            keep=keep,\n",
        "            frac_nodes_kept=(n_pref/n_full if n_full>0 else np.nan),\n",
        "            frac_edges_kept=(m_pref/m_full if m_full>0 else np.nan),\n",
        "            nodes_cut=max(n_full-n_pref,0)\n",
        "        ))\n",
        "df_cov = pd.DataFrame(rows_cov)\n",
        "summ = df_cov.groupby(\"keep\").agg(\n",
        "    graphs=(\"frac_nodes_kept\",\"count\"),\n",
        "    nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "    nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "    edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "    edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "    nodes_cut_median=(\"nodes_cut\",\"median\")\n",
        ").round(3)\n",
        "print(\"\\n==== Prefix coverage sanity on TEST (mode='node_induced') ====\")\n",
        "print(summ)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGXTf_5hjZ_l",
        "outputId": "e38eaddc-ab91-4331-81d2-11594f27fe3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 774 graphs from provided. Class balance: [376 398]\n",
            "[TRAIN] Fit with 1732 prefix samples (from 433 graphs). Features: 29\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.2: (0.531, 0.033, 0.042), 0.4: (0.842, 0.022, 0.667), 0.6: (0.834, 0.022, 0.667), 0.8: (0.809, 0.011, 0.51)} ...\n",
            "  LogReg: {0.2: (0.542, 0.0, 0.0), 0.4: (0.85, 0.022, 0.667), 0.6: (0.856, 0.033, 0.688), 0.8: (0.828, 0.022, 0.656)} ...\n",
            "  RF: {0.2: (0.528, 0.0, 0.0), 0.4: (0.814, 0.011, 0.542), 0.6: (0.783, 0.022, 0.604), 0.8: (0.833, 0.033, 0.667)} ...\n",
            "\n",
            "==== Accuracy by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.477   0.484  0.484\n",
            "0.4        0.768   0.774  0.729\n",
            "0.6        0.768   0.755  0.787\n",
            "0.8        0.768   0.761  0.781\n",
            "1.0        0.768   0.755  0.774\n",
            "\n",
            "==== Precision by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.400   0.000  0.000\n",
            "0.4        0.867   0.909  1.000\n",
            "0.6        0.855   0.839  0.961\n",
            "0.8        0.855   0.841  0.883\n",
            "1.0        0.844   0.828  0.869\n",
            "\n",
            "==== Recall by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.025   0.000  0.000\n",
            "0.4        0.650   0.625  0.475\n",
            "0.6        0.662   0.650  0.612\n",
            "0.8        0.662   0.662  0.662\n",
            "1.0        0.675   0.662  0.662\n",
            "\n",
            "==== Specificity by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.960   1.000  1.000\n",
            "0.4        0.893   0.933  1.000\n",
            "0.6        0.880   0.867  0.973\n",
            "0.8        0.880   0.867  0.907\n",
            "1.0        0.867   0.853  0.893\n",
            "\n",
            "==== F1 by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.047   0.000  0.000\n",
            "0.4        0.743   0.741  0.644\n",
            "0.6        0.746   0.732  0.748\n",
            "0.8        0.746   0.741  0.757\n",
            "1.0        0.750   0.736  0.752\n",
            "\n",
            "==== Auc by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.474   0.481  0.514\n",
            "0.4        0.782   0.785  0.760\n",
            "0.6        0.796   0.801  0.800\n",
            "0.8        0.791   0.809  0.800\n",
            "1.0        0.802   0.795  0.786\n",
            "\n",
            "==== Fpr by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.040   0.000  0.000\n",
            "0.4        0.107   0.067  0.000\n",
            "0.6        0.120   0.133  0.027\n",
            "0.8        0.120   0.133  0.093\n",
            "1.0        0.133   0.147  0.107\n",
            "\n",
            "==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\n",
            " keep_frac  model  tp  fp  tn  fn  accuracy  precision  recall  specificity       f1\n",
            "       0.2    HGB   2   3  72  78  0.477419   0.400000  0.0250     0.960000 0.047059\n",
            "       0.2 LogReg   0   0  75  80  0.483871   0.000000  0.0000     1.000000 0.000000\n",
            "       0.2     RF   0   0  75  80  0.483871   0.000000  0.0000     1.000000 0.000000\n",
            "       0.4    HGB  52   8  67  28  0.767742   0.866667  0.6500     0.893333 0.742857\n",
            "       0.4 LogReg  50   5  70  30  0.774194   0.909091  0.6250     0.933333 0.740741\n",
            "       0.4     RF  38   0  75  42  0.729032   1.000000  0.4750     1.000000 0.644068\n",
            "       0.6    HGB  53   9  66  27  0.767742   0.854839  0.6625     0.880000 0.746479\n",
            "       0.6 LogReg  52  10  65  28  0.754839   0.838710  0.6500     0.866667 0.732394\n",
            "       0.6     RF  49   2  73  31  0.787097   0.960784  0.6125     0.973333 0.748092\n",
            "       0.8    HGB  53   9  66  27  0.767742   0.854839  0.6625     0.880000 0.746479\n",
            "       0.8 LogReg  53  10  65  27  0.761290   0.841270  0.6625     0.866667 0.741259\n",
            "       0.8     RF  53   7  68  27  0.780645   0.883333  0.6625     0.906667 0.757143\n",
            "       1.0    HGB  54  10  65  26  0.767742   0.843750  0.6750     0.866667 0.750000\n",
            "       1.0 LogReg  53  11  64  27  0.754839   0.828125  0.6625     0.853333 0.736111\n",
            "       1.0     RF  53   8  67  27  0.774194   0.868852  0.6625     0.893333 0.751773\n",
            "\n",
            "==== Prefix coverage sanity on TEST (mode='node_induced') ====\n",
            "      graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                  \n",
            "0.2      155            0.199                0.2            0.003              0.000               4.0\n",
            "0.4      155            0.398                0.4            0.156              0.154               3.0\n",
            "0.6      155            0.602                0.6            0.467              0.462               2.0\n",
            "0.8      155            0.801                0.8            0.696              0.692               1.0\n",
            "1.0      155            1.000                1.0            1.000              1.000               0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# your “COMBINED” table = a static classifier trained on (attrs + your full-graph structure pack), with class weights and a threshold picked to hit target_recall≈0.85.\n",
        "\n",
        "# the early-detection notebook = a prefix detector trained on prefix-derived topology/role/burst features, then thresholded by an FPR cap (~5%) and made monotone over keeps.\n",
        "\n",
        "# Even at keep=1.0, those aren’t the same pipeline: the features aren’t the same, the split isn’t necessarily the same, and the thresholding rule is totally different (FPR≤α vs “hit recall 0.85”). That’s why your 1.0 row doesn’t equal the COMBINED row.\n",
        "\n",
        "# To make 1.0 “look like” your COMBINED results, you need to align three things:\n",
        "\n",
        "# Same features — append your static df_num attrs (and, if you want, the same full-graph structure pack you used in COMBINED).\n",
        "\n",
        "# Same split — if COMBINED used group-aware CV, reuse the exact split (or fix the same groups and fold).\n",
        "\n",
        "# Same threshold policy — instead of FPR≤α, pick a threshold that maximizes F1 or hits a target recall (e.g., 0.85) on the calibration fold."
      ],
      "metadata": {
        "id": "pPMP_F2lmYCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================\n",
        "# Early-Detection vs COMBINED@1.0 (aligned)\n",
        "# - prefix + role/burst features (edge/node)\n",
        "# - static ATTRS from df_num (aligned to merged)\n",
        "# - same split (group-aware if available; or supply tr_idx_fixed/te_idx_fixed)\n",
        "# - thresholds: (A) α-FPR cap per keep, (B) COMBINED-like target-recall at keep=1.0\n",
        "# ===========================================\n",
        "import os, glob, warnings, math\n",
        "from pathlib import Path\n",
        "import numpy as np, pandas as pd, networkx as nx\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_recall_fscore_support\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "\n",
        "# ------------------ CONFIG ------------------\n",
        "# Prefix policy\n",
        "MODE_PREFIX = \"node\"               # \"edge\" or \"node\" — set \"node\" to match your node-induced coverage sanity\n",
        "KEEP_FRACS_EVAL  = [0.20,0.40,0.60,0.80,1.00]\n",
        "KEEP_FRACS_TRAIN = [0.20,0.40,0.60,1.00]\n",
        "\n",
        "# Threshold policies\n",
        "TARGET_SPEC   = 0.95              # α = 1 - TARGET_SPEC (FPR cap)\n",
        "ALPHA_MARGIN  = 0.01              # calibrate stricter on train-calib\n",
        "ALPHA_TRAIN   = max(0.0, (1.0 - TARGET_SPEC) - ALPHA_MARGIN)\n",
        "TARGET_RECALL = 0.85              # COMBINED-like recall target (used only at keep=1.0, extra view)\n",
        "\n",
        "# Split policy\n",
        "RANDOM_STATE = 42\n",
        "N_SPLITS     = 5\n",
        "TEST_FOLD    = 0\n",
        "\n",
        "# Model choices + class-weight to mimic your COMBINED setting\n",
        "CLASS_WEIGHT = {0:1, 1:2}\n",
        "\n",
        "# Role concentration hyperparams\n",
        "TOPK_MIN, TOPK_MAX, TOPK_FRAC = 3, 25, 0.02\n",
        "# --------------------------------------------\n",
        "\n",
        "# ---------- Discovery (use merged if present) ----------\n",
        "def _infer_label_from_path(p: str) -> int:\n",
        "    s = p.lower()\n",
        "    pos = (\"attack\",\"mal\",\"comprom\",\"advers\",\"redteam\",\"evil\")\n",
        "    neg = (\"benign\",\"normal\",\"baseline\",\"clean\",\"safe\")\n",
        "    if any(k in s for k in pos) and not any(k in s for k in neg): return 1\n",
        "    if any(k in s for k in neg) and not any(k in s for k in pos): return 0\n",
        "    return 0\n",
        "\n",
        "def _discover():\n",
        "    if \"merged\" in globals():\n",
        "        df = globals()[\"merged\"]\n",
        "        file_col = next((c for c in df.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), None)\n",
        "        if file_col is not None:\n",
        "            paths = [Path(str(p)) for p in df[file_col].tolist()]\n",
        "            y_global = globals().get(\"y\", None)\n",
        "            if y_global is not None and len(y_global)==len(paths):\n",
        "                return paths, np.asarray(y_global).astype(int), \"provided\"\n",
        "            # label column fallback\n",
        "            for c in df.columns:\n",
        "                lc = c.lower()\n",
        "                if any(k in lc for k in (\"cohort\",\"label\",\"is_attack\",\"y\",\"target\")):\n",
        "                    vals = df[c].astype(str).str.lower()\n",
        "                    yy = vals.isin([\"1\",\"true\",\"attack\",\"attk\",\"pos\",\"positive\"]).astype(int).values\n",
        "                    if len(yy)==len(paths): return paths, yy, \"provided\"\n",
        "            yy = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "            return paths, yy, \"provided\"\n",
        "    # glob fallback\n",
        "    hits = glob.glob(\"**/*.gpickle\", recursive=True) + glob.glob(\"**/*.gpkl\", recursive=True)\n",
        "    paths = [Path(h) for h in sorted(set(hits))]\n",
        "    if not paths: raise RuntimeError(\"No graphs found (.gpickle/.gpkl). Provide `merged` or place graphs under CWD.\")\n",
        "    y = np.array([_infer_label_from_path(str(p)) for p in paths], dtype=int)\n",
        "    return paths, y, \"glob\"\n",
        "\n",
        "# gpickle loader\n",
        "if \"load_gpickle\" in globals() and callable(globals()[\"load_gpickle\"]):\n",
        "    _loader = load_gpickle\n",
        "else:\n",
        "    from networkx.readwrite.gpickle import read_gpickle as _loader\n",
        "\n",
        "paths, y, src = _discover()\n",
        "mask = np.array([p.exists() for p in paths], bool)\n",
        "paths = [p for p, ok in zip(paths, mask) if ok]; y = np.asarray(y)[mask]\n",
        "\n",
        "print(f\"[INFO] Found {len(paths)} graphs from {src}. Class balance: {np.bincount(y) if y.size else []}\")\n",
        "\n",
        "# ---------- Prefix builders ----------\n",
        "_TS_KEYS = (\"end_ts\",\"ts\",\"timestamp\",\"time\",\"t\",\"t_ms\")\n",
        "def _edge_time_or_idx(d, idx):\n",
        "    for k in _TS_KEYS:\n",
        "        if k in d and d[k] is not None:\n",
        "            try: return float(d[k])\n",
        "            except: pass\n",
        "    return float(idx)\n",
        "\n",
        "def prefix_by_edges(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    m = G_full.number_of_edges()\n",
        "    if m == 0: return G_full.__class__(), 0.0\n",
        "    edges = list(G_full.edges(keys=True, data=True))\n",
        "    edges_sorted = sorted(\n",
        "        [(u, v, k, _edge_time_or_idx(d, i)) for i,(u,v,k,d) in enumerate(edges)],\n",
        "        key=lambda x: (x[3], str(x[0]), str(x[1]), str(x[2]))\n",
        "    )\n",
        "    k = max(1, int(round(m * max(0.0, min(1.0, keep_frac)))))\n",
        "    keep_e = set((u,v,k_) for (u,v,k_,_) in edges_sorted[:k])\n",
        "    nodes_keep = set()\n",
        "    for (u,v,k_) in keep_e:\n",
        "        nodes_keep.add(u); nodes_keep.add(v)\n",
        "    H = G_full.subgraph(nodes_keep).copy()\n",
        "    H2 = G_full.__class__()\n",
        "    H2.add_nodes_from(H.nodes(data=True))\n",
        "    for (u,v,k_,d) in edges:\n",
        "        if (u,v,k_) in keep_e:\n",
        "            H2.add_edge(u,v,key=k_,**d)\n",
        "    return H2, k/m\n",
        "\n",
        "def prefix_by_nodes(G_full: nx.Graph, keep_frac: float):\n",
        "    if not isinstance(G_full, nx.MultiDiGraph): G_full = nx.MultiDiGraph(G_full)\n",
        "    if G_full.number_of_nodes()==0: return G_full.__class__(), 0.0\n",
        "    node_t = {n: np.inf for n in G_full.nodes()}\n",
        "    for idx,(u,v,d) in enumerate(G_full.edges(data=True)):\n",
        "        t = _edge_time_or_idx(d, idx)\n",
        "        if np.isfinite(t):\n",
        "            if t < node_t[u]: node_t[u] = t\n",
        "            if t < node_t[v]: node_t[v] = t\n",
        "    order = [n for n,_t in sorted(node_t.items(), key=lambda kv: (kv[1], str(kv[0])))]\n",
        "    k = max(1, int(round(len(order)*max(0.0,min(1.0,keep_frac)))))\n",
        "    return G_full.subgraph(order[:k]).copy(), k/len(order)\n",
        "\n",
        "PREFIX_FN = prefix_by_edges if MODE_PREFIX==\"edge\" else prefix_by_nodes\n",
        "\n",
        "# ---------- Features: base + dynamic roles + burst + relatives ----------\n",
        "def _safe_div(a,b): return float(a)/float(b) if b not in (None,0) else 0.0\n",
        "\n",
        "def _gini(arr):\n",
        "    x = np.asarray(arr, float); x = x[np.isfinite(x)]\n",
        "    if x.size==0 or np.all(x==0): return 0.0\n",
        "    x = np.sort(np.abs(x)); n = x.size; cumx = np.cumsum(x)\n",
        "    return float((n+1 - 2*np.sum(cumx)/cumx[-1]) / n)\n",
        "\n",
        "def _centralization(scores):\n",
        "    if not scores: return 0.0, 0.0\n",
        "    vals = np.array(list(scores.values()), float); vals = vals[np.isfinite(vals)]\n",
        "    if vals.size==0: return 0.0, 0.0\n",
        "    ssum = vals.sum()\n",
        "    k = max(TOPK_MIN, min(TOPK_MAX, int(round(TOPK_FRAC * max(1, vals.size)))))\n",
        "    top = np.sort(vals)[-min(k, vals.size):][::-1]\n",
        "    return float(top.sum()), float(top.sum()/ssum if ssum>0 else 0.0)\n",
        "\n",
        "def _basic_stats(G):\n",
        "    n = G.number_of_nodes(); m = G.number_of_edges()\n",
        "    if n==0:\n",
        "        return dict(n=0, m=0, density=0.0, avg_deg=0.0, n_comp=0, largest=0, epn=0.0, largest_frac=0.0, deg_gini=0.0)\n",
        "    degs = [d for _,d in G.degree()]\n",
        "    try:\n",
        "        comps = list(nx.weakly_connected_components(G)) if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)) else list(nx.connected_components(G))\n",
        "    except Exception:\n",
        "        comps = []\n",
        "    largest = max((len(c) for c in comps), default=0)\n",
        "    return dict(\n",
        "        n=n, m=m,\n",
        "        density=(nx.density(G) if n>1 else 0.0),\n",
        "        avg_deg=(float(np.mean(degs)) if degs else 0.0),\n",
        "        n_comp=len(comps),\n",
        "        largest=largest,\n",
        "        largest_frac=(largest/n if n>0 else 0.0),\n",
        "        epn=(m/n if n>0 else 0.0),\n",
        "        deg_gini=_gini(degs)\n",
        "    )\n",
        "\n",
        "def _role_features_DYNAMIC(G):\n",
        "    if G.number_of_nodes()==0:\n",
        "        return dict(pr_top=0.0, pr_top_ratio=0.0, hub_top=0.0, hub_top_ratio=0.0, auth_top=0.0, auth_top_ratio=0.0)\n",
        "    Gu = nx.Graph(G) if isinstance(G,(nx.MultiDiGraph,nx.DiGraph)) else nx.Graph(G)\n",
        "    try:    pr = nx.pagerank(Gu, alpha=0.85, tol=1e-6, max_iter=100)\n",
        "    except: pr = {n: 1.0/Gu.number_of_nodes() for n in Gu.nodes()} if Gu.number_of_nodes()>0 else {}\n",
        "    pr_top, pr_top_ratio = _centralization(pr)\n",
        "    if isinstance(G,(nx.DiGraph,nx.MultiDiGraph)):\n",
        "        try:\n",
        "            Hsimple = nx.DiGraph(); Hsimple.add_nodes_from(G.nodes())\n",
        "            for u,v in G.edges(): Hsimple.add_edge(u,v)\n",
        "            hubs, auths = nx.hits(Hsimple, max_iter=200, tol=1e-8, normalized=True)\n",
        "        except Exception:\n",
        "            hubs  = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "            auths = {n: 1.0/G.number_of_nodes() for n in G.nodes()}\n",
        "    else:\n",
        "        hubs  = {n: 0.0 for n in G.nodes()}\n",
        "        auths = {n: 0.0 for n in G.nodes()}\n",
        "    hub_top,  hub_top_ratio  = _centralization(hubs)\n",
        "    auth_top, auth_top_ratio = _centralization(auths)\n",
        "    return dict(pr_top=pr_top, pr_top_ratio=pr_top_ratio,\n",
        "                hub_top=hub_top, hub_top_ratio=hub_top_ratio,\n",
        "                auth_top=auth_top, auth_top_ratio=auth_top_ratio)\n",
        "\n",
        "def _temporal_burst_in_prefix(G):\n",
        "    m = G.number_of_edges()\n",
        "    if m==0: return dict(prefix_burst=0.0, t_span=0.0)\n",
        "    ts=[]\n",
        "    for idx,(_,_,d) in enumerate(G.edges(data=True)):\n",
        "        ts.append(_edge_time_or_idx(d, idx))\n",
        "    ts = np.array(ts, float); ts.sort()\n",
        "    span = max(1e-9, ts[-1]-ts[0])\n",
        "    t_cut = ts[0] + 0.8*span\n",
        "    return dict(prefix_burst=float((ts>=t_cut).mean()), t_span=float(span))\n",
        "\n",
        "def feature_row_prefix_vs_full(H, keep_frac, S_full, R_full):\n",
        "    S_p = _basic_stats(H)\n",
        "    R_p = _role_features_DYNAMIC(H)\n",
        "    B_p = _temporal_burst_in_prefix(H)\n",
        "    out = dict(\n",
        "        n_nodes=S_p[\"n\"], n_edges=S_p[\"m\"], density=S_p[\"density\"], avg_deg=S_p[\"avg_deg\"],\n",
        "        n_components=S_p[\"n_comp\"], largest_cc=S_p[\"largest\"], largest_cc_frac=S_p[\"largest_frac\"],\n",
        "        edges_per_node=S_p[\"epn\"], deg_gini=S_p[\"deg_gini\"],\n",
        "        keep_frac=float(keep_frac),\n",
        "        pr_top=R_p[\"pr_top\"], pr_top_ratio=R_p[\"pr_top_ratio\"],\n",
        "        hub_top=R_p[\"hub_top\"], hub_top_ratio=R_p[\"hub_top_ratio\"],\n",
        "        auth_top=R_p[\"auth_top\"], auth_top_ratio=R_p[\"auth_top_ratio\"],\n",
        "        prefix_burst=B_p[\"prefix_burst\"], t_span=B_p[\"t_span\"],\n",
        "        # relatives to full graph\n",
        "        frac_nodes_of_full=_safe_div(S_p[\"n\"], S_full[\"n\"]),\n",
        "        frac_edges_of_full=_safe_div(S_p[\"m\"], S_full[\"m\"]),\n",
        "        density_rel=_safe_div(S_p[\"density\"], S_full[\"density\"]) if S_full[\"density\"]>0 else 0.0,\n",
        "        epn_rel=_safe_div(S_p[\"epn\"], S_full[\"epn\"]) if S_full[\"epn\"]>0 else 0.0,\n",
        "        deg_gini_rel=_safe_div(S_p[\"deg_gini\"], S_full[\"deg_gini\"]) if S_full[\"deg_gini\"]>0 else 0.0,\n",
        "        pr_top_ratio_rel=_safe_div(R_p[\"pr_top_ratio\"], R_full[\"pr_top_ratio\"]) if R_full[\"pr_top_ratio\"]>0 else 0.0,\n",
        "        hub_top_ratio_rel=_safe_div(R_p[\"hub_top_ratio\"], R_full[\"hub_top_ratio\"]) if R_full[\"hub_top_ratio\"]>0 else 0.0,\n",
        "        auth_top_ratio_rel=_safe_div(R_p[\"auth_top_ratio\"], R_full[\"auth_top_ratio\"]) if R_full[\"auth_top_ratio\"]>0 else 0.0,\n",
        "        nodes_ahead=_safe_div(S_p[\"n\"], S_full[\"n\"]) - keep_frac,\n",
        "        edges_ahead=_safe_div(S_p[\"m\"], S_full[\"m\"]) - keep_frac,\n",
        "        epn_x_keep=S_p[\"epn\"]*keep_frac,\n",
        "    )\n",
        "    return pd.Series(out, dtype=\"float64\")\n",
        "\n",
        "# ---------- Optional static attributes (df_num) ----------\n",
        "if \"df_num\" in globals():\n",
        "    file_col_m = next((c for c in merged.columns if c.lower() in (\"file\",\"path\",\"filepath\",\"graph_path\")), \"file\")\n",
        "    if \"file\" not in df_num.columns:\n",
        "        raise RuntimeError(\"df_num must contain 'file' for alignment.\")\n",
        "    _aligned_num = df_num.set_index(\"file\").loc[merged[file_col_m]].reset_index()\n",
        "    _num_cols = [c for c in _aligned_num.columns\n",
        "                 if c not in (\"file\",\"graph_id\",\"bundle\",\"cohort\")\n",
        "                 and pd.api.types.is_numeric_dtype(_aligned_num[c])]\n",
        "    Z = _aligned_num[_num_cols].replace([np.inf, -np.inf], np.nan)\n",
        "    med = Z.median(numeric_only=True); Z = Z.fillna(med)\n",
        "    q01 = Z.quantile(0.01, numeric_only=True); q99 = Z.quantile(0.99, numeric_only=True)\n",
        "    for c in Z.columns:\n",
        "        lo = q01.get(c, None); hi = q99.get(c, None)\n",
        "        if lo is not None and hi is not None and np.isfinite([lo,hi]).all():\n",
        "            Z[c] = Z[c].clip(lo, hi)\n",
        "    ATTRS_df = Z.astype(\"float64\").copy()\n",
        "    ATTRS_df.columns = [f\"attr__{c}\" for c in ATTRS_df.columns]\n",
        "else:\n",
        "    print(\"[WARN] df_num not found; proceeding without static ATTRS.\")\n",
        "    ATTRS_df = pd.DataFrame(index=range(len(paths)))\n",
        "\n",
        "def _attrs_row(i: int) -> pd.Series:\n",
        "    if ATTRS_df.shape[0]==0: return pd.Series(dtype=\"float64\")\n",
        "    return ATTRS_df.iloc[i]\n",
        "\n",
        "def feature_row_with_ATTRS(H, keep_frac, S_full, R_full, i_index: int):\n",
        "    base = feature_row_prefix_vs_full(H, keep_frac, S_full, R_full)\n",
        "    return pd.concat([base, _attrs_row(i_index)], axis=0)\n",
        "\n",
        "# ---------- Models ----------\n",
        "def _mk_hgb(): return Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", HistGradientBoostingClassifier(random_state=RANDOM_STATE))\n",
        "])\n",
        "def _mk_lr():  return Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"sc\", StandardScaler()),\n",
        "    (\"clf\", LogisticRegression(max_iter=3000, class_weight=CLASS_WEIGHT, random_state=RANDOM_STATE))\n",
        "])\n",
        "def _mk_rf():  return Pipeline([\n",
        "    (\"imp\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=800, n_jobs=-1,\n",
        "                                  class_weight=\"balanced_subsample\",\n",
        "                                  random_state=RANDOM_STATE))\n",
        "])\n",
        "MODELS = {\"HGB\": _mk_hgb(), \"LogReg\": _mk_lr(), \"RF\": _mk_rf()}\n",
        "\n",
        "# ---------- Split (match COMBINED groups if provided; or accept fixed indices) ----------\n",
        "if \"tr_idx_fixed\" in globals() and \"te_idx_fixed\" in globals():\n",
        "    tr_idx, te_idx = np.asarray(tr_idx_fixed), np.asarray(te_idx_fixed)\n",
        "else:\n",
        "    outer = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
        "    tr_idx, te_idx = list(outer.split(np.arange(len(y)), y))[TEST_FOLD]\n",
        "\n",
        "# further split TRAIN into fit/calibration (like you did)\n",
        "tr_fit_idx, tr_cal_idx = train_test_split(tr_idx, test_size=0.30, stratify=y[tr_idx], random_state=RANDOM_STATE)\n",
        "\n",
        "# ---------- Precompute full-graph packs ----------\n",
        "def _full_pack(G):\n",
        "    if not isinstance(G, nx.MultiDiGraph): G = nx.MultiDiGraph(G)\n",
        "    return _basic_stats(G), _role_features_DYNAMIC(G)\n",
        "\n",
        "full_pack = {}\n",
        "for i in set(tr_fit_idx) | set(tr_cal_idx) | set(te_idx):\n",
        "    full_pack[i] = _full_pack(_loader(paths[i]))\n",
        "\n",
        "# ---------- TRAIN (prefix-augmented) ----------\n",
        "X_train_rows=[]; y_train=[]\n",
        "\n",
        "# Optional: sample weights to mimic class_weight for HGB (which lacks class_weight)\n",
        "def _class_weights_for_indices(idx):\n",
        "    # class weights {0:1,1:2}\n",
        "    ys = y[idx]\n",
        "    sw = np.ones_like(ys, dtype=float)\n",
        "    sw[ys==1] = CLASS_WEIGHT.get(1,1.0)\n",
        "    sw[ys==0] = CLASS_WEIGHT.get(0,1.0)\n",
        "    return sw\n",
        "\n",
        "for i in tr_fit_idx:\n",
        "    Gfull = _loader(paths[i]); S_full,R_full = full_pack[i]\n",
        "    for kf in KEEP_FRACS_TRAIN:\n",
        "        H, actual = PREFIX_FN(Gfull, kf)\n",
        "        X_train_rows.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i) )\n",
        "        y_train.append(y[i])\n",
        "X_train = pd.DataFrame(X_train_rows); y_train = np.array(y_train)\n",
        "SCHEMA = X_train.columns.tolist()\n",
        "\n",
        "# Fit models\n",
        "for name, model in MODELS.items():\n",
        "    if name==\"HGB\":\n",
        "        sample_w = np.array([CLASS_WEIGHT.get(lbl,1.0) for lbl in y_train], float)\n",
        "        model.fit(X_train, y_train, clf__sample_weight=sample_w)\n",
        "    else:\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "print(f\"[TRAIN] Fit with {len(X_train)} prefix samples (from {len(tr_fit_idx)} graphs). Features: {len(SCHEMA)}\")\n",
        "\n",
        "# ---------- Helpers ----------\n",
        "def _safe_auc(y_true, s):\n",
        "    try: return float(roc_auc_score(y_true, s))\n",
        "    except Exception: return float('nan')\n",
        "\n",
        "def _pick_thresh_fpr_cap(y_true, s, alpha=ALPHA_TRAIN):\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    eps = 1e-12\n",
        "    ok = (fpr <= (alpha + eps))\n",
        "    if ok.any():\n",
        "        idx = np.argmax(tpr[ok])\n",
        "        cands = np.flatnonzero(tpr[ok] == tpr[ok][idx])\n",
        "        pick = cands[np.argmin(fpr[ok][cands])]\n",
        "        k = np.flatnonzero(ok)[pick]\n",
        "    else:\n",
        "        minf = np.min(fpr); cands = np.flatnonzero(fpr == minf)\n",
        "        k = cands[np.argmax(tpr[cands])]\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "def _pick_thresh_hit_recall(y_true, s, target_recall=TARGET_RECALL):\n",
        "    # choose smallest threshold achieving TPR >= target; tie-break by larger precision (lower FPR)\n",
        "    fpr, tpr, ths = roc_curve(y_true, s)\n",
        "    ok = (tpr >= target_recall)\n",
        "    if ok.any():\n",
        "        idxs = np.flatnonzero(ok)\n",
        "        # among ok, pick the one with highest (tpr - fpr) then lowest fpr\n",
        "        best = None; best_key=None\n",
        "        for k in idxs:\n",
        "            key = (tpr[k]-fpr[k], -fpr[k])\n",
        "            if (best is None) or (key > best_key):\n",
        "                best, best_key = k, key\n",
        "        k = best\n",
        "    else:\n",
        "        # fallback: maximize F1 on calib\n",
        "        pr_list=[]; rec_list=[]; th_list=[]\n",
        "        yb = (y_true==1).astype(int)\n",
        "        for th in np.unique(ths):\n",
        "            yhat = (s>=th).astype(int)\n",
        "            p,r,f,_ = precision_recall_fscore_support(yb, yhat, average=\"binary\", zero_division=0)\n",
        "            pr_list.append(p); rec_list.append(r); th_list.append(th)\n",
        "        if th_list:\n",
        "            f1s = [2*p*r/(p+r+1e-12) for p,r in zip(pr_list,rec_list)]\n",
        "            k = int(np.argmax(f1s))\n",
        "            th = th_list[k]\n",
        "            # find k closest in ROC grid\n",
        "            kk = np.argmin(np.abs(ths - th))\n",
        "            return float(th), float(fpr[kk]), float(tpr[kk])\n",
        "        k = np.argmax(tpr - fpr)\n",
        "    return float(ths[k]), float(fpr[k]), float(tpr[k])\n",
        "\n",
        "# ---------- CALIBRATION: per keep (α-cap) + 1.0 recall-target ----------\n",
        "theta_map = {name:{} for name in MODELS}\n",
        "flip_map  = {name:{} for name in MODELS}\n",
        "calib_rep = {name:{} for name in MODELS}\n",
        "theta_recall_at_1 = {}   # COMBINED-like θ at keep=1.0\n",
        "\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xs=[]; yc=[]\n",
        "    for i in tr_cal_idx:\n",
        "        Gfull = _loader(paths[i]); S_full,R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xs.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA) )\n",
        "        yc.append(y[i])\n",
        "    Xc = pd.DataFrame(xs); yc = np.array(yc)\n",
        "    for name, model in MODELS.items():\n",
        "        s = model.predict_proba(Xc)[:,1]\n",
        "        auc = _safe_auc(yc, s); flipped=False\n",
        "        if np.isfinite(auc) and auc < 0.5:\n",
        "            s = 1.0 - s; flipped=True; auc = 1.0 - auc\n",
        "        th, fpr_c, tpr_c = _pick_thresh_fpr_cap(yc, s, alpha=ALPHA_TRAIN)\n",
        "        theta_map[name][keep] = th; flip_map[name][keep] = flipped\n",
        "        calib_rep[name][keep] = dict(auc=auc, fpr=fpr_c, tpr=tpr_c)\n",
        "        if abs(keep-1.0) < 1e-9:\n",
        "            th2, fpr2, tpr2 = _pick_thresh_hit_recall(yc, s, TARGET_RECALL)\n",
        "            theta_recall_at_1[name] = dict(theta=th2, fpr=fpr2, tpr=tpr2, auc=auc)\n",
        "\n",
        "print(f\"[CALIB] α_train={ALPHA_TRAIN:.3f} (margin {ALPHA_MARGIN:.3f}) — first keeps:\")\n",
        "for name in MODELS:\n",
        "    demo = {k: (round(calib_rep[name][k]['auc'],3),\n",
        "                round(calib_rep[name][k]['fpr'],3),\n",
        "                round(calib_rep[name][k]['tpr'],3)) for k in KEEP_FRACS_EVAL[:4]}\n",
        "    print(f\"  {name}: {demo} ...\")\n",
        "\n",
        "# ---------- TEST SCORES ----------\n",
        "SCORES = {name:{} for name in MODELS}; YTEST={}\n",
        "for keep in KEEP_FRACS_EVAL:\n",
        "    xt=[]; yt=[]\n",
        "    for i in te_idx:\n",
        "        Gfull = _loader(paths[i]); S_full,R_full = full_pack[i]\n",
        "        H, actual = PREFIX_FN(Gfull, keep)\n",
        "        xt.append( feature_row_with_ATTRS(H, actual, S_full, R_full, i).reindex(SCHEMA) )\n",
        "        yt.append(y[i])\n",
        "    Xt = pd.DataFrame(xt); yt = np.array(yt); YTEST[keep]=yt\n",
        "    for name, model in MODELS.items():\n",
        "        p = model.predict_proba(Xt)[:,1]\n",
        "        if flip_map[name][keep]: p = 1.0 - p\n",
        "        SCORES[name][keep] = p\n",
        "\n",
        "# ---------- Metrics under α-cap thresholds (monotone yhat over keeps) ----------\n",
        "def _metrics_table_from_preds(Ytrue_by_keep, Scores_by_keep, Theta_by_keep, enforce_monotone=True):\n",
        "    rows=[]\n",
        "    keeps_sorted = sorted(Ytrue_by_keep.keys())\n",
        "    for name in MODELS:\n",
        "        ths = np.array([Theta_by_keep[name][k] for k in keeps_sorted], float)\n",
        "        # stack scores as KxN\n",
        "        P = np.vstack([Scores_by_keep[name][k] for k in keeps_sorted])\n",
        "        YH = (P >= ths[:,None]).astype(int)\n",
        "        if enforce_monotone:\n",
        "            YH = np.maximum.accumulate(YH, axis=0)\n",
        "        for kk, keep in enumerate(keeps_sorted):\n",
        "            yt = Ytrue_by_keep[keep]; yhat = YH[kk]\n",
        "            tp = int(((yhat==1)&(yt==1)).sum())\n",
        "            fp = int(((yhat==1)&(yt==0)).sum())\n",
        "            tn = int(((yhat==0)&(yt==0)).sum())\n",
        "            fn = int(((yhat==0)&(yt==1)).sum())\n",
        "            acc = (tp+tn)/max(tp+tn+fp+fn,1)\n",
        "            prec = tp/max(tp+fp,1) if (tp+fp)>0 else 0.0\n",
        "            rec  = tp/max(tp+fn,1) if (tp+fn)>0 else 0.0\n",
        "            spc  = tn/max(tn+fp,1) if (tn+fp)>0 else 0.0\n",
        "            f1   = (2*prec*rec)/(prec+rec+1e-12) if (prec+rec)>0 else 0.0\n",
        "            auc  = _safe_auc(yt, P[kk])\n",
        "            rows.append(dict(keep_frac=keep, model=name,\n",
        "                             tp=tp, fp=fp, tn=tn, fn=fn,\n",
        "                             accuracy=acc, precision=prec, recall=rec, specificity=spc, f1=f1, auc=auc))\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "res_all = _metrics_table_from_preds(YTEST, SCORES, theta_map, enforce_monotone=True)\n",
        "\n",
        "def _pv(df, metric):\n",
        "    return df.pivot_table(index=\"keep_frac\", columns=\"model\", values=metric).round(3)\n",
        "\n",
        "print(\"\\n==== Accuracy by keep ====\");     print(_pv(res_all,\"accuracy\"))\n",
        "print(\"\\n==== Precision by keep ====\");    print(_pv(res_all,\"precision\"))\n",
        "print(\"\\n==== Recall by keep ====\");       print(_pv(res_all,\"recall\"))\n",
        "print(\"\\n==== Specificity by keep ====\");  print(_pv(res_all,\"specificity\"))\n",
        "print(\"\\n==== F1 by keep ====\");           print(_pv(res_all,\"f1\"))\n",
        "print(\"\\n==== AUC by keep ====\");          print(_pv(res_all,\"auc\"))\n",
        "\n",
        "print(\"\\n==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\")\n",
        "print(res_all.sort_values([\"keep_frac\",\"model\"]).head(15).to_string(index=False))\n",
        "\n",
        "# ---------- EXTRA: COMBINED-like view at keep=1.0 (target recall θ) ----------\n",
        "records_comb=[]\n",
        "keep = 1.0\n",
        "if any(abs(k-keep)<1e-9 for k in KEEP_FRACS_EVAL):\n",
        "    yt = YTEST[keep]\n",
        "    for name, model in MODELS.items():\n",
        "        p = SCORES[name][keep]\n",
        "        th2 = theta_recall_at_1.get(name,{}).get(\"theta\", None)\n",
        "        if th2 is None:\n",
        "            continue\n",
        "        yhat = (p >= th2).astype(int)\n",
        "        tp = int(((yhat==1)&(yt==1)).sum())\n",
        "        fp = int(((yhat==1)&(yt==0)).sum())\n",
        "        tn = int(((yhat==0)&(yt==0)).sum())\n",
        "        fn = int(((yhat==0)&(yt==1)).sum())\n",
        "        acc = (tp+tn)/max(tp+tn+fp+fn,1)\n",
        "        prec = tp/max(tp+fp,1) if (tp+fp)>0 else 0.0\n",
        "        rec  = tp/max(tp+fn,1) if (tp+fn)>0 else 0.0\n",
        "        spc  = tn/max(tn+fp,1) if (tn+fp)>0 else 0.0\n",
        "        f1   = (2*prec*rec)/(prec+rec+1e-12) if (prec+rec)>0 else 0.0\n",
        "        auc  = _safe_auc(yt, p)\n",
        "        records_comb.append(dict(model=name, accuracy=acc, precision=prec, recall=rec, specificity=spc, f1=f1, auc=auc,\n",
        "                                 tp=tp, fp=fp, tn=tn, fn=fn, theta=th2))\n",
        "    if records_comb:\n",
        "        df_comb = pd.DataFrame(records_comb)\n",
        "        print(\"\\n==== keep=1.0 — COMBINED-like threshold (hit target recall on calib) ====\")\n",
        "        print(df_comb.set_index(\"model\")[[\"accuracy\",\"precision\",\"recall\",\"specificity\",\"f1\",\"auc\",\"tp\",\"fp\",\"tn\",\"fn\",\"theta\"]].round(3))\n",
        "    else:\n",
        "        print(\"\\n[NOTE] No COMBINED-like θ computed at 1.0 (unexpected).\")\n",
        "\n",
        "# ---------- Coverage sanity ----------\n",
        "def _coverage_summary(mode=MODE_PREFIX):\n",
        "    rows=[]\n",
        "    for keep in KEEP_FRACS_EVAL:\n",
        "        for i in te_idx:\n",
        "            Gfull = _loader(paths[i])\n",
        "            n_full = Gfull.number_of_nodes(); m_full = Gfull.number_of_edges()\n",
        "            H, actual = (prefix_by_edges(Gfull, keep) if mode==\"edge\" else prefix_by_nodes(Gfull, keep))\n",
        "            n_pref = H.number_of_nodes(); m_pref = H.number_of_edges()\n",
        "            rows.append(dict(keep=keep, n_full=n_full, n_pref=n_pref, m_full=m_full, m_pref=m_pref,\n",
        "                             frac_nodes_kept=(n_pref/n_full if n_full>0 else np.nan),\n",
        "                             frac_edges_kept=(m_pref/m_full if m_full>0 else np.nan),\n",
        "                             nodes_cut=max(n_full-n_pref,0)))\n",
        "    df = pd.DataFrame(rows)\n",
        "    def q(x,p):\n",
        "        x = np.asarray(x, float); x = x[np.isfinite(x)]\n",
        "        return np.quantile(x, p) if x.size else np.nan\n",
        "    summ = df.groupby(\"keep\").agg(\n",
        "        graphs=(\"keep\",\"count\"),\n",
        "        nodes_kept_mean=(\"frac_nodes_kept\",\"mean\"),\n",
        "        nodes_kept_median=(\"frac_nodes_kept\",\"median\"),\n",
        "        edges_kept_mean=(\"frac_edges_kept\",\"mean\"),\n",
        "        edges_kept_median=(\"frac_edges_kept\",\"median\"),\n",
        "        nodes_cut_median=(\"nodes_cut\",\"median\"),\n",
        "    ).round(3)\n",
        "    print(f\"\\n==== Prefix coverage sanity on TEST (mode='{mode}') ====\")\n",
        "    print(summ)\n",
        "\n",
        "_coverage_summary(MODE_PREFIX)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8rY2VhzmYIG",
        "outputId": "42c16287-bd18-4b19-b16f-ea16fd4f0bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Found 774 graphs from glob. Class balance: [376 398]\n",
            "[WARN] df_num not found; proceeding without static ATTRS.\n",
            "[TRAIN] Fit with 1732 prefix samples (from 433 graphs). Features: 29\n",
            "[CALIB] α_train=0.040 (margin 0.010) — first keeps:\n",
            "  HGB: {0.2: (0.552, 0.022, 0.062), 0.4: (0.874, 0.033, 0.698), 0.6: (0.83, 0.022, 0.646), 0.8: (0.838, 0.033, 0.688)} ...\n",
            "  LogReg: {0.2: (0.608, 0.0, 0.0), 0.4: (0.85, 0.022, 0.667), 0.6: (0.845, 0.022, 0.667), 0.8: (0.828, 0.022, 0.656)} ...\n",
            "  RF: {0.2: (0.528, 0.0, 0.0), 0.4: (0.814, 0.011, 0.542), 0.6: (0.783, 0.022, 0.604), 0.8: (0.832, 0.033, 0.667)} ...\n",
            "\n",
            "==== Accuracy by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.523   0.484  0.484\n",
            "0.4        0.781   0.768  0.729\n",
            "0.6        0.781   0.768  0.781\n",
            "0.8        0.781   0.768  0.781\n",
            "1.0        0.781   0.768  0.768\n",
            "\n",
            "==== Precision by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.875   0.000  0.000\n",
            "0.4        0.859   0.893  1.000\n",
            "0.6        0.859   0.893  0.942\n",
            "0.8        0.859   0.855  0.883\n",
            "1.0        0.859   0.855  0.855\n",
            "\n",
            "==== Recall by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.088   0.000  0.000\n",
            "0.4        0.688   0.625  0.475\n",
            "0.6        0.688   0.625  0.612\n",
            "0.8        0.688   0.662  0.662\n",
            "1.0        0.688   0.662  0.662\n",
            "\n",
            "==== Specificity by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.987    1.00  1.000\n",
            "0.4        0.880    0.92  1.000\n",
            "0.6        0.880    0.92  0.960\n",
            "0.8        0.880    0.88  0.907\n",
            "1.0        0.880    0.88  0.880\n",
            "\n",
            "==== F1 by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.159   0.000  0.000\n",
            "0.4        0.764   0.735  0.644\n",
            "0.6        0.764   0.735  0.742\n",
            "0.8        0.764   0.746  0.757\n",
            "1.0        0.764   0.746  0.746\n",
            "\n",
            "==== AUC by keep ====\n",
            "model        HGB  LogReg     RF\n",
            "keep_frac                      \n",
            "0.2        0.531   0.459  0.514\n",
            "0.4        0.789   0.772  0.760\n",
            "0.6        0.751   0.788  0.801\n",
            "0.8        0.799   0.809  0.798\n",
            "1.0        0.810   0.809  0.786\n",
            "\n",
            "==== Confusion counts (tp/fp/tn/fn) at each keep (first few rows) ====\n",
            " keep_frac  model  tp  fp  tn  fn  accuracy  precision  recall  specificity       f1      auc\n",
            "       0.2    HGB   7   1  74  73  0.522581   0.875000  0.0875     0.986667 0.159091 0.530750\n",
            "       0.2 LogReg   0   0  75  80  0.483871   0.000000  0.0000     1.000000 0.000000 0.459083\n",
            "       0.2     RF   0   0  75  80  0.483871   0.000000  0.0000     1.000000 0.000000 0.514500\n",
            "       0.4    HGB  55   9  66  25  0.780645   0.859375  0.6875     0.880000 0.763889 0.789417\n",
            "       0.4 LogReg  50   6  69  30  0.767742   0.892857  0.6250     0.920000 0.735294 0.771833\n",
            "       0.4     RF  38   0  75  42  0.729032   1.000000  0.4750     1.000000 0.644068 0.759917\n",
            "       0.6    HGB  55   9  66  25  0.780645   0.859375  0.6875     0.880000 0.763889 0.751417\n",
            "       0.6 LogReg  50   6  69  30  0.767742   0.892857  0.6250     0.920000 0.735294 0.788000\n",
            "       0.6     RF  49   3  72  31  0.780645   0.942308  0.6125     0.960000 0.742424 0.800667\n",
            "       0.8    HGB  55   9  66  25  0.780645   0.859375  0.6875     0.880000 0.763889 0.798583\n",
            "       0.8 LogReg  53   9  66  27  0.767742   0.854839  0.6625     0.880000 0.746479 0.808667\n",
            "       0.8     RF  53   7  68  27  0.780645   0.883333  0.6625     0.906667 0.757143 0.797500\n",
            "       1.0    HGB  55   9  66  25  0.780645   0.859375  0.6875     0.880000 0.763889 0.810167\n",
            "       1.0 LogReg  53   9  66  27  0.767742   0.854839  0.6625     0.880000 0.746479 0.808667\n",
            "       1.0     RF  53   9  66  27  0.767742   0.854839  0.6625     0.880000 0.746479 0.786167\n",
            "\n",
            "==== keep=1.0 — COMBINED-like threshold (hit target recall on calib) ====\n",
            "        accuracy  precision  recall  specificity     f1    auc  tp  fp  tn  fn  theta\n",
            "model                                                                                \n",
            "HGB        0.677      0.663   0.762        0.587  0.709  0.810  61  31  44  19  0.322\n",
            "LogReg     0.626      0.596   0.850        0.387  0.701  0.809  68  46  29  12  0.685\n",
            "RF         0.613      0.579   0.912        0.293  0.709  0.786  73  53  22   7  0.085\n",
            "\n",
            "==== Prefix coverage sanity on TEST (mode='node') ====\n",
            "      graphs  nodes_kept_mean  nodes_kept_median  edges_kept_mean  edges_kept_median  nodes_cut_median\n",
            "keep                                                                                                  \n",
            "0.2      155            0.199                0.2            0.003              0.000               4.0\n",
            "0.4      155            0.398                0.4            0.156              0.154               3.0\n",
            "0.6      155            0.602                0.6            0.467              0.462               2.0\n",
            "0.8      155            0.801                0.8            0.696              0.692               1.0\n",
            "1.0      155            1.000                1.0            1.000              1.000               0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### just one improve:"
      ],
      "metadata": {
        "id": "bzMVbzkymYLN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GxPIgRUQmYNk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}